\begin{savequote}
"Ideas are always reusable, because they have been usable before[...] The history of ideas should never be continuous; it should be wary of resemblances, but also of descents and filiations; it should be content to mark the thresholds through which an idea passes, the journeys it takes that change its nature or object." 
\qauthor{Giles Deleuze and Felix Guattari, A Thousand Plateaus \footnote{This is from Brian Massumi's translation.}}
\end{savequote}
# Introduction


The book you have before you is in some ways a companion piece to my earlier essay collection, Big and Small Computing. You will not need to read that book in order to understand this one, but doing so will illuminate why I consider a book like this one necessary, and I hope that the two of them together will be worth more than either one alone. Where that book is three quarters manifesto and polemic, this one is intended to be three quarters concrete technical information. Where that book focuses on what computing could be, this one focuses on already-existing systems. What they have in common is a focus on alternatives: systems that deviate from nigh-omnipresent norms and challenge our expectations of how a computer needs to operate. This book is a bestiary of alien forms, positioned to infect you.

## What is a 'system'

This book has two titles. One is 'Interfaces and Systems', and the other is 'A Survey of Alternatives'. I have chosen the term 'system' because I was unable to find a better single term for what I am trying to describe, however, the term is already loaded with meaning from centuries of use. While I am influenced in my analysis by the early cyberneticists, I am not using 'system' in the way that cybernetics or systems theory uses it. Instead, for the purposes of this book, I will be using the following definition:

\begin{quote}
A system is a complete and internally consistent way of thinking about computing. An interface is a way to interact with a computer that uses a system as its underlying structure.
\end{quote}

In other words, systems and interfaces (usually) come in pairs. Iverson Notation is a system that, after a decade or so of use, gained an interface called APL; forth is a system but a particular running forth implementation is (generally) an interface. At the same time, while interfaces are always concrete and instantiated, systems can have various degrees of abstraction: both 'object orientation' and 'smalltalk' are systems, and 'smalltalk' is a concrete, detailed, and specific version of the 'object orientation' system that differs from (say) 'self', 'java', 'unicon', or 'bless': the 'object orientation' system merely declares that computing is done via many small self-contained 'objects' that interact, without specifying things like class/object divisions, inheritance, or access control; 'forth' is a more concrete instantiation of the 'stack language' system, which merely declares that computing is done by manipulating values on a stack; 'Windows 95' and 'Mac OS 9' are two systems that are more concrete versions of the more general system 'WIMP GUI', which only declares that a computer contains overlapping windows that themselves contain widgets such as drop-down menus, icons, and buttons, all of which should be manipulated by a pointing device. The more concrete a system, the more it implies its own interface, and so we can say that every interface has a system that corresponds directly to it -- a conceptual model with every bug documented as a feature.

The thing about this system-interface pairing is that because an interface defines the parts of the dynamics of an underlying model that are accessible (and how accessible they are), the interface functionally defines the system. Features that exist internally to the implementation but are not accessible from the interface and do not impact the percieved behavior of the system are not "part of the system". Furthermore, idioms and other purely socially constructed constraints are also part of the system, to the extent that they are socially enforced. Systems define the landscape of the adjacent-possible, and therefore they define what is easily imaginable; since interfaces define systems, interfaces control not only what we can do but what we can plan. We "think outside the system" by thinking in another system, where things that are difficult or awkward here become easy. To think in another system usually involves experience with an interface that defines that system (and systems become alien in the same way living creatures do: incremental change, with occasional dramatic random mutations or horizontal transfers).

As software developers, in both our personal and professional use of computers, while we interact with a variety of interfaces, almost all of these interfaces share the same broad underlying systems. Certain systems 'won', due to specific historical conditions, and we have based new systems on those old systems, whether or not the conditions that selected them still apply. The dominant systems we interact with today -- WIMP GUIs, bourne-compatible shells in emulators of vt100 terminals, ALGOL-68 derived languages with class-based object models bolted onto the side, tree-structured markup, WYSIWYG rich-text editors -- are like the dodo before the arrival of explorers: thriving in an environment with no natural predators, but uniquely vulnerable to alien threats. I have mined history and the stranger corners of present experimentation in order to deliver some invasive species: alien systems (some are more-extreme versions of familiar forms, like introducing argentine ants where previously only carpenter ants had been; others are completely alien, like introducing flying squid into a population of birds) complete with interfaces that the reader can run at home in order to get hands-on experience.

## On alternatives

\marginnote{This section is adapted from  \cite{onalternatives}}

I love Ted Nelson’s concept of “ALTERNATIVE COMPUTER UNIVERSES” (more than I love Ted’s actual hypertext ideas, which is a whole damned lot) because I really do feel like we made a whole bunch of wrong turns.

Many times in history, we had the opportunity to do the right thing (“fight for the users”, build that “bicycle for the mind”\footnote{see \cite{bicycle}}) and instead we did the profitable or easy-to-imagine thing. Then, once we realized our mistake, there was a whole legacy locked into path dependence and a whole industry dedicated to ensuring we never forgot or corrected the mistakes of our youth.

Unix is the typical example given for the “worse is better” philosophy\footnote{see \cite{riseofwib}, \cite{wiw}, and \cite{wib}}, but it’s not a good candidate because Unix, for all its flaws, has the kind of respect for its users that leads it to trusting them with powerful and flexible tooling. Two better candidates are the web (and all of web tech) and the original Macintosh’s UI ideas.

The web is a master class in taking a bunch of already-flawed off-the-shelf technologies and hammering them into places they don’t belong without thinking about how they might be used in the future. The web standards are a Kafkaesque museum of technical debt. Using web tech is always something that seems like a good idea during a half-burnt-out all-nighter and then continues to make your life harder for the rest of your days, because that is how each element of it was developed. But, at least it has reified the capacity for regular people to make ugly hacks, which should be considered their right.

The Macintosh is the other side of the worse-is-better coin: they were so focused on making a rushed\footnote{see \cite{ninetyhours}, \cite{resourceman}, and \cite{realartistsship}} and underpowered\footnote{see \cite{werenothackers}} machine look polished and well designed that they decided to enumerate all the things that could be done on it and polish only those\footnote{see \cite{ornaments} and \cite{diagnosticport}}, removing the capacity for a user to actually treat it as an augmentation for their mind. A computer acts as an extension of one’s mental space, but the original Macintosh is the cognitive equivalent of one of those example rooms in an Ikea: it looks desirable from a distance, but the same six rooms exist in every store, none of the televisions have power cables, and all of the pages of all of the books in the bookshelf are blank. Such an inflexible environment can never become a home: it is too sterile. You can only create a home through forcibly changing the elements of your environment that clash with your desires (functional or aesthetic), and with the Macintosh and its successors (in the movement it inaugurated) you didn’t own the furniture in your own computer & didn’t have the right to modify it.

I engage in a kind of archaeology of computing’s forgotten experiments because in the end, almost all of the decisions we’ve collectively made have been bad ones: in terms of the state of computing, we live in the worst of all possible worlds, or close to it. Luckily, we have a power we rarely acknowledge: from a technical perspective, doing better is easier than continuing on our current path, so great is the friction from the mistakes of our youth. It is literally easier to rebel against the currents and create our own private utopian experiments than to continue downstream. Our primary antagonists are lack of imagination and ignorance of history — and fixing one will fix the other.

## Let's pretend this never happened

\marginnote{This section is adapted from  \cite{letspretend}. Thanks to Denise Long at Medium for editing the featured version of this post.}

My current desktop wallpaper is a photograph of the Xerox Alto (one of the machines covered in this book) with the words "Let's pretend this never happened" written across it. If I were to make it again today, it would instead be a photograph of the original Macintosh. After all, the mistakes that ruined GUIs forever — the rejection of composition, the paternalism toward users, the emphasis on fragile inflexible demo-tier applications that look impressive in marketing material but aren’t usable outside extremely narrow & artificial situations — weren’t invented at PARC but imposed by fiat by Steve Jobs upon a team that already knew better than to think any of it was OK.

Nevertheless, this image acts as a kind of memento mori: a reminder that everything you consider solid — all this path-dependence — is a side effect of the decisions of people who aren’t substantially smarter than you are, and those decisions didn’t occur very long ago. In one sense, ‘the computer revolution is over’ because the period of exponential growth behind the tech ended 10 years ago. In another sense, it hasn’t begun: we have sheltered ourselves from the social and intellectual ramifications of computing. Documents are still simulations of paper, & capitalism still exists. We're living in the computing equivalent of the brief period when printing presses existed but used a faux-calligraphic font.

The WIMP paradigm is not the only way to do a UI. In fact, the unix command line is itself a UI, with its own idioms — distinct from other text-based UIs (such as the AS/400, ColorForth, or any programming language's REPL) — and a UI that can usefully inform other kinds of UIs (as we see with notebook interfaces, the appearance of autosuggest and autocomplete on phones, and the integration of history in chat applications).

What should ‘never have happened’ is the widespread adoption of a shallow, dysfunctional misrepresentation of PARC’s ideas. Our idea about what a GUI looks like (the standard WIMP policies of totally independent applications written by strangers, manifesting as a set of overlapping windows that are primarily interacted with through pointing at and clicking large icons) comes out of the Macintosh project -- an attempt to duplicate the look and feel of a short Alto demo (seen by a non-programmer with no technical skills) on an already-outdated machine in assembly language during a time and budget crunch. The technical limitations of an intentionally low-end machine being built in 1982 and a manager's assumptions about a brief demo have become the basis for every interface an average user uses during their lifetime -- one of two interface paradigms familiar to the average professional developer, thirty-five years later.

What's more is that this interface has paternalistic, user-hostile assumptions baked in: flexibility is treated as a complication too technical for non-programmers, so rigid single-use 'applications' are provided by developers, with no user-servicible parts inside. (As you will see from the systems we cover, it is possible to make interesting kinds of flexibility intuitive to users, and it is possible to make the transition between use of third party applications and programming brand new applications smooth and gradual enough to be accessible to users who identify as non-technical.)

“Worse is better” might have won, but it doesn’t need to keep winning. We can stop it. All we need is luck and courage — and if enough of us have courage, we won’t need the luck.

The most useful thing we can do, when trying to imagine a desirable future for computing, is to try to forget the entire Macintosh legacy and send our minds back to 1976 -- when people acknowledged that we hadn't yet found the single 'right way' to make user interfaces.


## What you should expect from this book

This book is, as its title suggests, a survey of alternatives. This is to say that it will briefly cover a number of systems that differ from the norm. Some of these will be old systems -- developed in the 1970s or earlier. Some of them will be very new systems. Some have several interesting ideas, and others will have only one. Most fit into one or more traditions or lineages -- themes that have gradually emerged in my research -- and I will try to emphasize these elements in covering each system, although I will also provide more abstract and theoretical descriptions in their own section.

I will be calling these 'systems' or 'interfaces' because their actual form varies. Some are full operating systems (like Menuet); others are operating environments that run on top of an existing host OS but provide their own way of doing things (like Squeak); still others were machines that can now be emulated (like the Canon Cat). I will try to avoid covering systems familiar to the average reader, except to compare them to less-familiar systems.

While many of these systems are historical & I provide some historical detail, this book is not primarily a history. This book is intended to be a monster manual for planning your own campaigns into the user interface borderlands -- an aid to populate the imagination. I have included a diagram of influence flows in UI space, including many of the systems I will cover, but the primary function of this diagram is to illustrate the scope of possible systems rather than to teach the reader the lineage of particular ideas. These lineages are disputed, and my diagram shows connections based on shared attributes that may well be the result of independent reinvention.

Reading about user interfaces can sometimes be as futile as dancing about architecture. I have taken pains to cover mostly systems that readers can use themselves -- either on modern hardware or in emulation. Resources (and, if necessary, instructions) for running the systems I cover are in an appendix. If my description of the operation of a system is unclear, actually using the system may illuminate it.

Because I am not covering systems I myself developed, I am standing on the shoulders of giants. I have listed the resources I have drawn on in footnotes, and provided a bibliography for further reading. The materials gathered in researching for this book are listed on my website (<a href="http://www.lord-enki.net">www.lord-enki.net</a>), in the section about this book. In some cases, readers might find these resources more clear.

Finally, I encourage excerpts of this book to be distributed for educational purposes. Writing this kind of material is not (and cannot be) a primarily money-making enterprise: I am writing this because I care deeply about opening up the conversation on these subjects, and everything else is of secondary importance.


## "Small computing" and the ingeniousness of the "nontechnical" end user

"DON'T MAKE ME THINK," cries the conventional UX designer -- ostensibly on behalf of "nontechnical" end users. He optimizes the "happy path" of applications so that the thing that corporate wants users to do is as frictionless as possible. The other side of this (the so-called "dark patterns") is implied: anything that corporate *doesn't* want users to do (but that must be at least theoretically possible, if only for legal reasons) is to be made difficult, complicated, awkward, requiring many planned steps.

Naturally, "nontechnical" end users circumvent dark patterns on a daily basis (since not doing so generally costs them money). "Nontechnical" users are often experts in some other field than computing, which itself may be highly technical (even if it is not culturally recognized as such -- like knitting); nearly all users, whether or not they can be called "technical experts" in any field, regularly imagine uses of computers that ought to be possible but for which there are no obvious paths toward. Users develop elaborate folklore about how they believe the computing applications they use work -- mental models built on direct experience -- and use these models to solve problems, often developing extremely complex and time-consuming procedures to perform tasks that no application developer has catered to. Developers sometimes see the edges of such procedures when the occur in the workplace -- think of the ever-important giant shared excel macro that exists in practically every company -- and typically interpret it as an attempt by amateurs to muscle into the domain of the professional developer. If he decides to help at all, he will typically try to figure out one of the many tasks that this hairball performs and then do it in a "professional" way (which is to say, with tools that none of his colleagues are familiar with) -- and even this will fail, because the hairball performs many tasks, and is being improved by many people, all of whom understand the tasks better than he does (even if they do not understand "professional" software development at all). In the typical case, though, these procedures do not manifest in anything so closely resembling code, and so the developer does not see it (though an IT professional may see these procedures, generally just as some external factor unknown to the user has caused them to no longer work).

While the "nontechnical" users are by no means professional programmers, in my view they certainly qualify as programmers. They are using an internal model of the machine's state to plan a series of operations by which to perform tasks. In a sense, they are feral programmers, since they haven't learned their manners (the complex set of cultural and social rules that constitute "professional" development practices), nor have they learned conventional languages for their programming. To me, this is perfectly fine: they have important work of their own to do, and should not need to beg permission of us (and memorize our arcane rites, which really only make sense for professional programming in groups in a particular kind of corporate environment & are a drag elsewhere) to access computers at all. Instead, since personal computers are intended *for* end users, we should make them cater to the needs and desires of feral programmers.

Some feral programmers are feral because they have been scared off: the "intro to python" book is too thick, or somebody made fun of them for being interested in visual basic, or for being the only girl in class, or they heard all the programmers being called geniuses & didn't feel like they could do something that even geniuses were having trouble with. Some are simply uninterested in computers per-se and are only interested in what computers can do for them. And some have recognized that their struggles in getting the computer to do the things that application developers don't care about has been a hard road (and that "software engineering" is much worse in roughly the same ways).

We developers are not so different. Most of us used to be feral programmers: figuring out awkward hacks, until we learned to do it better, first with some much-derided "beginner's language" and then more "professionally". Most of us sometimes still are: when we don't want to dive into a hairy codebase or learn a new stack, but we still need to get around some limitation. The only difference is that we bit the bullet and spent several years reading documentation and gaining mastery over a language that has been blessed for professional work -- that and about $40,000 a year.

For this reason, when discussing systems in this book, I do not embrace the distinction between developer and user. The systems I'm discussing have such a small user base that most users will be developers and most pieces of software will have been developed for personal use, and some of them are not usable at all without at least a casual understanding of some programming language, but if a reader of this book comes out of it thinking that it would be nice to make a new system that's just like one of these but "user friendly" (by way of having powerful features hidden or removed from the user's control entirely), he has not understood the promise of computing at all.


## "User friendliness" and some varieties of alternative directions

In researching this book, the systems I have found most interesting have generally had, as a component in their design, one or more items from a pool of recurring ideas. We can call these traditions or lineages -- and in some cases, the designers of these systems have been conscious of their history -- but in many cases, these are actually ideas that have been re-invented in slightly different forms; in some cases, ideas are credited to some earlier system in which they are so deeply buried that it makes more sense to say that they were truly reinvented.

The near-monoculture of "modern" systems hides within it both deep archaeological stratification and inner complexity; it is not that these systems are bereft of useful ideas per-se, but that we are so accustomed to them (in all their myriad details) that we cannot see the fragments of better-designed or more interesting systems peeking through.

Unix, now half a century old, is the basis for not only nearly all headless systems large enough to have memory protection but is also the origin of now-ubiquitous features like nested directories (brought into DOS by Microsoft, a Unix vendor, and missing at the time from CP/M); the lower layers of both major mobile OSes are Unix, as is Apple's Mac OS X, and the Windows NT kernel (which sits beneath every Windows release since 2001) was designed specifically to compete with Unix in the server market by bringing in features previously missing from "personal" computers. While Unix itself grew organically and developed many stratifications & internal inconsistencies, this book will be covering Plan 9, the OS developed after Unix by many of the same people & consciously understood by some of its major developers as embodying the most interesting ideas of Unix in a more pure form. X11 (and its clone, XOrg), the GUI layer that sits on top of most 'conventional' Unix systems, has a very different conceptual structure that has much in common with some models of distributed systems -- because it was intended as a distributed system -- and derives some of these ideas from the LISP machine tradition, which we will also cover.

Any GUI that you're liable to stumble across in the wild has its origins in Xerox PARC's Alto smalltalk environment -- and mostly by way of Apple's interpretation, itself smoothed down through two generations of heavy user experience studies. We will certainly cover smalltalk. The most interesting thing about the smalltalk environment to Apple in the early 1980s -- the promise of 'discoverability', wherein people who had never so much as seen a computer before could learn to perform basic tasks without reading a manual, purely through experimentation -- no longer seems interesting to us moderns, living in a world where toddlers interact confidently with smartphones and tablets, and we have long since recognized that tasks not anticipated by the designers of GUI software are rarely 'discoverable' (particularly if the designers, so confident in the results of their user studies, have thrown the user-facing documentation away or, worse net, never made any). Instead, the goals of PARC's own smalltalk team are more compelling: to make an environment totally mutable by the user, and invite the user (even a child) to steadily learn and grow with it. Such an environment is personal in some senses of the term: one's own computer, after a little while, becomes incomprehensible to anyone else, like the shorthand invented by twins who have grown up together.

Apple stripped one version of "personal computing" from the collective consciousness -- the abovementioned total mutability and accessibility that still characterizes smalltalk -- but, in the development of the Macintosh, they contributed something, too: the forked file or resource database model, which is still preserved somewhat in Mac OS X but is most recognizable in mobile OSes (where Android has extended the logic of the forked file into a new model of permissions better attuned to a single-user device). A forked file system supports an attitude on the UI layer that is task-first rather than data-first: a user decides to edit an image and is presented with all images, or with recent images. (Of course, by wasting some resources and developer time, any underlying system can be made to support any kind of UI.) At the same time, the forked file is somewhat anti-personal in that it treats file type as a proxy for application, as though application developers owned the files on their users' disks, and to this day the development and distribution of Macintosh and iOS applications is centrally controlled (so that ultimately, application power is an extension of Apple's own power, the same way that a feudal lord's hold on his peasantry was an extension of the king's power to give and take away titles); Android is only a little looser. The logic of the forked file is reflected (though not enforced) in the vendor- and application-specific directory structure found in Windows, wherein even resources that could be shared are duplicated for each vendor (justified as a kind of hedge against "dependency hell" in development), and while the slow death of commercial unixes and ascent of open source ones has encouraged a different kind of culture in the Unix world, 20-30 years ago these vendor-specific structures were commonplace there as well.

Forked files, vendor-specific application structures, and expensive developer licenses are all old news now. Anything running locally can be cracked by a sufficiently canny user -- even locked behind a trusted computing hypervisor. The new, trendy way to take power away from users is geographically: the actual logic of your application lives in a Unix system wrapped in a container, running in a VM owned by Amazon, and the user is charged a subscription fee (or shown advertisements) in exchange for access to a web page that is modified dozens of times per second in order to masquerade as a GUI application. Some of that subscription fee then gets sent to Amazon to pay for the container. When have users become the enemy?

Even though the systems I'm covering may make assumptions about the user (for instance, that he is an expert in x86 assembly language, or lisp, or forth, or smalltalk), these systems are all "user friendly" in the sense that they extend to the user a greater degree of control than is normal in the Unix tradition. This is not necessarily intentional; in some cases, it is a result of a focus on developers, or a lack of resources, or simply a disinterest in jumping through the hoops necessary to develop a multi-user preemptive multi-threading system with memory protection. But, over and over, developers involved in these systems have made explicit statements about this control and how it makes these systems "personal". The lineages I am following are all about being "personal" and "user friendly" in some sense:

* Cutting out unnecessary features is "user friendly" because it makes it easier for a user to fully understand and predict the system's behavior
* Homogenous systems are "user friendly" because a user can, by learning a single language, access, manipulate, and even re-write the entire system
* Dramatically consistent systems are "user friendly" because there are fewer concepts to learn
* Dramatically inconsistent or various systems are "user friendly" because they provide the user opportunity to exercise his own creativity
* Built-in documentation, suggestions, assistants, and guides are "user friendly" because they invite the user to adapt to the system
* The absence of security controls on personal computing systems is "user friendly" because the user is invited to adapt the system
* Terse languages do not waste the user's screen space or make him type too much; verbose languages invite him to play without reading the manual; constraint languages do the difficult work of designing an implementation for him; high-syntax languages allow a variety of visually-distinct and memorable strategies, while low-syntax languages require little study to jump into using

Our conventional systems -- Unix, Windows, Mac OS, iOS, and Android -- are amalgamations of elements of different traditions, developed under specific constraints. Unix, to which we owe so much, was developed with the expectation that one installation of Unix running on one machine would be used by dozens of untrusted users simultaneously via a remote link, with a trusted administrator employed to keep the users from using too many resources. The technologies built to support this assumed use case has been transplanted to Windows, Mac OS, and Android, and preserved on Unix containers -- even though most Windows and Mac OS machines and practically all Android machines are used by a single trusted user, and almost no Unix systems even have an administrator account anymore. Even Unix containers, which still mostly act as servers, do not generally support untrusted users directly, but are merely acting as a support system for a daemon by which untrusted users are provided an extremely locked-down service. Mac OS and Windows were originally developed with the assumption of a single trusted user, but were retrofitted with security features to support deployments in small company networks (and then, later, internet access).

Rather than develop a new system for a new set of circumstances, layers have been added on top of existing systems (or in some cases, the bottom layer has been replaced and additional layers have been added to mate the alien symbiote with its host). We preserve these systems as though they are the faith of our fathers (and in fact, some interesting and important cultural knowledge is embedded in how certain elements work), and rather than hardening applications (or better yet, developing applications in such a way that they do not need to be secure against users) we wrap an entire OS inside a container and stick it on another Unix (or sometimes, another container) the way that cell membranes carry a simulacrum of the primordial seas that held our prokaryote ancestors. But the resources consumed by this layering and duplication, though hidden from us, are real and truly consumed: the extra electricity consumed during the additional twenty or thirty milliseconds we have taken for a now-pointless unix facility to run is produced by hauling coal out of the earth and burning it, and even if we don't care about that, we're paying Amazon for this extra space and time too. We do all of this in order to extract money from the users -- money necessary to pay that Amazon bill, and to bribe the developers (who would rather be doing something interesting with their time) away from an early retirement spent on hobby projects.

It's true: if the norm was for systems to respect users as people, rather than treating them simultaneously as idiots and dangerous enemies, we developers would make less money. On the other hand, we would have such systems for ourselves.

