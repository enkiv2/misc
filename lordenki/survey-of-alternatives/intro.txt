# Introduction

"Ideas are always reusable, because they have been usable before[...] The history of ideas should never be continuous; it should be wary of resemblances, but also of descents and filiations; it should be content to mark the thresholds through which an idea passes, the journeys it takes that change its nature or object." - Giles Deluze and Felix Guattari, A Thousand Plateaus. \footnote{This is from Brian Massumi's translation.}

The book you have before you is in some ways a companion piece to my earlier essay collection, Big and Small Computing. You will not need to read that book in order to understand this one, but doing so will illuminate why I consider a book like this one necessary, and I hope that the two of them together will be worth more than either one alone. Where that book is three quarters manifesto and polemic, this one is intended to be three quarters concrete technical information. Where that book focuses on what computing could be, this one focuses on already-existing systems. What they have in common is a focus on alternatives: systems that deviate from nigh-omnipresent norms and challenge our expectations of how a computer needs to operate. This book is a bestiary of alien forms, positioned to infect you.

## What is a 'system'

This book has two titles. One is 'Interfaces and Systems', and the other is 'A Survey of Alternatives'. I have chosen the term 'system' because I was unable to find a better single term for what I am trying to describe, however, the term is already loaded with meaning from centuries of use. While I am influenced in my analysis by the early cyberneticists, I am not using 'system' in the way that cybernetics or systems theory uses it. Instead, for the purposes of this book, I will be using the following definition:

\begin{quote}
A system is a complete and internally consistent way of thinking about computing. An interface is a way to interact with a computer that uses a system as its underlying structure.
\end{quote}

In other words, systems and interfaces (usually) come in pairs. Iverson Notation is a system that, after a decade or so of use, gained an interface called APL; forth is a system but a particular running forth implementation is (generally) an interface. At the same time, while interfaces are always concrete and instantiated, systems can have various degrees of abstraction: both 'object orientation' and 'smalltalk' are systems, and 'smalltalk' is a concrete, detailed, and specific version of the 'object orientation' system that differs from (say) 'self', 'java', 'unicon', or 'bless': the 'object orientation' system merely declares that computing is done via many small self-contained 'objects' that interact, without specifying things like class/object divisions, inheritance, or access control; 'forth' is a more concrete instantiation of the 'stack language' system, which merely declares that computing is done by manipulating values on a stack; 'Windows 95' and 'Mac OS 9' are two systems that are more concrete versions of the more general system 'WIMP GUI', which only declares that a computer contains overlapping windows that themselves contain widgets such as drop-down menus, icons, and buttons, all of which should be manipulated by a pointing device. The more concrete a system, the more it implies its own interface, and so we can say that every interface has a system that corresponds directly to it -- a conceptual model with every bug documented as a feature.

The thing about this system-interface pairing is that because an interface defines the parts of the dynamics of an underlying model that are accessible (and how accessible they are), the interface functionally defines the system. Features that exist internally to the implementation but are not accessible from the interface and do not impact the percieved behavior of the system are not "part of the system". Furthermore, idioms and other purely socially constructed constraints are also part of the system, to the extent that they are socially enforced. Systems define the landscape of the adjacent-possible, and therefore they define what is easily imaginable; since interfaces define systems, interfaces control not only what we can do but what we can plan. We "think outside the system" by thinking in another system, where things that are difficult or awkward here become easy. To think in another system usually involves experience with an interface that defines that system (and systems become alien in the same way living creatures do: incremental change, with occasional dramatic random mutations or horizontal transfers).

As software developers, in both our personal and professional use of computers, while we interact with a variety of interfaces, almost all of these interfaces share the same broad underlying systems. Certain systems 'won', due to specific historical conditions, and we have based new systems on those old systems, whether or not the conditions that selected them still apply. The dominant systems we interact with today -- WIMP GUIs, bourne-compatible shells in emulators of vt100 terminals, ALGOL-68 derived languages with class-based object models bolted onto the side, tree-structured markup, WYSIWYG rich-text editors -- are like the dodo before the arrival of explorers: thriving in an environment with no natural predators, but uniquely vulnerable to alien threats. I have mined history and the stranger corners of present experimentation in order to deliver some invasive species: alien systems (some are more-extreme versions of familiar forms, like introducing argentine ants where previously only carpenter ants had been; others are completely alien, like introducing flying squid into a population of birds) complete with interfaces that the reader can run at home in order to get hands-on experience.

## On alternatives

\marginnote{This section is adapted from  \cite{onalternatives}}

I love Ted Nelson’s concept of “ALTERNATIVE COMPUTER UNIVERSES” (more than I love Ted’s actual hypertext ideas, which is a whole damned lot) because I really do feel like we made a whole bunch of wrong turns.

Many times in history, we had the opportunity to do the right thing (“fight for the users”, build that “bicycle for the mind”\footnote{see \cite{bicycle}}) and instead we did the profitable or easy-to-imagine thing. Then, once we realized our mistake, there was a whole legacy locked into path dependence and a whole industry dedicated to ensuring we never forgot or corrected the mistakes of our youth.

Unix is the typical example given for the “worse is better” philosophy\footnote{see \cite{riseofwib}, \cite{wiw}, and \cite{wib}}, but it’s not a good candidate because Unix, for all its flaws, has the kind of respect for its users that leads it to trusting them with powerful and flexible tooling. Two better candidates are the web (and all of web tech) and the original Macintosh’s UI ideas.

The web is a master class in taking a bunch of already-flawed off-the-shelf technologies and hammering them into places they don’t belong without thinking about how they might be used in the future. The web standards are a Kafkaesque museum of technical debt. Using web tech is always something that seems like a good idea during a half-burnt-out all-nighter and then continues to make your life harder for the rest of your days, because that is how each element of it was developed. But, at least it has reified the capacity for regular people to make ugly hacks, which should be considered their right.

The Macintosh is the other side of the worse-is-better coin: they were so focused on making a rushed\footnote{see \cite{ninetyhours}, \cite{resourceman}, and \cite{realartistsship}} and underpowered\footnote{see \cite{werenothackers}} machine look polished and well designed that they decided to enumerate all the things that could be done on it and polish only those\footnote{see \cite{ornaments} and \cite{diagnosticport}}, removing the capacity for a user to actually treat it as an augmentation for their mind. A computer acts as an extension of one’s mental space, but the original Macintosh is the cognitive equivalent of one of those example rooms in an Ikea: it looks desirable from a distance, but the same six rooms exist in every store, none of the televisions have power cables, and all of the pages of all of the books in the bookshelf are blank. Such an inflexible environment can never become a home: it is too sterile. You can only create a home through forcibly changing the elements of your environment that clash with your desires (functional or aesthetic), and with the Macintosh and its successors (in the movement it inaugurated) you didn’t own the furniture in your own computer & didn’t have the right to modify it.

I engage in a kind of archaeology of computing’s forgotten experiments because in the end, almost all of the decisions we’ve collectively made have been bad ones: in terms of the state of computing, we live in the worst of all possible worlds, or close to it. Luckily, we have a power we rarely acknowledge: from a technical perspective, doing better is easier than continuing on our current path, so great is the friction from the mistakes of our youth. It is literally easier to rebel against the currents and create our own private utopian experiments than to continue downstream. Our primary antagonists are lack of imagination and ignorance of history — and fixing one will fix the other.

## Let's pretend this never happened

\marginnote{This section is adapted from  \cite{letspretend}. Thanks to Denise Long at Medium for editing the featured version of this post.}

My current desktop wallpaper is a photograph of the Xerox Alto (one of the machines covered in this book) with the words "Let's pretend this never happened" written across it. If I were to make it again today, it would instead be a photograph of the original Macintosh. After all, the mistakes that ruined GUIs forever — the rejection of composition, the paternalism toward users, the emphasis on fragile inflexible demo-tier applications that look impressive in marketing material but aren’t usable outside extremely narrow & artificial situations — weren’t invented at PARC but imposed by fiat by Steve Jobs upon a team that already knew better than to think any of it was OK.

Nevertheless, this image acts as a kind of memento mori: a reminder that everything you consider solid — all this path-dependence — is a side effect of the decisions of people who aren’t substantially smarter than you are, and those decisions didn’t occur very long ago. In one sense, ‘the computer revolution is over’ because the period of exponential growth behind the tech ended 10 years ago. In another sense, it hasn’t begun: we have sheltered ourselves from the social and intellectual ramifications of computing. Documents are still simulations of paper, & capitalism still exists. We're living in the computing equivalent of the brief period when printing presses existed but used a faux-calligraphic font.

The WIMP paradigm is not the only way to do a UI. In fact, the unix command line is itself a UI, with its own idioms — distinct from other text-based UIs (such as the AS/400, ColorForth, or any programming language's REPL) — and a UI that can usefully inform other kinds of UIs (as we see with notebook interfaces, the appearance of autosuggest and autocomplete on phones, and the integration of history in chat applications).

What should ‘never have happened’ is the widespread adoption of a shallow, dysfunctional misrepresentation of PARC’s ideas. Our idea about what a GUI looks like (the standard WIMP policies of totally independent applications written by strangers, manifesting as a set of overlapping windows that are primarily interacted with through pointing at and clicking large icons) comes out of the Macintosh project -- an attempt to duplicate the look and feel of a short Alto demo (seen by a non-programmer with no technical skills) on an already-outdated machine in assembly language during a time and budget crunch. The technical limitations of an intentionally low-end machine being built in 1982 and a manager's assumptions about a brief demo have become the basis for every interface an average user uses during their lifetime -- one of two interface paradigms familiar to the average professional developer, thirty-five years later.

What's more is that this interface has paternalistic, user-hostile assumptions baked in: flexibility is treated as a complication too technical for non-programmers, so rigid single-use 'applications' are provided by developers, with no user-servicible parts inside. (As you will see from the systems we cover, it is possible to make interesting kinds of flexibility intuitive to users, and it is possible to make the transition between use of third party applications and programming brand new applications smooth and gradual enough to be accessible to users who identify as non-technical.)

“Worse is better” might have won, but it doesn’t need to keep winning. We can stop it. All we need is luck and courage — and if enough of us have courage, we won’t need the luck.

The most useful thing we can do, when trying to imagine a desirable future for computing, is to try to forget the entire Macintosh legacy and send our minds back to 1976 -- when people acknowledged that we hadn't yet found the single 'right way' to make user interfaces.

## What you should expect from this book

This book is, as its title suggests, a survey of alternatives. This is to say that it will briefly cover a number of systems that differ from the norm. Some of these will be old systems -- developed in the 1970s or earlier. Some of them will be very new systems. Some have several interesting ideas, and others will have only one. Most fit into one or more traditions or lineages -- themes that have gradually emerged in my research -- and I will try to emphasize these elements in covering each system, although I will also provide more abstract and theoretical descriptions in their own section.

I will be calling these 'systems' or 'interfaces' because their actual form varies. Some are full operating systems (like Menuet); others are operating environments that run on top of an existing host OS but provide their own way of doing things (like Squeak); still others were machines that can now be emulated (like the Canon Cat). I will try to avoid covering systems familiar to the average reader, except to compare them to less-familiar systems.

While many of these systems are historical & I provide some historical detail, this book is not primarily a history. This book is intended to be a monster manual for planning your own campaigns into the user interface borderlands -- an aid to populate the imagination. I have included a diagram of influence flows in UI space, including many of the systems I will cover, but the primary function of this diagram is to illustrate the scope of possible systems rather than to teach the reader the lineage of particular ideas. These lineages are disputed, and my diagram shows connections based on shared attributes that may well be the result of independent reinvention.

Reading about user interfaces can sometimes be as futile as dancing about architecture. I have taken pains to cover mostly systems that readers can use themselves -- either on modern hardware or in emulation. Resources (and, if necessary, instructions) for running the systems I cover are in an appendix. If my description of the operation of a system is unclear, actually using the system may illuminate it.

Because I am not covering systems I myself developed, I am standing on the shoulders of giants. I have listed the resources I have drawn on in footnotes, and provided a bibliography for further reading. The materials gathered in researching for this book are listed on my website (<a href="http://www.lord-enki.net">www.lord-enki.net</a>), in the section about this book. In some cases, readers might find these resources more clear.

Finally, I encourage excerpts of this book to be distributed for educational purposes. Writing this kind of material is not (and cannot be) a primarily money-making enterprise: I am writing this because I care deeply about opening up the conversation on these subjects, and everything else is of secondary importance.

