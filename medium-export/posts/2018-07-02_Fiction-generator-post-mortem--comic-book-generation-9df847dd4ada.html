<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Fiction generator post-mortem: comic book generation</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Fiction generator post-mortem: comic book generation</h1>
</header>
<section data-field="subtitle" class="p-summary">
One of the open problems in the procedural generation of fiction is how to maintain reader interest at scale. Fiction generation is very…
</section>
<section data-field="body" class="e-content">
<section name="45bc" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4878" id="4878" class="graf graf--h3 graf--leading graf--title">Fiction generator post-mortem: comic book generation</h3><p name="ba19" id="ba19" class="graf graf--p graf-after--h3">One of the open problems in the procedural generation of fiction is how to maintain reader interest at scale. Fiction generation is very good at producing short-form work that makes sense and is evocative, but when existing techniques for it are scaled up, works start to seem repetitive (showing the limits of their templates or corpora), meandering (showing failure to adhere to a large-scale pattern the way that they adhere to small-scale patterns), or overly inaccessible (because the generator, unable to determine how much information is ideal, has erred on the side of being too dense — becoming boring from inaccessibility rather than from repetition).</p><p name="3d8a" id="3d8a" class="graf graf--p graf-after--p">My favorite way to attack this problem is to make use of the Eliza Effect: people tend to be more willing to put up with flaws in generated output if those flaws are given context that would excuse or explain them if produced by humans. Because of this, computer-generated poetry is a lot more readable than computer-generated prose — the onus is much more heavily on readers to make sense of poetry than on authors to ensure their poetry is easily understood. There are prose forms that are associated with experimentation and vagueness, and mimicing these forms can make it easier for readers to put up with machine-generated prose.</p><p name="832c" id="832c" class="graf graf--p graf-after--p">In 2014, <a href="https://github.com/dariusk/NaNoGenMo-2014/issues/70" data-href="https://github.com/dariusk/NaNoGenMo-2014/issues/70" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Greg Borenstein wrote a comic generator called Generative Detective,</a> producing comics from public-domain detective novels &amp; creative commons licensed Flickr images. He used a semi-manual process, tagging sentences with keywords which he then used to search for associated images. The result: a surreal but compelling comic composed of juxtapositions.</p><figure name="b4bc" id="b4bc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Rl8BKLa46rEG1xCBEPGr6w.jpeg" data-width="500" data-height="375" src="https://cdn-images-1.medium.com/max/800/1*Rl8BKLa46rEG1xCBEPGr6w.jpeg"><figcaption class="imageCaption">The first frame of the first page of <a href="http://gregborenstein.com/comics/generated_detective/1/" data-href="http://gregborenstein.com/comics/generated_detective/1/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Generated Detective</a></figcaption></figure><p name="5e2d" id="5e2d" class="graf graf--p graf-after--figure">Initially, the panel layout was simple, but it became slightly more complex with <a href="http://gregborenstein.com/comics/generated_detective/4/" data-href="http://gregborenstein.com/comics/generated_detective/4/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">page 4,</a> which introduced the use of rows of panels.</p><p name="a68f" id="a68f" class="graf graf--p graf-after--p">The entire thing is worth reading. Once again, as with earlier NaNoGenMo entries that experimented with piggybacking off existing forms, the illusion of an unintended narrative emerged, phantasmagorical, from the ruins of theme and style.</p><p name="992a" id="992a" class="graf graf--p graf-after--p">At the time, I asked how far this could be taken. Juxtapositions often worked best when the image was a poor match for the keywords, rather than a good one. The long history of experimental comics, for those familiar with them, invited readers to project narrative onto these pages quite intensely. So, I wondered: could we get more bang for our buck with less work by dropping the use of keywords, dropping flickr searches, and simply associating arbitrary images with arbitrary pieces of text?</p><p name="4d0e" id="4d0e" class="graf graf--p graf-after--p">In 2017, I answered my own question with an unrelated project. I wrote <a href="https://github.com/enkiv2/misc/blob/master/barbara_holzer.py" data-href="https://github.com/enkiv2/misc/blob/master/barbara_holzer.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">a juxtaposition machine called Holzertron</a>: a script that combined the work of the feminist experimental artists Barbara Kreuger (best known for superimposing slogans, in white Helvetical on a red stripe, on top of black and white photographs) and Jenny Holzer (best known for projecting slogans on buildings). I desaturated random photographs, then superimposed random slogans taken from a database of those used by Holzer, formatted in the style of Kreuger’s work, and posted the result on twitter automatically with the tags #barbaraholzer and #jennykreuger.</p><p name="fa8d" id="fa8d" class="graf graf--p graf-after--p">The resulting images were often arresting, and many became popular. Of course, the juxtaposition was entirely random. Non-resonant images were ignored; resonant ones got retweeted extensively.</p><p name="9cd1" id="9cd1" class="graf graf--p graf-after--p">Having only rarely worked with image processing before, this project gave me the confidence to attempt my own take on Greg’s work, and answer the questions he hadn’t addressed. At the same time, I decided to mimic attributes of comics that Greg’s system didn’t duplicate, like the use of color and complex panel layouts. This was <a href="https://github.com/NaNoGenMo/2017/issues/54" data-href="https://github.com/NaNoGenMo/2017/issues/54" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">one</a> of <a href="https://github.com/NaNoGenMo/2017/issues/created_by/enkiv2" data-href="https://github.com/NaNoGenMo/2017/issues/created_by/enkiv2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">several projects</a> I did for <a href="https://github.com/NaNoGenMo/2017" data-href="https://github.com/NaNoGenMo/2017" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">NaNoGenMo 2017</a>.</p><p name="d9f7" id="d9f7" class="graf graf--p graf-after--p">In high-profile manga, because of the details of manga distribution, it is often the case that multiple coloring &amp; shading styles will be used: full-color with outlines for the first few pages, followed by a few pages of less distinct watercolor work, followed by outlines + smooth shading for most of the remainder of the tankoban &amp; unshaded outlines for omake and sketches in the back. <a href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/sketchify.py#L6" data-href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/sketchify.py#L6" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">I decided to write filters for each of these stages</a>.</p><p name="ec57" id="ec57" class="graf graf--p graf-after--p">First, I create a ‘sketch’ by finding the edges in a copy of the image that has had its contrast adjusted. These are the outlines. If I am producing an outline-only version, I simply return this image; otherwise, I produce either a posterized (for color) or greyscale copy and blend the outline with this second layer.</p><figure name="3781" id="3781" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BdcTykXgbPgSC7jcts5POw.png" data-width="2556" data-height="720" src="https://cdn-images-1.medium.com/max/800/1*BdcTykXgbPgSC7jcts5POw.png"><figcaption class="imageCaption">Posterization &amp; outline blending for an already-drawn image (a screenshot from Studio SHAFT’s Denpa Onna To Seishun Otoko (2011))</figcaption></figure><figure name="6212" id="6212" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*bYQzhj1YR0ylT6MlfjFjiA.png" data-width="1613" data-height="362" src="https://cdn-images-1.medium.com/max/800/1*bYQzhj1YR0ylT6MlfjFjiA.png"><figcaption class="imageCaption">Posterization &amp; outline blending for a photograph (a screenshot from John Carpenter’s Prince of Darkness)</figcaption></figure><p name="5a7b" id="5a7b" class="graf graf--p graf-after--figure">The resulting images were not necessarily reliably reminiscent of traditionally-drawn comics (and I didn’t make an effort to simulate hatching-based shading as Greg did), but they reminded me of Dave McKean’s digital work on Millenium, and this satisfied me, since I wanted to evoke more experimental works.</p><p name="ebb3" id="ebb3" class="graf graf--p graf-after--p">I was also able to get interesting panel layouts with <a href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/panel_layout.py#L92" data-href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/panel_layout.py#L92" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">a simple algorithm</a>. I wanted to pick random images &amp; make them align but maintain their aspect ratio as much as possible, so I decided on a few rules: I would only shrink the images (never growing them), and only stretch an image if the difference between its current size and its stretched size would be smaller than the size of the gutter (in other words, small enough that I couldn’t fit another image in the left-over space). Given these rules, I simply laid out images from left to right and from top to bottom, shrinking each image so that it fit within square-shaped leftover regions of space along both dimensions. The result was a page with a mix of tall columns and smaller square panels arranged in rows — a structure interesting but familiar to comic readers, varied but with a clear sense of reading order.</p><p name="d616" id="d616" class="graf graf--p graf-after--p">Once an image had its target position and size, <a href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/sketchify.py#L19" data-href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/sketchify.py#L19" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">I shrank it to that size, applied the appropriate filter chain, drew a black box around it, then shrank it again and placed it on a white background — producing gutters through the white border aorund each frame</a>.</p><figure name="c08c" id="c08c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*W61kmRBYBRP52c-FV74ZDg.png" data-width="720" data-height="1080" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*W61kmRBYBRP52c-FV74ZDg.png"><figcaption class="imageCaption">A laid-out page without text. These images were chosen randomly a folder on my machine.</figcaption></figure><p name="ee82" id="ee82" class="graf graf--p graf-after--figure">Text layout was done in the most straightforward way possible: I aligned the upper-left-hand corner of each text block with the upper left hand corner of any panel, at an offset. I used black text in a yellow box — the standard Marvel convention for narration.</p><p name="aa83" id="aa83" class="graf graf--p graf-after--p"><a href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/panel_layout.py#L149" data-href="https://github.com/enkiv2/misc/blob/3099c05bac98058ba693261e4aefc3edca0c7cc5/nanogenmo-2017/panel_layout.py#L149" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The comic generator is set up as a module that takes a set of lines of text and applies them to generated comic pages, exiting when the book has gotten 100 pages or exhausted all input text.</a></p><p name="ab03" id="ab03" class="graf graf--p graf-after--p">I generated several comics: one with <a href="https://github.com/enkiv2/misc/blob/master/nanogenmo-2017/comic.pdf" data-href="https://github.com/enkiv2/misc/blob/master/nanogenmo-2017/comic.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">scrambled Raymond Chandler works</a>, one with <a href="https://github.com/enkiv2/misc/blob/master/nanogenmo-2017/holzercomic.pdf" data-href="https://github.com/enkiv2/misc/blob/master/nanogenmo-2017/holzercomic.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Jenny Holzer slogans</a>, and one with <a href="https://github.com/enkiv2/misc/blob/master/nanogenmo-2017/dad-joke-apocalypse.pdf" data-href="https://github.com/enkiv2/misc/blob/master/nanogenmo-2017/dad-joke-apocalypse.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">a database of ‘dad jokes’</a>. I think the ‘dad joke’ version is the most effective, since the jokes, when juxtaposed with largely technical or imposing imagery, produce an uneasy feeling — the result borders on experimental horror.</p><p name="7e0f" id="7e0f" class="graf graf--p graf-after--p">Actual reader review: “It feels like someone injected laundry detergent into my eyes and forced me to scroll through Google Image Search.”</p><p name="cc5c" id="cc5c" class="graf graf--p graf-after--p graf--trailing">The biggest problem with this output is actually that the lines tended to be too long — going off the page, or off the end of the panel, or obscuring the panel entirely. This is not a problem with the comic generator per-se (though I could reject assignment of lines to panels that are too small for them), and the appropriate solution is probably to use small, fragmentary phrases or break lines into small fragments before passing them into the comic generator.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@enkiv2" class="p-author h-card">John Ohno</a> on <a href="https://medium.com/p/9df847dd4ada"><time class="dt-published" datetime="2018-07-02T15:11:26.997Z">July 2, 2018</time></a>.</p><p><a href="https://medium.com/@enkiv2/fiction-generator-post-mortem-comic-book-generation-9df847dd4ada" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on March 29, 2021.</p></footer></article></body></html>