Initially, my machines were named after classical philosophers and mathematicians (starting with Euclid, after the computer in Pi, but then proceeding to Plato, Aristotle, and Archimedes). However, Plato & Aristotle died almost simultaneously, and I switched to a non-classical schema focusing on mathematicians with Descartes, Liebniz, and Hume. Currently, my network consists of: Vonneumann (the server), Erdos (my work machine), Hilbert (my smartphone — because Hilbert is associated with both infinite hotels and space-filling curves), and Agrippa (the chromecast). Retired names include Godel (the PDA), Cantor (a now-dead server), and Conway (a laptop, named after John Conway of Game of Life fame).
Against certain naive varieties of transhumanist sentiment
A frequent idea that I run across when speaking to technophiles with transhumanist leanings these days is the superiority of tech over biology. People will say, “I’ll upload my brain and live forever”, or “I’ll replace my arm so that I can be stronger”, or “I’ll get a wetwire to the internet so I can read faster”. This isn’t a new idea; I said variations on the same thing when I was fifteen. But, it’s absolutely stupid.
We have never built a machine with a lifespan and resilience comparable to a human being. Machine failure rates fall along a bathtub curve, but while an expected human lifespan is betweenseventy and eighty years these days, it’s the rare freak of a machine that still functions after ten or twenty years — let along thirty, let alone continuously. Biological systems have insanely complex self-repair and self-maintenance systems, and the reason we live for eighty yearsis that our parts are continuously being maintained, rather than undergoing routine maintenance on a human scale of weekly-monthly-yearly. The very first programmable electromechanical computers were built in the 30s (depending on who you ask and how you define it, you can push it forward or back about ten years), meaning that a human being living an average lifetime that was born at the same moment as the very first programmable computer in the modern sense would be dying *right now*; drum and disk storage is twenty years younger(along with transistors, ram that’s not based on relays/mercury tubes/ CRTs, programming languages other than machine code, and interactive terminals), and the internet is about fifteen years younger than *that* (along with email, pipes, directories, and asymmetric key cryptography). Someone born at the moment the first packet was sent over the internet would be middle-aged. Nevertheless, all these systems have changed drastically many times over the course of their lifetime, in incompatible ways. All of the component parts have been replaced many times over. At various points in the past, all these systems have had *complete* failures (yes, including the internet). These systems are not at the point where they could be expected to safeguard the lifetime of a rat, let alone extend the lifetime of a human being.
Likewise, with prosthetic arms. Prosthetic arms are great — for people who are missing their arms. Cochlear implants aren’t competing with people’s real ears; they’re competing with being deaf. The prosthetic eyes that have finally gotten FDA approval have approximately the same resolution as a TRS-80 Model 100  — they aren’t competing with real eyes, but with total blindness.
Wetwires are in an even worse position. The current state of the art in brain implants can, with incredibly invasive and dangerous brain surgery, temporarily hook your brain up to 200 I/O lines, each of which neurons *might* decide to grow on. Wetwires are competing with reading: a four thousand year old technology that’s constantly being improved upon, that takes advantage of the human eye and optic nerve — a pipe so fat that the eye is considered to be a part of the brain, a pipe so fat that the eye does complex processing independently of the visual cortex and can independently trigger signals to the amygdala about emotionally striking scenes before the visual cortex can even receive the image data. Furthermore, reading is a technology that the government of every developed nation spends huge amounts of money on installing into its citizens! Wetwires can’t compete with that.
That said, this isn’t the end of transhumanism, or even of grinding. Implants aren’t going to go away. It’s just that we aren’t looking at them correctly.
Implants are a *long* way away from replacing the things that human beings already do well, like living and thinking and reading and moving. Generally speaking, to the extent that it’s trivial to do so, when there’s a problem with scale, we invent an external technology to handle it — when we need to turn bolts, we build wrenches that fit in our hands instead of hacking our arms off and replacing them with wrenches. If we depend upon implant tech (and other varieties of transhuman tech) to fund itself by being an improvement over what humans already are capable of doing, then the whole field will go bankrupt. But, there aretwo fields in which this kind of tech can excel. One is performing aworse job at than the human body at tasks that the human body already does — prosthetics for people with missing limbs, and replacement parts for people whose parts are faulty or missing. The other is allowing human beings to do things they’ve never done before — not by increasing scale, but by qualitative change.
The cochlear implant kind of sucks. When it’s installed, wires are stuck to the cochlea — a snail-shaped fluid-filled organ in the inner ear that does the heavy lifting in hearing (the rest of the ear is basically involved in amplification and protection). In normal hearing, vibrations from outside the ear are amplified by a set of bones that operate like a cam assembly, before pressing on a flexible membrane on the big end of the cochlea, and the movement of tiny hairs inside the cochlea produces the perception of sound, with the position of the hairs that are most stimulated determined by the frequency of the sound. In a cochlear implant, the wires cause the hairs to be stimulated directly, with galvanism, and so the number of wires installed corresponds to the resolution of sound available. We do not have the technology to produce CD-quality sound. We don’t even have the technology to produce speak-and-spell-quality sound. People with cochlear implants are stuck trying to decode speech based on fewer distinct frequencies than there are bars on a child’s xylophone. But the cochlear implant, as an accident of its nature, has one improvement over the ear — it has a built-in headphone jack. Cochlear implant-like technologies are far from being an improvement over ears, but when combined with throat mics or other mechanisms for producing the fundamental elements of speech from subvocalizations, they might be an improvement over the walkie-talkie. At the point at which this technology has enough demand to make people voluntarily submit to brain surgery, I expect that this is exactly how it will be used (and I expect the first market to be military or paramilitary — people who, on a life or death basis, need to communicate without using their hands and without being heard by other people nearby).
There’s another trend going on, as well. Just as desktops became laptops and laptops became smartphones, smartphones are on the cusp of becoming wearables, and wearables will become implants.
However, this change-over is very rarely quick, and even more rarely complete. Before desktops, we had minicomputers, and before minicomputers, mainframes; however, minicomputers are not quite gone (IBM still sells machines running z/ OS, although most of the market is dying), and desktops are hardly going anywhere.
Every few years, the entire tech industry pulls out twenty-year-old project from the MIT Media Lab or CMU or PARC or somewhere and collectively decides to shit its pants over it. Recently, we’ve been hitting a quadruple-whammy: wearable computers, the Internet of Things, 3d printing, and virtual reality.
The current wearable computer boom started with Google Glass taking pretty much equally from the work that Thad Starner and Steve Mann were doing in the early 90s; appropriately, Starner was brought onto the Glass project, while Mann was completely uncredited despite the fact that they took the name from him. And, despite the fact that Glass was a complete PR disaster, Google definitely decided what parts of Starner’s work to borrow with an eye toward PR — Starner’s most interesting idea, subliminal reminders, was omitted from Glass and the Glass UI standards and Glass UI frameworks were written in such away that subliminal reminders should be completely impossible. Now, in an almost hilariously ironic turn of events, Microsoft has taken essentially exactly the same technology, made it steroscopic, reframed it in terms of geolocation-centric AR (something Glass was never going to be capable of doing, by design), and turned it into a massive PR success.
In comparison, the current Internet of Things boom seems to be driven entirely by industry-wide amnesia. That’s not entirely unfair, since the industry has, until now, had a very hard time figuring out what to call it. The current term of art is the Internet of Things, but from around 1995 to around 2005, everybody was calling it Ubiquitous Computing. The IoT is hitting a lot of the same media roadblocks as VR did in the early90s, which makes me think that it’s probably around the same point in the hype cycle, although technologically, it’s definitely further along.
Ten years ago, when I was an unemployed teenager, I had two big projects that were lighting up my eyes. One of them was a wearable computer project. The other was a UbiComp project — what you’d now call the Internet of Things. At the time, the wearable computer project was by far less feasible; displays were expensive, cpus were expensive, making either run off a battery and getting the thing small enough and light enough to fit on your body meant lowering its capabilities to an extreme. I designed several prototype wearable computers around the AT90S8515 — an 8-bit microcontroller that cost$10 and had 127 bytes of ram — and various LED-based displays, but it was clear that unless I was willing to either buy thousand-dollar equipment or strap a laptop to my back and make due with The First Church of Space JesusThe First Church of Space Jesusaudio cues as an interface, wearable computers were really infeasible. (I ended up strapping a laptop to my back and using audio cues, in the end.) The UbiComp project, on the other hand, was completely within the realm of possibility — I had a working prototype for a system for communal cooperative use of a single computer, based on identifier tokens stored on a cheap wiimote knockoff that doubled as an input device; the cost of the system was the cost of a random desktop computer, a projector, and a $20 wiimote knockoff. If I had had steady disposable income, I could have formed a corporation and finished my prototype and become yet another failed IoT startup — the technology was there, solid, and absolutely trivial.
Today, IoT is even easier. My potentially-$300 [≈ cost of PS3 gaming system, 2011] computer could be replaced with a $20 raspberry pi. Wiimote knockoffs don’t even cost $20 anymore. The projector costs more than the rest of the system in total, and my homebrewed account-sharing system could be replaced with the kind of cloud-based thing that newbies whip up in minutes and brag about on hacker news. A couple years ago, I did a wearable computer, too — with about $350worth of parts (a raspberry pi, a twiddler, a $100 head mounted display, and a USB battery pack), I built something that, while not comparable in usability to a laptop, beat the pants off the absolute best I could do with that kind of money in 2005 — mostly because of economies of scale provided by the popularity of smartphones. PDAs manufactured in 2005 couldn’t really run 800x600 color VGA, or even 300x200 color VGA — too slow. (Maybe you could do it if you were especially clever. I wasn’t clever enough to make up for my lack of riches — wagering the cost of disassembling an expensive PDA on my ability to make it drive a display was too rich for my blood.) A single-board computer capable of running Linux in 2005 was a fucking high-end single-board computer. But, the iPhone came out — a single board computer running BSD shoved into a PDA — then the Android phones started appearing a couple years later — cheaper single board computers running Linux and Java shoved into PDAs. Now the chips that run Linux in smartphones are cheap enough that Texas Instruments will give away a handful of free samples to anybody with a university-affiliated email address, complete with specialized circuitry for fast video decoding. Single board computers running Linux can be sold for $20 and make enoughmoney to prop-up a non-profit organization. Meanwhile, some nerds figured out that a series of cheap wifi chips could be reflashed, andnow you can buy complete postage-stamp-sized wifi-enabled systems that can run Lua for $5.
So, we’re at the point now where you can stick the guts of a smartphone on the side of your head and have a head-mounted smartphone with a battery life of about two hours, or you can stick the guts of your smartphone on your wrist and have a smartphone with a battery life of about a day if you barely ever have the screen on. Or, you can stick the guts of a smartphone in your pocket and stick a screen on your head, and actually have a reasonable battery life with reasonable usage. We aren’t at the point where we can start making fully wearable never-take-em-off computers with reasonable battery life and reasonable capability, although I think that if we take a page out of the MIT Media Lab book and combine this with IoT, we might be able to make due with what we have for a little longer. This has problems — centralized IoT is the domain of natural monopolies, with most of them fated to go the way of AppleTalk (although centralized IoT is all the rage now, with every consortium of manufacturers competing to make their own incompatible standards on the off chance that theirs will be the one to take off); meanwhile, decentralized IoT is the stuff of IT nightmares, where failures in logistics and/or security can lead to a lightbulb DDoSing your house and/or the white house. My own design, which was based on a federated model with an open protocol and a market for competing vendors, has unfortunately been obviated by time — it was based on the assumption that the normal use would be an evolution of the cyber-cafe, and it probably would have worked in 2005, but no longer makes sense in the same universe as widespread smartphone ownership and devices like chromecast. Offloading computing from wearables onto IoT nodes will require an extreme of either security or naivete — and because security is complicated, I fully expect a future hellworld of incredibly insecure wearable/IoT mesh networking comparable to the amazing terror of running Windows 9x on the internet in the 90s. Welcome back to an era where anybody with a modicum of knowledge can remote control your computer and nobody can patch it for five years; except this time, the computer is strapped to your face.
This is a problem that *must* be solved before the wearables become implantables. Implants need to be smaller than wearables. Right now, the state of medical device security is pretty low — while medical device software, along with airplane control software and nuclear power plant software, has higher quality standards under normal operating conditions, it’s largely no better than normal consumer-grade software when it comes to resisting actual planned attacks, and sometimes worse. We already have computers in all sorts of things — horrible, insecure computers; our airplanes can be hijacked through the in-flight wifi network, our cars can be hijacked through the CD player, our pacemakers can be remote-controlled over wifi, and our routers are already sustaining self-replicating botnets. When these devices are on our bodies, the threats become more visible; when they are in our bodies, they become potentially fatal — not necessarily because of malice (it takes a special kind of person to actually shut down somebody’s heart by exploiting their pacemaker) but because of incompetence (it doesn’t particularly take a special kind of person to try to make a botnet out of every exploitable wifi-enabled device, including pacemakers, and then not check available memory and crash the pacemakers because he’s just written to an address that doesn’t exist).
Implants are coming, and wearables are coming first. Implants will come both faster and slower than we expect, because they won’t be used how we expect. They won’t make us live longer or read faster, but instead will let us do things we haven’t imagined yet. Let’s fix our shit before we’ve got buffer overflow vulnerabilities that’ll take actual brain surgery to patch.
This article was originally published at The First Church of Space Jesus.
Failure modes of science fiction
Failure modes of science fiction
There are several popular ways to look at science fiction as a genre. I have my own preferences. That said, the major opposing perspective — what I’d term the ‘machine-lit’ school of thought — has its merits, insomuch as it highlights a set of common tendencies in science fiction. I’d like to take this space to highlight the basic premise of machine-lit, the tendencies it breeds, and why I find most machine-lit to be relatively uninteresting.
(The third major perspective, what I call the spaceship-on-the-cover style, I find wholly uninteresting and is the subject of other essays; however, this perspective is becoming historically important lately because of some drama surrounding the Hugo awards being gamed by groups who prefer this style, so it’s worth mentioning in passing.)
Machine-lit is, in a general sense, the construction of a narrative around a concept invented by the author, as a capsule intended to introduce the reader to the concept. Lots of early science fiction is machine-lit for actual machines (Ralph 124C41+ being an ideal example of how this can go wrong yet still be very influential). The works of Ayn Rand are machine-lit for the Objectivist philosophy. Big-idea science fiction novels tend to be machine-lit for the ideas they represent.
One failure mode of machine-lit is that, because the narrative is intended as a delivery mechanism for the concepts, the narrative can itself be weak or nearly nonexistent if the author thinks the ideas themselves are interesting enough. (Ayn Rand, again, and Gernsback, again — but also major dystopian novels like Zamatayin’s We and 1984). Likewise, style can be a major issue in machine-lit, with The Unincorporated Man’s borderline-fanfic-quality-prose depending upon its intended audience of libertarians to forgive lack of technical skill in writing because the ideas are sufficiently in-line with the ideology, and PKD’s writing so heavily leaning on the ideas (not to mention the amphetamines) to pull it through (outside of rare stylistically-polished books like A Scanner Darkly).
There are definitely instances where books intended as machine-lit end up having well-developed plot and characters and a coherent and polished writing style (pretty much every Neal Stephenson book meets these criteria, as does Brave New World), but to some extent, doing so depends upon a kind of imagination and intellectual honesty that brings the book into the middle-ground between machine-lit and the world-building-based style of science fiction that I tend to champion, whose most extreme and visible example is seen in the post-Neuromancer works of William Gibson.
Another major failure mode of machine-lit is that, because of the dependence upon the central conceit of the book, if that conceit is uninteresting or unoriginal, the book as a whole fails along with it. With big-idea novels related to politics (Rand again) or philosophy (a handful of PKD books that lean too heavily on solipsism or philosophical zombies, and nearly every film adaptation of a PKD work), interest in these works falls evenly along either political-ideological or philosophical-education lines — a communist is, largely, going to find The Fountainhead or Anthem uninteresting; someone who is familiarenough with the idea of solipsism to find it fairly uninteresting will likewise find The Matrix uninteresting, while someone who rejects Serle’s Chinese Room paradox and the idea of philosophical zombies as based on an erroneous deification of consciousness will find the host of films about robots being incapable of emotion or of morality to be uninteresting. When the same idea is recycled into dozens of machine-lit works, the popularity of the idea itself can suffer, because while no longer wholly novel it will often be framed in similar ways, with similar changes based on the needs of the story or premise, by nearly identical stories (The Matrix has more in common with Simula-3 and its major film adaptations, World on a Wire and The Thirteenth Floor, than it does with Plato’s Allegory of the Cave, from which all of them were derived). Today, talking about solipsism will make people think of The Matrix rather than, say, Descartes’ “evil genius” — and despite my general feeling that The Meditations failed to be adequately convincing, we as a society are favoring an action franchise with major and obvious plotholes over a fairly heavily considered work by a brilliant philosopher.
Again, if a text develops its characters and plot adequately, the central conceit can essentially be ignored — a good ghost story is good even to people who don’t believe in ghosts, while a bad ghost story will fail to entertain enough to motivate people to suspend their disbelief.
Machine-lit shares with the rest of speculative fiction a basis in a counterfactual model of the world. That is to say, we start our world-building by setting some axioms that, in our world, are not true, and work from there. The difference is that machine-lit, by definition, performs the basic world building then immediately jumps to narrative, then stops as soon as something resembling a completed text is produced. Within world-building-based science fiction, a much more complex world is built, and the narrative and characters stem from that world organically.
This requires a dedication to completeness and intellectual honesty, in part because genuinely following the logical progression of the central mechanism of a counterfactual world can point out flaws in its structure.
In cryptography, the first and most important rule is never to roll your own crypto — always use a well-known and well-tested algorithm, at the very least, and ideally also use a well-known and well-tested implementation. The reason is that flaws are never intentionally introduced into crypto by people who want the crypto to succeed, and thus fatal flaws can only be identified by other people — and the more people there are looking for flaws in an algorithm, the faster such flaws are found (and the longer it takes to find fatal flaws in an algorithm, the more likely it is that such flaws are difficult to find). Everyone who designs crypto professionally is also skilled in trying to break crypto: you learn to avoid the flaws that you have discovered how to exploit. Likewise in computer security — the research arm of the computer security community consists of people who figure out how to break security and then figure out how to patch those holes.
In fact, this is a common pattern in legitimately serious enterprises. The scientific method is exactly this: suggest a model of the world, and then recruit people to attack it. The adversarial justice system is based on two groups of people presenting different models of the world and attacking each others’ models. Even in philosophy, philosophers engage in critiques of the ideas of other philosophers, rather than ignoring any idea they don’t agree with.
Any functional member of any of these communities will attempt, before putting their ideas out into the world, to stress-test them personally — formulate simple attacks, determine which portions of the idea are weak and whether they can be strengthened without complete restructuring.
Machine-lit, by and large, fails to perform these sanity checks. Machine-lit is the domain of people who are so in love with their ideas that they cannot bear to test their mettle before pushing them out into the world.
An ideology at the core of machine-lit, if properly investigated, would collapse upon itself or mutate such that it fails to be an ideology. A utopia at the core of machine lit would, upon close inspection, become a dystopia; a dystopia, upon close inspection, would yield some happy and fulfilled people, making the message of the book ambiguous. An actual machine at the core of machine-lit, if properly and rigorously tested, would become at worst a patent application but possibly an actual invention.
I’m perfectly in favor of optimism in science fiction. Nothing is to be gained from keeping the genre grimdark as a rule, in the same way that nothing is to be gained from keeping superhero movies grimdark. However, utopian science fiction represents a failure to take the medium seriously — and a shallow dystopia or cozy apocalypse is no better. Science fiction should be a genre of ideas, but there’s no point if we allow our ideological biases and our love of shiny toys to turn it into a genre of shallow ideas shielded from unforgiving reality. The real world has problems, and while escapism is fine, a work cannot simultaneously be an escapist fantasy and a serious analysis presenting a serious solution to the problems it fantasizes about escaping from.
Science fiction always starts as machine-lit. But, machine-lit is a larval stage that adult science fiction works outgrow.
This article was originally published at First Church of Space Jesus under the title “Utopianism and sci-fi as machine-lit”.
The key thing here is not that this time is different than the 60s, or that the people writing about this problem in the 60s were wrong. The state of technological optimism about AI has fluctuated over the years, but has almost never corresponded well with the reality of AI advancement. The people talking about this in the 60s were right, aside from the timeframe — because what everyone in 1959 thought would be normal in 1965 actually happened in 1995 and became normal in 2005.
Labor upsets related to technological changes do occur. There’s no rule saying that employment rates *need* to return to normal, but at least in a capitalist society, there are forces that encourage it (and capitalism is all about this kind cybernetic feedback). When a job is genuinely completely automated in such a way that, outside of the novelty or nostalgia factor of having a human make it (artisanal production), automated production is always preferable in terms of both price and quality, we can consider that a ‘functional singularity’ of that job — the machine has taken over that job, and humans aren’t going to take it back. Functional singularities happen all the time, and aren’t new. The first industrial revolution spawned luddites in part because a lot of cottage industries were undergoing functional singularities — machines can weave ornately patterned textiles better than humans, so jaquard looms really did take over the extremely specialized and skilled job of expert weavers in that context. Functional singularities happen at a smaller scale all the time, as well — think of the invention of sliced bread — and they are often viewed in a positive light as labor-saving devices, because they often take over tasks that had already been shouldered by the consumer (or by the housewife, or by servants — think sliced bread again, and the dishwasher, the washing machine, and the microwave). Indeed, the very first computers represented a functional singularity of the job of the ‘computer’ — a single computing machine took over the jobs of twenty or thirty young women with mechanical desk calculators; electromechanical telephone exchanges did the same for operators (again typically young women).
Historically, a functional singularity has been considered a labor-saving advancement by people who value the labor in question moreso than the livelihood of the people performing the labor. This is purely a market-related concern — if these people could fall back on a living wage once their tasks were replaced, it wouldn’t really be a concern that their task had been automated (because if they were doing it for some reason other than the money — say, because they got satisfaction from the work — they could continue doing it despite it no longer being profitable).
It takes time for new types of labor to appear in the wake of a functional singularity. To the extent that the ability to perform a task is specialized and full of non-transferrable skills, retraining time is time when the people who were doing the tasks are unemployed. And, even if we assume that such a thing always does happen, the rate at which it happens is clearly not strongly associated with the rate at which jobs are fully automated or the amount of skill displaced. The industrial revolution is so-called because of the vast amount of labor that was displaced, much of it skilled labor — and the various labor movements and luddite-style anti-automation movements of the era are the kind of thing that happen when so much labor is displaced. It’s important to note that the labor movements spawned by the industrial revolution took about a hundred years to get to the point where they are today, and mostly haven’t progressed since — we’re talking about a time scale wherein, historically, most of the displaced people who were the impetus for reforms did not live long enough to benefit from them, and remained displaced or operating in a diminished capacity (say, formerly highly-skilled weavers operating in the unskilled and less lucrative position of pulling a lever) for the remainder of their lives. Neither comparable new fields of labor nor reforms for improving their quality of life caught up to them in time to make any difference — theirs is a story of a lost livelihood.
We really need to be putting these measures in place preemptively, because doing so in a reactionary way will doom another generation to that generation’s equivalent of shit jobs — if even that. Many skilled jobs are on the cusp of replacement, and the unskilled jobs of today (retail, fast food) are already beginning to be replaced. Just as lever-pulling unskilled jobs were not numerous enough to replace the influx of displaced skilled cottage industry workers during the industrial revolution, there simply isn’t room to make every laid-off knowledge worker into a fry cook or grocery bagger — so, without improving the social safety net, expect a large increase in the homeless-and-hungry population. In the end, it doesn’t much matter when exactly the day fully comes, because progress toward total automation is happening, and displacement rates are already higher than reskilling rates.
It’s a mistake to call Uber a tech company.
It’s a mistake to call Uber a tech company. Uber is a taxi service that uses legal loopholes to avoid having to give its employees the various benefits legally granted to employees. As a result, democrats and leftists in general are against it, because the core of its business model is circumventing labor laws.
When Wal-Mart cleverly circumvents labor laws in order to deny rights to its employees, nobody claims that it’s justified by the technical leaps that Wal-Mart makes, but why? Wal-Mart has developed more new tech than Uber has. The answer is PR — Uber has decided to call itself a tech company, and thus, it has recieved the protection of tech company boosters.
When a genuinely innovative idea is put forth, there is no problem with regulation because regulation in that area doesn’t exist — the area doesn’t exist, so it’s a non-issue. Drones don’t meet this criteron — drones are an old technology, with early versions existing as far back as the 1940s, and they operate in a regulatory space that is largely well-developed. Uber doesn’t meet this critereon, because as we mentioned, it’s a taxi service.
Myths of competence and specialization
An idea has been going around for a while that science fiction, more than anything, is a literature of competence — the protagonists of science fiction are competent people who can be trusted to do the right things under the circumstances (given their knowledge of the situation), and their mistakes can generally be traced back to withheld information or the effects of external forces that manipulate their mental state (like drugs or mind control). This is true of a lot of golden age science fiction (wherein, generally speaking, the protagonists were also respectable, if not amiable — think Asimov & Heinlein), and is generally less true of new wave science fiction (think of Ellison, wherein occasionally our protagonists are mad or naive or belong to a culture with alien values) and first-generation cyberpunk (think of Neuromancer, wherein every character who isn’t mad is varying degrees of self-loathing and self-destructive). But, a fiction of competence is also the lens through which many people see the real world — and some of them are probably drawn to golden-age science fiction for this reason.
I have a friend who is, like me, a software engineer. He clearly sees the world through this lens. He sees people as, generally speaking, professionals; what I consider to be design errors he considers to be some unfortunate but inevitable product of circumstance that must have very good and acceptable reasons behind it. He acknowledges the occasional genuinely poor decision, when it’s undeniable that there’s no good excuse for it, but he considers such things rare and rarely acknowledges poor decisions made by people he respects. When faced with a problem, he prefers to theorize about it rather than probe it experimentally, and is willing to spend more time generating an elaborate mental model of a problem than experimentally discovering its contours. In other words, he has confidence in the integrity of his mind and the minds of others, and considers the production of mental models to be a generally foolproof method for exploring the world.
Although I respect him a great deal, and although I admit that his knowledge of many fields is deeper than mine, I consider his attitude naively optimistic.
My model of the world is compatible with the rule of the blind idiot god. The universe is complex enough that few elements can be modeled perfectly by human beings. Because competence is difficult to achieve, few people achieve it — incompetence and poor decisions are the rule, rather than the exception. Furthermore, even competent people have little reason to exercise their competence — the illusion of competence is rewarded moreso than actual competence, and exercising one’s competence takes time and energy that pretending to exercise one’s competence does not — and society rewards behaviors that are incompatible with the production and maintenance of genuine competence.
Human beings tend to value confidence in themselves. I consider this a major failure. Because the world cannot be perfectly modeled, all models are by definition imperfect — and confidence is faith in the predictive success of one’s mental model for situations upon which it has not yet been tested. Confidence is valued in oneself in part because confidence (i.e., lack of hesitation) is valuable in genuine emergencies — if you are being chased by a bear, spending mental effort determining whether the bear genuinely exists or is an illusion produced by a trickster god is detrimental to your expected lifespan. Genuine emergencies are more rare now than they were when the adrenal and peripheral nervous system first developed in our distant forebears, and they are less important to the survival of our genetic line — we are more likely to fail to reproduce out of a bias against children or financial instability or a lack of attraction to the opposite sex than out of actually being killed by something we could run away from (like a bicycle, an enemy, or a wild animal); as a result, in today’s world, it is generally more risky to be sure than to be unsure. The same confidence in the correctness of your mental model of the world that will save you from a wild animal will get you run over by a truck, because change blindness is part of the same set of energy-saving heuristics that allow human beings to do things faster and with less effort by introducing errors into our models of the world; the same confidence that would allow a human being in a nomadic-band-of-hunter-gatherers situation to fight effectively against another band trying to use the same resources will lead a modern person to fight and die in a religious war.
Human beings also value confidence in leaders. This is for a similar reason — if you are in a nomadic band of fewer than 150 other people, and you are being attacked by another group of approximately the same size, your odds are about even so long as your hesitation level is about even, but lack of hesitation gives you a tiny advantage. Your leader, because he is in charge of coordinating tactics, is the bottleneck — his hesitation is your hesitation. This is the context where leaders are useful — when discounting planning time your odds are 50/50, but when every second of hesitation counts against you, fortune favors fools who rush in over the ones who consider the situation carefully. But, few genuinely important situations today depend upon split-second decision-making. Unless you’re in the military, your ability to make poor decisions quickly will never be more important to your lifespan than your ability to make good decisions (although the ability to make good decisions quickly is beneficial in a wide variety of situations, it’s not really practical to develop), and unless you play professional sports the same is true of your livelihood. A good leader in typical modern circumstances is someone who takes minutes or hours to think a decision through, and who knows when to back off and reconsider a decision that has proven to be flawed — in other words, exactly the kind of person who appears unconfident to the point of neurosis. Because our heuristics are stuck in the stone age, to become a leader you must appear confident, but in order to be a good leader your apparent confidence must be an illusion.
This is not to say that I don’t believe in competence. In fact, I think competence is undervalued and under-sold. Take, for instance, the polymath.
A lot of people these days say that polymaths can no longer exist — that the world has gotten too complex. Bullshit. Our models of the world have gotten better — which means that our ability to predict the world has gotten better. It’s easier to be a polymath today than ever before, because being a polymath means being competent in a variety of fields, and great strides have been made in every field with regard to our ability to learn to become competent in them. The world has not gotten more complex, but instead, through human endevours, it has gotten slightly simpler — not because we have changed the world but because we have changed our minds, developing mental tools for organizing the massive clusterfuck that is reality into more and more useful predictive models, wherein the complexity of the model grows slower than its predictive utility.
The same narrative that claims that there can be no more polymaths tells us that specialization is desirable, or at worst an unfortunate necessity. If we can’t learn a variety of mental models because the models have gotten more complex, then we need to stick to our lane and go deep into one silo, solving the problems that fit into that domain.
But, all problems are in reality multidisciplinary. Disciplines and problem domains are inventions of human beings, and reality has no interest in them. The specialist is blind to this. The specialist sees the portions of the problem that fall into his domain, and perhaps slightly foggily sees the portions that fall into neighbouring domains; the remainder is some vast undifferentiated miasma that must be left to other people to figure out. As a result, the specialist can be very confident about his results — because he has chopped off everything in the universe that he doesn’t know how to model, and has applied a model to the tiny portion that has been left over. His model may not yield useful results, because he has ignored most of the universe, and he really can’t effectively isolate his subject that way.
The generalist, on the other hand, sees the universe and applies several different models that apply to different aspects of the subject (as well as sections of the world immediately surrounding it). The polymath, who is a generalist upgraded with the knowledge of several specialists, does the same thing with better results because he has a wider variety of useful models and the experience to determine which models are appropriate. The polymath can do this because he realises that each specialized field is a pattern recognition machine, and because some patterns can be found in the world wherever you look, many disciplines have independently reinvented the same or very similar models with different terminology. He can combine the similar models to form superior hybrid models, and when the models are exactly the same he can learn the new terminology or use the shared model to synthesize its sister models across domains. And, since models build upon each other based on shared patterns, he can use models from one discipline to more efficiently learn models from another, unrelated discipline because they essentially accidentally share patterns. Because of the polymath’s wider scope, he also is aware of common failures in various forms of various models — he is aware that the failures can compound, and so despite having better predictive results at a lower cost, he also has lower confidence; he has eliminated the artificially inflated confidence of the specialist and is left with a level of confidence more appropriate to the actual situation.
I feel like this myth of competence and confidence — the Captain Kirk character voyaging into the unknown and believing that he already knows it, confidently applying human biases to non-human situations and considering himself to be morally superior to cultures that don’t share his values — is not merely naive and optimistic, but actually regressive and dangerous. Any confident leader and man of action can be percieved, with a minor shift of perspective, as an arrogant fool who acts without thinking; any crusade against evil people doing evil things can be reframed as an intolerant bigot battling a system of values he doesn’t understand. This kind of literature transplants into the space age the kind of leader who hasn’t really been appropriate for a leadership role since the dawn of agriculture.
Another great music podcast (which may or may not be still running) is Solipsistic Nation.
Another great music podcast (which may or may not be still running) is Solipsistic Nation. It focuses on electronic music, and occasionally does shows focusing on particular labels or particular artists (for instance, it did a show with Black Moth Super Rainbow), but it also does nice theme shows (one theme show that I particularly liked was the “science fiction film soundtrack” show — wherein the host chose a playlist he felt would make an appropriate soundtrack for a Blade Runner-like cyberpunk film; but, he’s also had a mashup show and other more generic themes).
If your tastes run more to the obscure, I’d also recommend El Diabolik’s World of Psychotronic Soundtracks — a show focusing on music made for 60s and 70s european grindhouse films. A lot of really interesting stuff was being done by professional musicians in the 60s and 70s essentially as library music — in other words, these musicians would make tracks and shelve them, and then film-makers would buy a license to use them in films. There were some very interesting and influential acts that only ever worked in psychotronic music for films — for example, Goblin, a prog rock band who only ever made music for horror movies and whose music you will recognize if you’ve ever seen a Dario Argento film.
If your tastes run even more obscure, I’d recommend National Cynical Network  — essentially a mashup/theme show that’s an offshoot of the Church of the Subgenius (the same joke religion slash cult that gave us Devo, Pee Wee’s Funhouse, and the Slackware linux distribution). A typical episode will be somewhere between a Negativland-style sound collage and one of the more eclectic mashup artists (think Illuminoids or Niel Cicierega), and themes include “Teeth”, “Star Trek”, and “Songs about Mary”, in addition to having cover shows (the “Not Devo Show” consists entirely of Devo covers, grouped by original songs; the Pink Floyd episode has a mashup of covers of Dark Side of the Moon).
While I’ve seen lots of snappy, short, intelligent, and witty pieces of academic writing, I feel like people who criticize academic writing are really complaining about several different tendencies that rarely occur in the same piece of writing, all of which occur more often in academic writing than in most other kinds.
One tendency is to use specialized jargon. This makes perfect sense — terminology is invented in order to quickly reference a nuanced idea, at the cost of being impenetrable to people unfamiliar with the nuanced idea (and thus unfamiliar with the term). Using jargon as a shibboleth is also not without merit — explaining the meaning of these terms requires explaining the ideas behind them, which takes time and energy & is a waste of effort when the readership is already familiar with them; excluding readers without the prerequisite background to understand nuanced points is perfectly reasonable, particularly when good resources exist to bring a general audience up to speed on the ideas and terms. People working in the depths are not necessarily the best popularizers, and they don’t need to be.
Another tendency is to draw arguments out, making explanations overly long. I feel like this comes out of either avoiding jargon or not having access to the appropriate jargon.
A third tendency bears superficial resemblance to the first two, and is genuine obscurantism. This is a lot less common in academic writing than a lot of people think; it’s pretty common in business.
A fourth tendency, related to tendency number one, is to mimic the formal structures common in one’s field even when they are inappropriate for the task at hand. I don’t think that extreme examples of this are very common — academics are human beings, too, and can tell when something really isn’t working. But, how egregious examples of this appear depends heavily upon how familiar with the common structures the reader is — even something almost universal, like the format of a scientific paper with its list of references at the end and its abstract, can seem strange and awkward to someone who has never read one before. Again, the skill-set for communicating with a general audience is different from the skill-set for communicating with academics in the same field, and while some people have both skill-sets, they do not necessarily write a single document for both audiences.
(It’s also useful to note that some of the most entertaining academic work plays with the jargon and the structure in use in the field in a playful and perverse way. Single-line published papers and theorems, for instance, do this and are sometimes extremely influential. But, these things are even less accessible to a general audience.)
Interacting with Fiction
Interacting with Fiction
This essay may be disorganized. Treat it as a brain dump on the material, rather than a serious analysis.
I’d like to discuss a few different kinds of interactive fiction, coming from different traditions and with different attributes. I’d like to discuss how the forms themselves play with ideas about constraint and agency, and how treating them seriously might change the way we think about fiction and fictional worlds. I’d also like to discuss how each of these subverts certain ideas about interactive fiction taken from non-interactive fiction, and make connections between these forms and other related forms that I haven’t seen made due to accidents of history and geneology.
Dramatis Personae
I’d like to introduce our fictional forms, along with their attributes, an exemplar of each form, and a few other forms that bear similarities.
Classic IF: Also called the ‘text adventure’ genre, Classic IF (which I will use interchangably with ‘IF’ in this essay) is written fiction in the form of a computer program that can be interacted with via free-form text input. The exemplar I choose is Collosal Cave Adventure. Usually, when people talk about ‘interactive fiction’, they mean this. Most of the attributes of classic IF carry over into the ‘point and click adventure’ genre, because historically, most creators of point and click adventures started out in text adventures; I am treating the ability to click on any object in a crowded scene to be of the same class of player agency as free-form text input for the purposes of this essay and using IF to refer to both forms, for reasons that will become clear in the next section. Genre conventions in classic IF include difficult puzzles and a stance of habitual contempt for the player. Player habits developed by this form include exhaustive searches of possibility space (picking up all objects, trying all verbs, clicking everywhere on the screen).
Visual Novels: Also called ‘VNs’, visual novels consist of sequences of scenes interspersed with player choices. Visual novels differ from classic IF in that player choices are strictly limited — typically no more than four options are ever given, these options are clearly presented to the user (no free-form text input), and the options chosen almost always cause meaningful narrative changes. If classic IF has a maze structure, VNs have a tree structure. I’ve chosen as an exemplar of the form Everlasting Summer, because it’s free & contains many of the genre-typical attributes and features. Genre conventions include plotted routes based on romantic pairings (being associated romantically with a particular character will give you a very different sequence of choices and events than with another character) and framing devices involving time travel. Player habits include re-playing in order to play through all possible routes (or at least, get all possible endings). Many recent twine games are similar in structure to visual novels, and so I would classify them the same way; while some FMV games are best classified as part of the point and click adventure genre, many are better grouped with VNs.
Wiki-based Choose Your Own Adventure stories: While these are not typically considered in essays like this, I think they add several interesting dimensions of possibility. My chosen exemplar is the Infictive Research Wiki Adventure. Wiki adventures have a primary method of play similar to visual novels, but differ in that players can modify scenes and options.
Fan work: Here is where we get a bit meta. Fan work, also called doujinshi, is the blanket term for any creative work related to a franchise not made by the franchise license holders. If we include fanon in this definition, we can classify it as a genuine interaction with a static fictional world that can result in apparent mutations to that fictional world. My exemplar is the fan theories subreddit.
A note on our characters: I have avoided classifying the behemoth of triple-a games as part of interactive fiction because in modern high-budget games, gameplay mechanics and visual sophistication often take priority over storytelling, and to the extent that storytelling is done it is entirely non-interactive. Unless the player character can meaningfully change the story being told (in a more complex way than winning or losing) and the story being told takes a prominent role in the experience, I would not classify it as interactive fiction. As far as I’m aware, the only recent triple-a game franchise to meet these criteria as well as the least suitable VN has been Mass Effect; however, that franchise also struggled with a percieved betrayal of the fanbase’s expectation for meaningful interaction with the fictional world during the end of the final game. Because our focus is on agency and constraint in interactive storytelling, my position is that games that allow the player character free and detailed movement in 3d space (or indeed 2d space) are, generally speaking, providing levels of agency superfluous to the goal of storytelling and potentially directly counter to it. The fact that these games often mimic the styles of non-interactive forms of storytelling like film for their storytelling elements while having primary gameplay mechanics be of no use during designated storytelling portions indicates that storytelling and gameplay are considered to be separate domains potentially at odds in this kind of game, while the genres I am focusing on have gameplay elements that directly interact with the structure of narrative.
Agency and meta-agency
In classic IF, the player is in control of a player character. His control is, genrally speaking, limited to physics — he can control the player character’s geographical location in the game world, pick up and manipulate objects, and have limited interaction with characters, based on the limits of the command parser and the variety of interactions planned by the game designer. I call this physical and limited-conversational agency: the player can manipulate the physical state of the game and initiate pre-scripted entire conversations.
In a VN, the player is also in control of a player character. However, the player’s decisions are much more limited. Rather than being able to try whatever obscure sequence of words he can imagine, the set of possible options is laid out. The responsibility for enumerating the possibilities of the world has moved from player to developer, which makes for easier play — no rules are hidden. Classic IF will appear more mysterious than a VN of similar complexity, and it is possible to have options in a VN that in classic IF would make it unplayable because the player could not reasonably be expected to guess them. In both IF and VNs, the world is crystallized and all possible narrative paths through the world have been predetermined; however, in a VN, because of the requirement that these options be enumerated, we have limited the player’s agency to actions that have meaningful narrative effects. I call this narrative agency: the player’s actions directly select which path to take through the story tree.
In a wiki adventure, we have both narrative agency and meta-agency. A player can take whatever choices he likes, but can also create new narrative paths. The story is crystallized until the user decides to change it. Furthermore, there is a social element: stories are being mutated by a group, and feedback loops cause strange attractors in the group’s psychology to manifest in the fiction.
Finally, in fan work, we have only meta-agency. Fan work itself has no protagonist; the player navigates his own mental model of a narrative and creates new narratives from it. Once these narratives are released into the world they are crystallized; but, their mutability is ensured because new versions can be created by other fans. Occasionally, fan work creates a culture significantly divorced from the original and invents a very independent narrative universe, based more on trends and patterns in the fanbase than on any genuine attributes of the supposed source material — an extreme form of the feedback loops found in wiki adventure, generating narrative simulacra.
Completeness
A common habit of VN players is to get 100% completion — to visit all routes and view all possible outcomes. On one hand, this is a show of dedication, and an in-group signaling mechanism: VNs can be extremely long, so getting 100% completion is often time-consuming in addition to requiring some careful note-taking and book-keeping. Some VN engines include features to aid in keeping track of options and routes already taken, or features useful only on re-play (such as skipping over already-seen content). On the other hand, this kind of completionism is a godlike ability to model the entire work completely — akin to viewing every alternate timeline in a Burroughs-Wheeler MWI universe. This completionism is made possible by the enumeration of responses. It is not possible in classic IF, which can have a structure of similar complexity and choices of similar granularity, unless the player determines the set of all possible options and uses them at all possible points — and while engines that can recognize only expressions of the form <verb><noun> can be iterated over using all possible combinations of recognized verbs and nouns, some engines support more elaborate language constructs including embedding, which makes enumeration of all possible recognizable strings impossible.
However, our mutable forms (fan work and wiki adventures) are incompletable on yet another order of magnitude. They change along the axis of real time as well as fictional time. While you can take a snapshot of a wiki adventure at any given time and play it to 100% completion, it can be modified the next time you play it — at any point along its timeline. Fan work is even more extreme; by its nature it forks, so any given fanwork is at any given time geneologically connected to several others that differ and are themselves mutable in real time. Fan work is the most amorpous — combining the flexibility of language with mutation along time and geographic axes, yet still operating directly upon narrative without the use of a player-character intermediary. Nevertheless, fan work is a game — a game with no author and no end, created entirely by the players.
Peter Watts and p-zombies
I was surprised, upon listening to a two part interview with Peter Watts, to find him tentatively supporting Chalmer’s positions on qualia and the hard problem. Part of the reason is that Watts is a(n ex-) scientist with a background in biology and neuroscience, and also both very intelligent and spectacularly good at not avoiding unpleasant trains of thought. The other reason I was surprised is that I read Blindsight, and interpreted it as an amazingly good takedown of the Chalmers philosophical zombie idea along the same lines as Dennett’s.
This essay will contain spoilers for Blindsight, probably. Also, spoilers for the epistemology of Chalmers and Dennett. If you don’t like to learn things in orders not officially sanctioned by the establishment, I recommend you at least read Blindsight — it’s a great read, and Watts has been nice enough to put it online for free.
Chalmers presents the idea of consciousness as indicated by qualia — a representation of the subjective feeling of the outside world. His position, in my understanding, is that subjective feeling is a more difficult thing to model than other properties of the world. While I’m not sure about Chalmers himself, other people have used this idea that qualia is a “hard problem” as an excuse for reintroducing cartesian dualism into the world of epistemology — by claiming that qualia is so difficult to model that not even straight-up neurons can model it, and thus we need to bring in quantum nanotubules or some other structure as a stand-in for the soul.
A lot of people have been suspicious of the idea of qualia. After all, isn’t a representation a representation? Isn’t a subjective representation just a second-order representation? I agree with Dennett when he argues that it’s an unnecessary complication, with no evidence for it. I would furthermore argue that it’s a matter of preferring a mysterious answer to a mysterious question: complex behavior can be difficult to predict not because it’s irreducible — not because each piece is complex — but because lots of simple pieces combine in a complex way, but there’s a general tendency among people to try to keep emotional parity with explanations (mysterious things need to be explained in a way that retains the mystery or else you’ve lost the mystery; negative events can’t be explained as an interaction between purely positive intentions, or else where did the negative essence come from?) but ultimately reality doesn’t deal in emotional valences and so feelings of mystery do not need to be conserved.
Chalmers came up with a fascinating thought experiment in order to “prove” the existence of qualia. He suggested the idea of a ‘philosophical zombie’: a person indistinguishable from a regular person, but without qualia. Because qualia cannot be tested for, this person would be completely indistinguishable from a regular person.
Somehow, a lot of otherwise intelligent people thought that this was a good argument. I can’t see the invisible dragon in my garage, and therefore it must exist.
In Blindsight, Watts plays with a few variations on the philosophical zombie idea. He puts forth the idea of vampires being said to lack qualia — along with other cognitive anomalies that are of benefit to a humanoid with a very different position in the food chain. Certain optical illusions and cognitive biases don’t work on them. They have some differences in social behavior. They are largely lacking in empathy, without having the problems with impulse control that tend to be comorbid with lack of empathy in human sociopaths. A vampire, along with a split-brain patient, a personality collective, a person with extreme sensory modifications, and some other various neurodivergents take a space trip to meet a colony of intelligent starfish/squid-like aliens that are determined to have no qualia either and no sense of identity.
But, the ideas about qualia don’t line up here. I assumed it was on purpose.
Rather than ‘qualia’, each of these neurodivergent characters has some facility or attribute missing or strongly modified that is very clearly defined and very clearly not the same as qualia. And furthermore, each of these characters has very different behaviors based on their divergence from the norm. (This is along the same lines as the Rifters trilogy, particularly Starfish — we’re basically talking about circumstances where people who are psychologically and neurologically maladapted to normal life in a normal society end up being very well adapted to a fundamentally different environment.)
In other words, it’s a strong argument against philosophical zombies.
In the end of Blindsight, our protagonist gets back within radio range of Earth and can tell it’s been taken over by the vampires. Because Earth had stopped broadcasting music and entertainment, in favor of utilitarian communications. The vampires aren’t philosophical zombies, because they can be distinguished from humans. Because the particular kinds of things that they don’t experience lead them to live in a more utilitarian manner.
Indeed, no novel could deal with philosophical zombies. Because, by definition, philosophical zombies could not be distinguished from normal people. A novel about philosophical zombies could not be distinguished from a novel with no philosophical zombies in it.
Now, the argument for qualia is that, while human beings can experience something through their senses (like the color green), that experience cannot be identified in the brain itself. There is no neuron for ‘green’, and even if there was, the neuron itself wouldn’t be ‘green’ or contain the concept of ‘green’.
This argument has a handful of big flaws, some of which have been dissected elsewhere, so I’m going to dispatch it as efficiently as possible. First off, while some things do seem to have dedicated neurons (this is the ‘Grandmother Neuron’ model), most things don’t — however, this is not terribly unusual; we are very accustomed to another system for modeling the world where some configurations of state have single symbols and others have sets of meaningfully interconnected symbols: language. The word ‘green’ is not necessarily green — in fact, it might be red — and does not contain the concept of green, but instead gains its meaning from its relationship to other things. Ultimately, we can say that it gains real meaning by being in a relationship with other symbols in a manner that represents some configuration of the outside world as perceived through some people’s sensory apparatus, and gains utility insomuch as it allows us to communicate and make predictions. However, we can have syntactically meaningful configurations of symbols that could not have any semantic meaning — the colorless green ideas sleep furiously — or syntactically and semantically meaningful configurations of symbols that could not represent our universe — maxwell’s demon mounted the pink unicorn’s dragon-skin saddle and rode off at six times the speed of light in order to find some anti-entropic material and transmogrify it into orgone. Since language does this, there’s no reason for the brain to be incapable of it; since the brain makes language, the brain must be capable of doing it. It’s also not mysterious — even toy languages with heavily simplified grammars designed for computers to manipulate can do thing kind of thing (think RDF, or PROLOG).
As someone who has a background in biology and neurology, who works with words and language professionally, and who thinks deeply and clearly about most things, I would expect Watts to make these same judgments. If he has a counterargument in favor of qualia, I’d like to hear it. But, my general position is that to the extent that something that behaves similar to qualia exists, it is symbol manipulation, and to the degree that something like consciousness exists, it is something like self-simulation.
(Originally posted here)
I’m not sure I buy that this is the result of increased usability.
I’m not sure I buy that this is the result of increased usability. Kids learned BASIC by trial and error without consulting the manuals on 80s home computers. The learning curve is lower if literacy is not a prerequisite, which makes the rate at which motivated children learn seem more striking.
The origin of many of our most elaborate models of corporatism, advertising, and how these things infiltrate culture is — ironically enough — also the proximate origin of the very practices that theme parks embody: the situationist movement in Paris. They were equally concerned with modeling advertising and its subversion and with imagining a future urbanism where a post-scarcity city would see its primary goal as providing citizens with interesting and entertaining experiences.
The situationists would say that we can only ever win temporarily: the spectacle sees those things that subvert it, consumes them, and allows the defanged and sanitized symbols of that very subversion to become a part of itself. Just as punk was stripped of its ethos, just as Apple took an anti-consumerist minimalism and turned it into a reason to buy more things, Disney and its cohorts will find anything that opposes them and wear its skin. Nevertheless, we can continue to subvert. The situationists were marxists — they believed that capitalism would collapse under its own weight, and (presaging accelerationism) believed that constant subversion would quicken the fall of the spectacle by bloating it.
Alternatives to advertising
Thinkpieces about the ethics of ad blocking are all over the news recently, because apparently things only become newsworthy when Apple stops banning them. The time to discuss ad-blocking is not now, really — after all, the largest tech companies make all their money from advertising now. The time to discuss ad-blocking was 1994, when the first banner ad was introduced.
That said, there are new (or at least new-ish) things to say about alternatives to ad-based monetization on the web, in part because during the past few years alternatives have been successfully implemented, and in part because intelligent people like Jaron Lanier have been writing at length about possible alternatives recently.
If you’re reading this, you — like most people — have probably heard the idea that advertising is justified as the sole alternative to paywalls and merchandise sales, and swallowed it completely. You probably didn’t quite realize that Kickstarter and Patreon were genuine alternatives to an ad-based revenue model. Allow this post to be an introduction to the variety of ways in which you can distribute media for free and still get paid.
Why advertising is a bad model
Some reasons why advertising on the web is bad are probably familiar to you: targeted advertising implies tracking, which eats up bandwidth and is a potential violation of privacy; advertisements are typically both irritating and irrelevant. Other reasons will be familiar to people who have hosted ads: click-through rates are incredibly low and ads have become devalued over the past ten years such that providers like Google pay fractions of a cent per click and nothing per exposure, meaning that only extremely popular sites can make more than pocket change through ads; even ads hosted through big providers like Google can be full of malware. The big one is the one you haven’t heard of, though: ads don’t work. Depending upon advertising as the basis of the internet’s economy is like tying the value of paper money to tulip bulbs during the height of the Dutch tulip bubble; sooner or later the entire system will become devalued.
A selection of alternatives
This is, obviously, not a complete list. I’m going to address the most obvious and frequently-cited ones first.
• Subscriptions: rather than releasing your content for free, you charge for access. This monetization policy is very vulnerable to piracy — if there’s an alternative source, there’s no incentive to pay. It has low discoverability — if people can’t see your content without subscribing, they have no incentive to subscribe. It’s also not particularly sustainable unless you have a lot of high-quality content being released steadily — if the quality is too low, you lose subscribers; if the content isn’t released steadily enough or isn’t voluminous enough, subscribers feel cheated. • Paywalls: an attempt to solve the discoverability problem with subscriptions by releasing a certain amount for free. Paywalls are often easily circumvented in such a way that a subscription is never necessary. Furthermore, while discoverability is improved, the average quality of content must be high if the content released for free is to convince people to pay. Finally, and most damning: paywalls don’t even attempt to solve any of the problems of subscriptions other than discoverability. • Freemium: while most content is free, a subscription system exists that provides users with either extra content or extra functionality. Freemium systems walk a delicate balance: if the free version is too functional, nobody subscribes; if the free version is too incomplete, it’s considered crippleware (and somebody else with lower costs will undercut you). • Donations: fans pay on a fully voluntary basis while content is released for free. This is very sensitive to the size of a fan-base and to how much that fan-base feels like the creator is ‘in need’ — a popular franchise with a creator perceived to be wealthy will not get donations. • Merchandise: items are sold at a high markup with images tied into the media in question. This is essentially hiding a donation inside a purchase. Because of this, it’s easy for people to undercut your prices with merchandise similar to your own and take advantage of any fans who don’t realize that their purchase is a donation. Furthermore, the primary benefit over straight donation is that fans can advertise their association with the media in question — which requires a cohesive fan-base and a cohesive set of evocative symbols to make it profitable. A popular franchise without a fanbase that sees itself as set apart from the rest of the culture cannot reasonably benefit from merchandise, nor can a popular franchise with insufficiently iconic symbols. • Crowdfunding: fans donate money to pay for the creation of media not yet created, often with merchandise thrown in to encourage larger donations. Crowdfunding is sometimes extremely effective; however, franchises with large and cohesive existing fan-bases benefit more from it than others. Discoverability is low. Repeatability is low, since repeated fundraising runs will tire fans, meaning that this is ideal for one-off new additions to existing franchises. Generally, if merchandise works for you and you need to raise money from your existing fans for something big and expensive and non-repeating like a movie or a tour, crowdfunding will be ideal. • The Street-performer protocol: you create something and then use a crowdfunding-style fundraising round to finance it before releasing it. On the surface this looks strictly worse than crowdfunding; however, because the thing is already created, you can avoid the common crowdfunding pitfall wherein time estimates for completion are under-estimated and the product that was funded was never created. Furthermore, quite explicitly, in the street performer protocol, the resulting media is released free to everyone as soon as it is paid for; as a result, it’s highly piracy-resistant: nobody has a copy before it’s paid for except the creator, and afterwards everybody does. During the first iteration, discoverability can be a problem; however, discoverability becomes less of a problem the more iterations you go through. Because this was Bruce Schneier’s idea, there’s some mathematical formalism and technical detail to how trust is established between various parties, which I won’t discuss here. • Patreon-style funding: something akin to a marriage between the street-performer protocol and a subscription system. In patreon’s model, fans subscribe to a creator such that they pay a certain amount automatically upon the public/free release of a piece of media. Unlike the street performer protocol, the media isn’t created before people agree to fund it. Patreon provides a mechanism for incentivizing higher donations by allowing various donation tiers access to specific content not accessible to other tiers, but such content tends to be rough drafts and notes — in other words, content mostly interesting to hard-core fans who might donate anyhow, thus circumventing problems with a ‘freemium’ model. • Decision bidding: fans can pay for the ability to influence the end product. Occasionally this is integrated with another model — I’ve seen limited decision bidding as an element in patreon and kickstarter campaigns. However, by treating certain classes of decisions (like what to blog about) as auctions, you gain the possibility of bidding wars. This probably will only work if you have a very dedicated fanbase. • Decision futures: fans wager on decisions and in the process make those decisions. You can treat this as a variation of decision bidding, or an extension of the kind of decision-voting that blogs sometimes have (polls about which topic should be covered next, for instance). Fans pay some small amount to vote, and those who didn’t win get a refund. You can increase fairness and revenues by allowing multiple votes up to some limit. • Pay-for-privacy: in a service that remotely hosts user content, public hosting is free while private hosting costs money. Github uses this model. • Pay-to-remix: release media for free, but expect people who reuse or recontextualize it to send you some money. This is proposed in some versions of transcopyright, and implemented in some demo-scale systems like token-coin; it is also an implicit part of music industry licensing, in which artists pay a flat fee to release a cover song on a record, and in which writers and composers are paid royalties on their work regardless of who performs it.
I’d be interested in seeing how ebook sales trends match up with the shift from epaper to OLED/LCD displays on ebook readers. After all, the first few generations of Kindles and Nooks had epaper display — which in addition to having good battery life, mimics the properties of a paper book quite well; current generations (particularly post-ipad) have luminescent displays with color and faster refresh rates and are essentially just tablets. E-paper displays, because they lack back-lighting, are ideal for before-bed reading; because of their slow refresh rate, they aren’t ideal for anything *other* than reading full pages of text. By introducing tablet-style backlit displays, the new dedicated e-readers may be driving people who would otherwise use them for before-bed reading back to paper, while pushing the rest of their users toward non-book content.
(I’m also wondering if these trends you’ve mentioned extend outside of the Amazon-and-BN for-profit-ebook universe. Do the same trends exist in, say, download rates for epub files on Project Gutenberg? What about archive.org, or the pirate bay? It may be that prices or circumstances are lowering the rates at which people download ebooks that they need to pay for — and a variety of things could cause this, ranging from DRM policies to perceived reliability of the devices. After all, if you own a Kindle and all the books you own exist only on it with no capacity for backup, then bricking it would mean losing your entire library; it then makes more sense to buy the books on paper or to pirate them, since in the former circumstance it’s more difficult to lose all of them and in the latter circumstance it’s not difficult to back them up or get them again later.)
A big problem here is that advertising — particularly advertising on the internet (by which I mean both web advertising and email advertising) — has already poisoned the well. The ideas you propose about vetting ads, about keeping up quality standards for types of ads, and about trying to ensure appropriate targeting are old ideas and already widely adopted — but each of them, over time, has been subverted and worn down, because advertising (since its effectiveness per impression is exceedingly low in the best of cases and almost nil in the average case) almost always becomes a race to the bottom.
Google’s entire core business model is the automatic targeting of ads along with strong quality restrictions; adsense is the biggest web ad provider in the world, and doesn’t allow advertising with any of the qualities you mentioned under a metric for bad ads. Nevertheless, outside of AdBlockPlus having an option to avoid blocking only google-hosted plain-text ads, most ad blockers still block everything coming out of adsense.
The suggestion that users will interact with a piece of advertising to indicate its quality and relevance is also pretty questionable. After all, all adsense-served ads have a button for reporting low-quality or non-relevant ads. But, years of dealing with ads that claim to have such buttons but who actually use them to steal metrics and redirect users to different ads have trained users not to trust anything put onto a page by a third party.
Even if large advertising organizations were to sign on for the kinds of restrictions you propose (and recall that Google already essentially implements all of them), that won’t really discourage ad blocking. Well-behaved ads aren’t a problem, and people who host ill-behaved ads are doing so knowingly and intentionally, because they don’t care about user experience. You can’t police them, because they’re already too shady to care about what you think and aren’t afraid to ruin advertising for everyone; they are the people hosting fake versions of the facebook login page and redirecting you to twelve different ‘around the web’ links in frames whenever you click ‘sign in’. They’re the people who are buying zero days from the russian mafia and selling them to the NSA at a 700% markup. They’re the reason that you can’t use just one ad blocker; in order to block enough ads to matter, you need to run three different ones plus an anti-tracking extension, and maintain separate whitelists for each. When advertising as a revenue model is dead, they won’t care because they will have moved on to killing kittens and selling their pelts as synthetic wool.
The biggest issue with this article is not the argument that advertising can be saved; after all, a lot of smart people have thought that in the past, and the things you propose to save it have been proposed and implemented at scale by people who knew what they were doing. Instead, the biggest issue is the false dichotomy between advertising and paywalls. Paywalls are one of maybe ten or twelve different alternative monetization strategies, and they have a bad reputation for a good reason. I recommend you do a little bit of research into the variety of alternatives. Ad hosting and paywalls are old models and remain dominant because of inertia, but they are poor solutions in terms of effectiveness, returns, and increasing customer confidence.
Gelman’s characterization of his opposition is more than a little reductive.
Gelman’s characterization of his opposition is more than a little reductive. In brownian motion, the particles are wholly passive. However, the varieties of stimuli that are being studied with respect to their effect on human behavior are, in fact, primarily human-produced and can be modeled as a form of communication wherein ideas are spread subtly without analysis. It’s not controversial that this kind of communication (and this kind of spread) occurs within media, wherein we have dated archives showing various elements (arguably without rational basis) growing and shrinking in popularity in accordance with power laws; why should we believe that people do not communicate this way when they are not being recorded, or when they are outside the silos of media production?
The hidden benefits of NaNoGenMo
On November 1st of 2015, NaNoGenMo begins its third year. It’ll be the third year that I’ve participated, and the third year that it’s spawned articles in legitimate paper magazines and newspapers, none of which are, unfortunately, particularly distinct from the coverage of the story generator Brutus in 1999 in the New York Times or similar projects from years prior.
Media coverage seems to circle around the spectre of wholesale automation of authorship the way that hapless space-ships circle around a black hole. (Because we, as a community, have an interest in corpora — the ability to access and analyse data makes inserting variety into generative writing convenient — we have kept track of these articles.) Perhaps, being written by journalists, these articles are justified in having a bit of a hysterical bias. After all, certain classes of news stories are already being written mostly by software, and fear-mongering about automation has been a lucrative staple of the press since the invention of the automatic loom.
However, despite its universality, the narrative of automation of authorship is a poor lens with which to look at the current state of generative text. Some forms of journalism are trivially automated, and these are precisely the kinds that are automated. However, the variety of journalism that journalists are increasingly reaching for (beautiful, literary longform nonfiction, which thrives on the web because of the comparatively low cost of distribution and which stands out heavily from the landscape of low-quality under-considered short posts) is both far from the grasp of the current generation of text generators and far from the aim of NaNoGenMo in specific.
NaNoGenMo produces, quite consistency, a flurry of extreme creativity and a wide variety of aims, styles, and implementation techniques; many people start several entries with extremely different approaches and goals, and many people do not end up producing a novel-length text despite the utterly trivial requirements, because their amazingly creative techniques were unable to produce a novel whose originality they could be proud of. In its first year, we had (among other entries) a novel composed of a supercut of similar tweets, a novel composed of a supercut of Homeric fight scenes, and a mystery novel composed of the belabored meanderings of Alice and Bob in a labyrinthine house; in its second year, we got a deeply atmospheric comic book composed by pulling images from flickr, post-processing them, and superimposing thematically related lines from detective novels, as well as a wonderful book-length piece of asemic writing. This year — who knows?
Explicitly experimental techniques appear to produce the best results. This makes sense — experimental techniques tend to be very well-defined, and the results of experimental techniques in literature as executed by human beings tend to be dominated by the attributes of the techniques themselves (meaning that, were a machine to execute those same techniques, the results would be superficially very similar). We haven’t progressed to a level of understanding of the craft of writing that allows us to automate good, readable, page-turning fiction — and I doubt that even an author of best-selling potboilers has such an explicit model. As a result, our community is less Clairion and more Oulipo. Nevertheless, each year, we produce works that inch closer and closer to readable. We produce vast novelty with the (eventual) aim of mundane novelty.
As a result, it may be most sensible to consider NaNoGenMo to be an amateur expedition into the greater control and quantification of literature.
The Royal Society of London in the 17th and 18th centuries independently rediscovered many of the things already known to professional craftsmen of the physical sciences like doctors, midwives, miners, and sailors; nevertheless, by aiming to measure and control their experiments, they became the vanguard of systematic knowledge of the physical world, which made later developments easier to isolate and demonstrate. NaNoGenMo can be seen as doing the same for the craft of literature: by producing machinery that consistently executes particular literary techniques, we can produce large amounts of stylistically consistent text; we can perform systematic mutations of text; we can isolate important elements by seeing how text affects people with a level of purity and consistency and volume not possible with human-written text.
We are also holding engineering discussions about things like plot and style. We’re determining whether, given some N major plot events, any ordering of those N events can be made sensible via transitions. We’re determining whether macro-level plot beats produce a greater impression of novelty than sentence- and paragraph-level variation in structure, and whether either of them produce a greater impression of novelty in human readers than variation in word frequency. We’re trying to figure out how much novelty is not enough and how much is too much in the context of texts of different lengths. We’re talking about how to engineer the eliza effect in readers. We’re figuring out whether or not readers can identify descriptive fluff, and whether or not they care — and whether or not Ray Chandler was lying about how he structured detective novels, whether the Hero’s Journey really is too vague, and whether the beats in Save the Cat can truly produce compelling stories with minute-by-minute granularity at a feature-film scale.
You can make the argument that this will eventually lead to the possibility of fully automated journalism. But, being possible is not a particularly compelling argument for it to be likely. Even relatively crappy chess computers can now completely outclass chess grand-masters — and so we have augmented chess, wherein a human grand-master works hand in hand with a chess-playing machine to play against another grand-master-and-machine tag-team. Items like furniture are much better manufactured by machine (in terms of quality, price, and environmental impact), and yet we pay much more for artisanal furniture made by obsolete and wasteful processes because we value the idea of a human being doing something that doesn’t need to be done and doing it the hard way.
In the same way that there is a market for artisanal wicker chairs and artisanal bread, there will probably continue to be a market for artisanal journalism — and for the rest of us, human journalists may become symbiotes joined at the hip with machines that automate the less interesting parts of the job. We already have some such mechanisms — spell check, grammar check, layout tools, note-taking and mind-mapping and automatic summarization tools. Tools for augmenting creativity in authors aren’t new either — cut-ups pre-date digital computers, as do markov chains and bibliomancy, and oblique strategies are now half a century old. Cutups and markov chains actually produce text for you, to which you must act as editor; but all these ‘writing machines’ that augment creativity do so by acting as a source of semantic randomness, much as mind-warping drugs do. We accept the use of all these mechanisms already — we don’t criticize Thom Yorke or William S. Burroughs for using cut-ups any moreso than we criticize John Lennon or Hunter S. Thomson for using LSD. Automatic methods for the production of text will, if they gain acceptance among writers, gain as much acceptance among readers as spell check and LSD.
NaNoGenMo probably won’t produce the future journalism-symbiote I describe, in the same way that NaNoWriMo has never produced the great american novel; but, just as NaNoWriMo produces novelists (and published novels), NaNoGenMo will produce some of the figures and technologies and domains of collective knowledge and culture that will inform text generation in creative fiction in the near future.
To be honest, the closest thing to an interest feed as you describe it is tumblr — not because it doesn’t try to be a ‘social network’ but because it fails so hard at being a bidirectional communication medium.
Like twitter, tumblr has asymmetric following and a reblogging feature that constitutes most of the content. Unlike twitter, most active users keep separate accounts for separate topics. While you are still *following* *accounts*, it’s more like following one person or group’s curation of a topic than it is like following a person, and some portion of the network actually only pays attention to tags (meaning that they don’t follow anyone and don’t look at their feed, but instead watch a topic-centered feed via the search function). So it’s twitter without character limits.
The formula is stupid, simple, and thus far pretty reliably successful as far as literally anything in the domain of startups goes: take an existing service, fire all the employees and replace them with untrained contract workers, spend an afternoon on a smartphone app or mobile site, and call yourself a tech company. Since the tech involved is extremely simple — your fourteen year old nephew can do it after school — this is sort of a brave re-branding. But, non-tech companies in the valley don’t get ten billion dollar valuations, do they? And if you have a ten billion dollar valuation, who cares if you make a profit?
But, again, what reason do we have to believe that other humans have interiority if we ignore the indications of interior life that dogs share with humans? Humans produce more complex communications, but those communications are not necessarily better explained by interiority than by mere generative complexity.
This is not to say that I don’t believe humans feel pain, but instead, to say that if *you* believe that humans other than yourself feel pain then you should probably also believe that dogs feel pain, since there is roughly equal evidence; likewise, with intentionality, we can estimate it by looking at planning effectiveness. The closer a set of behaviors is to being the ideal path toward some goal, the more likely it is that those behaviors were planned by a goal-persuing system. Proving intentionality with a 100% success rate is not possible, just as proving interiority with a 100% success rate isn’t possible: a system with strong intentionality that is working off flawed axioms or flawed data or that is very limited in how many steps it can plan ahead will be almost indistiguishable from a system that performs behaviors semi-randomly based on simple rules without memory. But, communication is neither necessary nor sufficient to prove either interiority or intentionality: after all, limping and yelping are, effectively, communication insomuch as they are behaviors that provide information to humans and other animals about how likely the dog is to be in pain, and yet we can ignore those signals based on the assumption that all attribution of inner state to non-human animals is pure anthropomorphism.
You’re complaining that a free VR device came with your newspaper, even though by no means do you need to use it in order to gain the same enjoyment you usually do out of your newspaper, because it reminded you that computers exist? I hate to think of what will happen when you discover that the New York Times has had a tech section for thirty years.
I’m not sure that decision anxiety is the sole or even primary source of ‘prestige television’.
I’m not sure that decision anxiety is the sole or even primary source of ‘prestige television’. After all, a shift toward a preference for binge-watching (and the type of show that gets better when marathonned rather than getting worse) dates to the first instances of widespread time-shifted home viewing. Buffy replaced Kolchak in the public consciousness because Kolchak was too formulaic to watch in season-long chunks and networks wanted to hype new seasons with marathon reruns of previous seasons — and because Buffy was getting released on DVD. Netflix and other streaming services upped the ante in a UI way, but not via poor UI design: Netflix doesn’t update their catalog every time an episode comes out but instead every time a season comes out (sometimes with a multi-year delay), so binge-watching is the only way to watch that doesn’t involve either using another service that updates faster or forcing yourself to keep to a weekly schedule; however, unlike buying a DVD boxed set, queuing up a series on Netflix doesn’t cost any more than not doing so. Thus, a mainstream audience learned what the anime bootleg fansub community learned ten years earlier: when you can watch a whole season of a show in one sitting with no penalty for dropping it in the middle, doing so is for a large subset of shows — those shows that gain rewatch value by focusing on complex, intricate, and detailed plots and character arcs rather than going for a casual prime-time-TV audience with formulaic structures, running gags, and ripped-from-the-headlines topicality — will be far more enjoyable than watching an episode a week.
You’re absolutely right about Netflix’s UI, though. It’s awful.
This is not ‘no UI’ — it’s ‘text-based UI’.
This is not ‘no UI’ — it’s ‘text-based UI’. Human written language is a set of technologies that have been under active, heavy development for four thousand years — compared to GUIs, which were under active and heavy development at Xerox PARC for about five years and have barely changed since. I agree that text-based UIs are the future — the competent ones among us never switched away from them in the first place, because they have features and nuances that GUIs would not be able to compete with even had they been truly under active development for the forty years of their existence.
However, this doesn’t mean that traditional UI concerns are irrelevant to text-based UIs, or that there are no special UI concerns to consider. It’s very easy to screw up a text-based UI — compare the MS-DOS command shell with a modern UNIX shell like zsh, and the vast gulfs of difference are obvious; even so, MS-DOS is by far not the least competent text-based UI in existence.
Conversational interfaces, by catering almost exclusively to new users and eschewing efficiency, nuance, and a rich feature set in favor of a shallow learning curve, are almost universally unusable for genuinely complex real-world tasks and gain their popularity from the novelty element of a computer behaving like a person. A well-designed conversational interface would eventually, in the hands of a habitual user who has become comfortable with it, resemble a traditional command-line interface: it would efficiently execute unambiguous and richly expressive queries while making use of shortened mnemonic forms of those queries. However, the novelty of a conversational interface to a new user depends upon a cynical assumption about that user’s willingness and ability to learn new skills: the conversational interface must accommodate a user who knows nothing and has made no effort to learn, and to interpret ambiguous requests as their simplest possible evaluation — to complain about the ambiguity or request it to be resolved would be to suggest the user learn a programming language and to interpret any resolution other than the simplest would be to suggest that the system is either unreliable (like a human) or sensitive to subtle details (like a programming language). Assuming users are stupid and unwilling to learn how to perform simple tasks is part of good UI design in GUIs, but it works against you even moreso in text-based interfaces; conversational interfaces that do not take advantages of UI advances in command line interface design can never become any more than toys.
Novel UI systems have a lot of the same problems as other novel pieces of media.
Novel UI systems have a lot of the same problems as other novel pieces of media. The existing widgets, even when they are objectively terrible, represent a familiar visual language to users — in the same way that samey blockbusters combine familiar franchises with familiar special effects, familiar story structures, familiar characterization cliches, and familiar forms of cinematography for an ultimately conservative work that is unlikely to lose money, and in the same way that bubblegum pop combines familiar artists with familiar rhythms and familiar harmonies to produce minor variations on a familiar style that is unlikely to be rejected for being too extreme. When a UI is big business and lots of money rides on it being usable for a wide audience, that UI will be conservative, and in the rare exceptions to that rule often we end up with something universally reviled (Windows 8, the MS Word ‘ribbon’ interface, Microsoft Bob, the Macintosh finder’s ‘galaxy mode’ from the early 90s, Google Wave, literally every UI change on Facebook or Tumblr). Because UI is a form of creative media that people have to use every day — an experimental UI for an important and widespread product is like having Steve Reich and Daphne Oram compose the soundtrack for a large supermarket chain.
However, since the situation is comparable to more traditional forms of media, we can borrow a bit of media’s solution. The solution to Hollywood sequelitis and horrible blockbusters is the slow and careful importing of ideas, techniques, and talents from experimental film, just as visually striking import giallo and the rise of auteur directors in the 60s and 70s revived the 50s slump in creative moviemaking (along with big names like Hitchcock grabbing talent and ideas from other fields like experimental animation — compare North By Northwest’s cinematography with that of Vertigo), and just as the constrained compositional environment of punk revived stagnant arena-rock (itself descended from blues- and folk-derived attempts to use experimental electronic hardware to inject vitality into overly commercial 50s rock/r&b, itself an attempt to bring in new influences to revive a stagnant crooner-centric ecosystem, etc., going back long before Mozart). And, we certainly used to do this: PARC was so influential because they were an isolated group of geniuses quickly iterating on UI design. To some extent, we still do this: video games often have unusual UI ideas, most of which don’t work, and Alan Kay is still playing with UI innovation with Squeak. The thing is, for the most part, fringe UI designs haven’t gotten pulled into the mainstream since the early 80s, when “GUI” became synonymous with “Alto clone UI” and anything that didn’t use a mouse began to be considered as not ultimately related to UI.
Some UI experimentation continues, here and there. The biggest example of UI experimentation being pulled into the mainstream in the past five years or so is, shockingly, in text-based UIs, wherein conversational user interfaces based around messaging systems have begun approximating the kinds of features and nuances that command line interfaces in the UNIX world have shipped with since the late 70s. SIRI is a cross between Eliza and Autocorrect, trying blindly to approximate a cross between ZSH and Google while remaining fixated upon being accessible to new users, and it’s an interesting enough experiment; special-purpose slack and twitter bots are more innovative because the cost of failure is low.
My recommendation: release smart people in the UX field into a situation where they can iterate quickly on interesting projects of their choosing, none of which will ever be marketed. Then, get a couple canny con-artist types like Steve Jobs to drop in, take a look at the end result of five or ten years of iteration, and distribute a simplified version to the mainstream.
The thing about cultural niches is that they are created by and for ‘mass media’ as a novelty generator. Having an isolated group of people experimenting on the fringes of culture without the necessity of appealing to large numbers of people or dealing with expensive equipment means creating an alternate media universe with new and interesting ideas and techniques each with its own associated small audience, and cultural niches can have their ideas and parts of their audiences imported into the mainstream on a temporary basis in order to keep the perception of the mainstream ‘new’: whenever something gets imported, it seems alien and novel to mainstream viewers precisely because they have historically been unaware of or isolated from the cultural niche in which it is considered an inevitable and incremental step. By importing proven ideas from fringe groups, mainstream media creators get the best of both worlds: they can seem creative while ultimately being extremely conservative with investor money.
This particular way of looking at niche media isn’t new; at the latest, it dates back to the french situationist movement of the 50s and 60s (and specifically to The Society of the Spectacle, in which it is a central theme), although it’s probably older. I’m going to suggest that — at least since the 60s but probably starting earlier — this has been an explicit part of planning, rather than (as Debord suggests) a fully autonomous process inherent in appropriative capitalist media structures.
Niches come in a hierarchy of sizes, and these sizes are related to cost and audience — two factors that were a lot more closely correlated before widespread internet access made the marginal cost of expanded distribution drop precipitously. Nevertheless, distribution is not wholly free: distribution on ‘mainstream’ scales is expensive even on the internet, and while it’s cheaper than television and newspapers, it still justifies (for instance) the internet advertising ecosystem. Furthermore, there are often production costs that are essentially ultimately scaling costs: more professional-looking things often are considered more accessible or desirable by a larger audience, and professional things at minimum take more time and/or experience to create even when they don’t produce greater material costs (and this is the problem that people who create media for youtube run into: youtube isn’t charging them to upload videos, but at a certain point they feel like in order to expand their audience they need better quality cameras and microphones, makeup, lighting, and eventually professional actors and makeup artists and animation teams, which is how the Vlogbrothers video ecosystem became an insanely conventional-looking media establishment despite zero distribution costs).
Art-house films and experimental music, despite hipster cred, are great examples of niche media ecosystems. And, if you have familiarity with either, you can trace the influences of the niche forms into mainstream, as happens over and over. But, both of these are explicit niches. They aren’t so much ‘long tail’, particularly historically: film production was, until the advent of inexpensive high-capacity digital video cameras and video editing software, absurdly expensive even in its cheapest forms, and the same is true of both conventional acoustic instruments and pre-PC synthesizers of both analog and digital variety. You might instead cast experimental film and music as a kind of skunk-works, like Xerox PARC: get the smartest and most creative people together to make things, throw money at them, and never try to actually market the results because the real value is in which ideas and techniques you can appropriate later when they’ve become mature enough to be safe bets. There are similar kinds of skunk-works situations on different levels of niche-ness: the BBC radiophonic workshop was fundamental in the genesis of electronic music and was essentially a government-funded audio special effects studio; low-budget BBC TV shows in the 60s and 70s like Doctor Who and The Avengers were as influential as they were eclectic and despite being popular and having national or even international distribution their low budget nature made them less conservative: the cost of mistakes was low. Television in general was, once upon a time, a skunkworks for film, and public access television continues to be a testing ground for people looking to get into producing film or television. As an example of another level in this hierarchy, comic-book-based and SF-based films have become mainstream while westerns have become fringe — a complete reversal since the mid-70s, and one that allows time for the rich loam of gunslinger mythos to lie fallow and compost itself into a more fertile genre while formerly ignored and ridiculed genre fiction in the SF sphere injects its hard-won fruits and seeds into the mainstream, which, vampire-like, returns from the dead by sustaining itself on the blood of living subcultures.
The particular economic shift of internet distribution — wherein even if scaling up distribution has minimal marginal cost, scaling *out* distribution geographically has truly zero marginal cost barring the extra steps of translation — has meant that each culture operates as though it’s a media niche. Really, this isn’t very accurate in the scheme of things. Gangam Style was never niche: the South Korean music industry is huge and profitable and represents its own mainstream, and both the jpop and kpop industries are considered overly commercial and conservative to people who are in those countries, the same way that bubblegum pop is considered commercial and conservative in the united states. What’s happening is a greater-scale version of what often happens when import barriers are lifted: a niche group in one culture defines itself in part by bits and pieces of another culture’s mainstream. Just as the influx of american gangster movies into post-second-world-war France was the antithesis (and french identity the thesis) creating the synthesis of the french new wave movement in cinema and the influx of american science fiction films into post-war Japan was a major influence for japanese tokatsu/SFX fandom (and as how, earlier, the import of Disney cartoons spawned anime), americans latching onto jpop and british tv imports has created new subcultures. Gangam Style’s widespread familiarity outside South Korea is a slightly more extreme form of the same phenomenon that made Yellow Rose of Texas a huge hit in China, Frank Sinatra’s My Way a huge hit in Japan, Blueberry Hill big in Russia, Jerry Lewis in France, David Hasselhoff in Germany, and pretty much any other unexpectedly popular import. “Big in Japan” is literally a stock phrase in the music industry because the combination of good media relations between Japan and the english-speaking world with a boatload of subtle but complex cultural differences makes for a huge set of english-speaking acts who undergo unexpected success once exported to Japan. But, today, anywhere with widespread high-speed internet access has the same relationship to anywhere else with it that the United States and Japan had in the 1980s. Every piece of mainstream media has the capability of being exported and recontextualized and becoming “big in japan” in some unexpected place.
How to become Steve Ballmer: implement stack ranking, throw chairs at people during meetings, yell a lot.
How to become Steve Jobs: make fun of your employees until they cry and/or become suicidal, and then later take credit for their work.
Why do so many people want to emulate famous assholes from industry?
Jobs is interesting insomuch as he’s a figure who is inextricably associated with industries that, on the technical level, he had no particular familiarity with, and on the management level, he was not particularly successful with. I can’t help but imagine that, for the most part, Jobs is famous for a combination of charisma and luck: he was willing to present himself as a figurehead and invite the suggestion that he had a greater hand in the products he is selling than he really does, but he would be forgotten if it wasn’t that his post-1997 decisions for Apple were largely just as profitable as his 1980–1985 decisions were unprofitable.
In this sense, he’s similar to other larger-than-life figures particularly in the financial industry: a professional gambler in a sense whose personality and convictions about skill borne from luck have led him to become interesting as a character.
After all, many of Jobs’ positive contributions were minor and aesthetic (the beveled corners of early Macs) or were positive only due to a series of clearly unpredictable accidents, while others were unambiguously terrible ideas both at the time and in retrospect (avoiding any expansion ports on the mac ostensibly to save cost, despite the mac still being double the price and half the performance of the Amiga 1000 and comparing even worse price-and-performance-wise to the Atari ST, both of which had plenty). Any attempt to suggest that Jobs was skilled relies upon attributing psychic powers to him.
Of course, this is a wonderful story to tell a tech industry in the midst of a bubble. Elevate a non-technical guy in management for a tech company to a godlike status and claim that all his good choices were the result of skill and all his bad choices were good choices, thereby suggesting that the universe is orderly and that charismatic people have magic powers that allow them to excel in business without trying. It’s no wonder that the Jobs Hagiography is so popular: like the Prosperity Gospel, it promises monetary success and public adulation and moral justification to the masses while justifying itself by claiming that those who succeed were destined to do so while those who fail have only themselves to blame.
Yes, but not all of us physically assault our employees and then are lionized for it.
Yes, but not all of us physically assault our employees and then are lionized for it. That’s reserved for a special class of people whose luck has ruled them immune to criticism.
I keep on being reminded of “Sirius Cybernetic Corporation” — the company in the Hitchhiker’s Guide series that keeps producing robots who are irritating because they are too human. To a certain extent, it makes sense to treat corporations and media properties more like machinery — not insomuch as you kick them when they’re not working (to be honest I wouldn’t trust anybody who hurts a machine on purpose) but insomuch as your transactions with them are expected to be limited in scope. You don’t need to smile and say “good morning” to a vending machine but you are expected to do so to a cashier, which is not desirable for you *or* the cashier (because you’re both put in a situation where potentially both parties are supposed to feign happiness and friendliness); media entities on the internet are worse when they break out of the transactional mold, because they benefit less from thousands of people sending them cheerful messages (or, you know, hate mail). Ultimately, we want our robots to work like robots, and we want our computer-mediated commercial interactions to hide the human being behind the screen and just show us the robotic interface.
I see why this trend exists, of course. Intimacy means replacing a dunbar slot. If some very lonely person has 150 of their slots filled by corporations and the rest by actual family, those corporations have a very good defense against competition for that particular customer: because to the customer, Coca Cola is a best friend and Pepsi is a creepy stranger. Of course, this polarizes the effect when everybody tries to do this, and it polarizes the effect even moreso for people who have rich social lives and have most of their slots filled by human beings: suddenly every celebrity and corporate entity is a creepy stranger trying to con you out of money by being overly familiar.
There are a couple of models for self-directed learning resources that are useful to look at, because historically they’ve been extremely effective.
One is the reference-material model. Its extreme end is a system like wikipedia: few things are more self-directed than a huge set of extremely detailed articles all hyperlinked together. If you have a goal in mind, you can search and probably find what you’re looking for if you know the right keywords; if you have no goal, you can idly wiki-walk and sate your curiosity indefinitely. (For media, tvtropes may be better than wikipedia, but tvtropes also has elements of the next model I’m discussing — the collegiate model — and may fit better into that category for various reasons. However, for very particular domains there are better resources than wikipedia: memory alpha, for instance, has better coverage of star trek trivia than wikipedia does.)
Another is the collegiate model. I call it that because this is the kind of thing that universities try to produce in both students and research faculty, and it’s also one of the arguments for think-taks and open-plan offices. I would argue that the most extreme form of it is in collaborative open question-answering communities like Quora and Stack Overflow, with comment-centric news sites with voting mechanisms (reddit, hacker news) and any other discussion system with large numbers of extremely active members (imageboards like 4chan for instance) coming in second. The point of a collegiate model is that you have people from different fields of interest interacting with each other in a dynamic way. Everybody learns, everybody takes the role of both teacher and student, and often the community itself produces new ideas and names them. A reference-material model is extremely effective at allowing a motivated learner to access existing well-documented ideas, but a collegiate model produces new ideas and allows motivated learners (even if they are beginners) to participate in the process of producing those ideas. That said, the reference-material model imposes some rigor on ideas (mostly by transplanting the rigor of existing established systems like peer review, academic publishing, canonicity, and journalistic standards), while the collegiate model’s flaws and benefits come from its flexibility. A collegiate model system will produce many ideas, and only a few of them will be good; furthermore, any bias in membership is likely to perpetuate itself (male-heavy communities often over time become misogynistic and then become even more male-heavy as they shut out women; the same is true of sample biases in terms of economic situation, race, religion, and even otherwise innocuous differences like tendencies toward logical positivism versus epistemic agnosticism or belief in free will versus determinism. For extreme examples, look at 4chan’s various boards, many of which are incredibly creative and have had a huge impact on the culture of the internet but each of which have consistent and absurdly biased cultures and habits that often don’t translate well between boards).
Attempts to automate self-directed learning have historically relied upon sticking a layer of gamification on top of the reference-material model — everything from hand-held trivia games to twenty questions to duolinguo do this (and memrise does this while sticking a tiny bit of collegiate model in a bag on the side by allowing people to create and vote on mnemonic devices).
I don’t buy the idea that these kinds of restrictions will ever become universally (or even nearly universally) enforced.
Why?
1. Because advertising, like many other forms of capitalism, is a race to the bottom that subverts any attempts to ensure quality. Advertising optimizes for metrics because it is paid based on metrics, and the least-effort means to provide comparable metrics will be used. Bad ads can risk being totally ineffective because it’s a safe bet that the people paying for them won’t realize they’re ineffective until the bad ads have already made enough money to justify the effort. 2. Because the largest ad providers in the world (Google’s Adsense and Doubleclick) already enforce quality standards for ads and already enforce many of the constraints you’ve suggested (to the extent that they can). By doing so, they yield part of their potential market to middle-men who specialize in low-quality scammy ads. As long as we have ads, we’ll have scammy, grey-legal ads, and the attempt by the largest providers to improve general quality has led to a bifurcation in the ad market between up-market ads and explicitly crap ads. Some ad-blockers allow up-market ads (those approved by Adsense) to be shown with some constraints; this just means that the scammy ads get shown exclusively to the less-savvy users who were more likely to click them anyhow. 3. Ads don’t really work particularly well in the best of cases. Having an ad-based economy on the web is, essentially, a bubble. Any ad is inherently an annoyance, so getting rid of the absolute worst ads won’t keep people from using ad blockers. The widespread use of ad blockers will just make it clear to companies formerly using advertising that the utility of advertising (particularly on the web and particularly through media that are clearly ads) is minimal. Already, there are plenty of ways in which the traditional web-based ad ecosystem is being circumvented: patreon and similar crowd-funding mechanisms are supplementing or replacing ad-driven content because ads don’t pay without enormous volume; large companies are sponsoring high-quality content that is largely unrelated to them (GM sponsoring Backchannel & the Cracked Podcast, for instance); Google Contributor uses the existing Adsense infrastructure to directly pay ad hosts with money uploaded by users without involving the advertisers at all; while that standalone bitcoin-mining machine intended to pay for content passively doesn’t have its economics straight yet, a similar system is perfectly feasible in theory; various attempts at automating the music royalty system seem likely to yield or inspire a general-purpose transcopyright-style micropayment infrastructure for derivative works & content reuse. Some or all of these things will eat up enough of the web ad ecosystem to make the changes you propose pointless long before enforcing them becomes practical.
Something not mentioned in the article is that OpenAI is in direct competition with several other non-profits whose goals are (or include) to ensure provably friendly AI — MIRI being the obvious example. I’m a little worried that all of these seem to be run on the west coast of the US and funded by the same group of ‘california ideology’ folks — if there are any hardcore marxists around who want to ensure provably friendly AI, I recommend that they set up some competition too!
One peeve I have with this analysis is: even if you consider a dysfunctional government to be similar to a dysfunctional corporation, Trump is not a skilled businessman: he just plays one on TV. For all her past failures, Florina has a better record in that regard.
Scene/Sequel: Post-Mortem of a Fiction Generator
Abstract Fiction generators based on goal-directed planning in a simple state machine can produce reasonably human-like output without explicit modeling of multiple characters by treating the planner as the narrator and protagonist.
Background Fiction generation using world models is not new. The same kind of planning used in SHRDLU (Winograd, 1972) drove early planner-based fiction generators like TALE-SPIN (Meehan, 1976). This class of fiction generators is a middle ground between ‘simulationist’ models, wherein a large number of variables are modeled carefully and a story is extracted from the progression between states by dropping most of the information modelled, and ‘texture’ models, wherein large pieces of preexisting flavor text are pieced together. (An example of the former would be the history of the world produced by Dwarf Fortress, and an example of the latter would be BRUTUS (Bringsjord, 2000).) TALE-SPIN-like planner-based fiction generators have, historically, like TALE-SPIN, modeled a set of characters and modeled their interactions. The most interesting stories generated by these generators remain the “mis-spun tales”, wherein assumptions by the generator are flawed in a way that produces absurd violations of common sense; the remainder of tales are fairly mundane transcripts of interactions between characters. As demonstrations of a functional model, these transcripts are serviceable; as stories, they lack interesting forms of structured conflict.
Author Jim Butcher presents a model of plot construction I call the ‘scene- sequel model’ that diverges from the idea of the world as a perfect simulation and focuses on the kinds of stories readers find interesting. While Butcher’s interest is in human-written stories, his ideas lend themselves to generative fiction more so than other seemingly more mechanical models of plot construction like Plotto and the BS2. Quickly: within the scene-sequel model, a story has a single protagonist and consists of scenes (wherein the protagonist attempts to achieve his or her goal) and sequels (wherein the protagonist reacts to the content of the scene emotionally and performs his or her planning); scenes have scene-specific goals that are either achieved or not achieved and each scene may introduce complications that change the protagonist’s state.
Current work For NaNoGenMo 2015, I present a fiction generator based on a simplified scene-sequel model for caper novels. In this model, there is only one character modeled: the protagonist, who is also the narrator. The content of the story is generated by the planner with the help of flavor text.
The story is generated by the planner from a ‘world model’, which is a state machine with associated flavor text. The world model consists of a list of state objects; each state object has a list of other states it can transition to (along with a probability for this transition to occur and optional flavor text for describing attempted, successful, and failed transitions). It furthermore contains a set of potential complications, which themselves are state names paired with probabilities.
The planner is provided with a world model, a weight for state transition success rate, a weight for complication accretion rate, a starting state, and a goal state. The planner keeps a ‘goal pool’ consisting of a set of state names and weights, and this goal pool is initially set to the goal state. Typically, the goal state is the overarching goal of the protagonist. As complications are accrued, they are added to the goal pool.
At each step, we perform a sequel followed by a scene.
The sequel consists of information produced by the planner, which (starting from the current state) performs a walk of the state space, depth-first, searching for the goal state. Because the world object contains cycles, the tree-walking portion of the planner is provided with a recursion depth limit. The planner, multiplying and adding probabilities, determines the immediate next state with the greatest composite likelihood of reaching each state in the goal pool — in other words, the likelihood of reaching each goal is determined, adjusted by its goal weight, and then a composite is produced for ranking purposes. During this tree-walking and goal-weighing process, at an adjustable rate, information about the planning process and the decisions being made is emitted in the style of first-person narration of planning in a caper novel:
So, I figured, if I tried to steal them jewels by trying to pass as a museum employee I’d have maybe a 35% chance of succeding. I’ll try to remember that.
If I’m trying to get a museum uniform, what if In order to steal them jewels, I tried to pass as a museum employee. That has about a 1 in 10 chance of working. So, I figured, if I tried to steal them jewels by trying to get a museum uniform I’d have maybe a 10% chance of succeding. I’ll try to remember that.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
Once the planner has produced a candidate state, it uses the likelihood of success for the specific state transition and the global state transition weight to determine whether or not it succeeds, producing the appropriate flavor text (or a default if undefined), and then produces the set of complications to add. If the state transition is successful and the new state is the end goal state, the program stops here; if the state transition is successful and the new state is in the goal pool, that entry is removed from the goal pool.
Here is an example of the full output, using a world model about jewel theft:
This is the story of that time I decided to try and steal them jewels.
So, I figured, if I tried to steal them jewels by trying to pass as a museum employee I’d have maybe a 35% chance of succeding. I’ll try to remember that.
If I’m trying to get a museum uniform, what if In order to steal them jewels, I tried to pass as a museum employee. That has about a 1 in 10 chance of working. So, I figured, if I tried to steal them jewels by trying to get a museum uniform I’d have maybe a 10% chance of succeding. I’ll try to remember that.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I failed to get a museum uniform while trying to go about it the obvious way. Bummer. Now I have to get a smaller gun. I still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I failed to get a museum uniform while trying to go about it the obvious way. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I failed to get a museum uniform while trying to go about it the obvious way. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I totally succeeded in my attempt to get a museum uniform by trying to go about it the obvious way. Yay! Now I have to get a smaller gun, again. I still need to steal them jewels.
So, since I’m trying to get a museum uniform I decided to steal them jewels by trying to pass as a museum employee. So, I have to pass as a museum employee. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to get a museum uniform.
I failed to pass as a museum employee while trying to get a museum uniform. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to get a museum uniform I decided to steal them jewels by trying to pass as a museum employee. So, I have to pass as a museum employee. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to get a museum uniform.
I failed to pass as a museum employee while trying to get a museum uniform. Bummer. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to go about it the obvious way I decided to steal them jewels by trying to get a museum uniform. So, I have to get a museum uniform. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to go about it the obvious way.
I totally succeeded in my attempt to get a museum uniform by trying to go about it the obvious way. Yay! Now I have to get a smaller gun, again. I still need to steal them jewels.
So, since I’m trying to get a museum uniform I decided to steal them jewels by trying to pass as a museum employee. So, I have to pass as a museum employee. I also have to get a smaller gun. I also have to steal them jewels. Right now, I’m trying to get a museum uniform.
I totally succeeded in my attempt to pass as a museum employee by trying to get a museum uniform. Yay! Now I have to escape the museum. I still need to get a smaller gun. I also still need to steal them jewels.
So, since I’m trying to pass as a museum employee I decided to steal them jewels by trying to steal them jewels. So, I have to steal them jewels. I also have to get a smaller gun. I also have to escape the museum. Right now, I’m trying to pass as a museum employee.
I totally succeeded in my attempt to steal them jewels by trying to pass as a museum employee. Yay! Now I no longer need to steal them jewels. Now I have to heal my arm wound. Now I have to heal my chest wound, too. I still need to get a smaller gun. I also still need to escape the museum.
THE END
Here is the world model that produced that story:
world={} world[“go about it the obvious way”]={“get a museum uniform”: {“probability”:0.5, “complications”:{“get a smaller gun”: { “probability”:0.9 }}}, “go to the ninja supply store”:{“probability”:1}, “go to the gun store”:{“probability”:1}} world[“go to gun store”]={“get a smaller gun”:{“probability”:0.7, “complications”:{“find my stolen wallet”:{ “probability”:0.2}, “success_descr”:[“The gun store carried an antique gun intended for defending women on bicycles against dogs in the late nineteenth century. “, “The gun store carried a half-scale airsoft dart gun version of a Walther PPK, and poison darts. “]}, “descr”:[“The gun store was a tiny brick building by the side of the highway, in the bad part of town. “], “success_descr”:[“The owner glared at me, and then at my ID, and then back at me. Then he grunted, accepted my cash, and handed me the new gun. “], “failure_descr”:[“After banging on the locked door for ten minutes, I noticed a tiny sign at the lower left hand corner of the window embedded in the door. It had hours. It turns out, this store is closed on Tuesdays. “]}, “goal_reqs”:{“or”:[“get a smaller gun”]}} world[“get a smaller gun”]={“go to gun store”:{“probability”:1}, “go about it the obvious way”:{“probability”:1}, “pass as a museum employee”: {“probability”:0.3}} world[“get a museum uniform”]={“pass as a museum employee”: {“probability”:0.3, “complications”:{“heal my leg wound”:{ “probability”:0.2}, “escape the museum”:{ “probability”: 0.7} } }, “descr”: [“The costume shop was tucked into a strip mall down town, between a laundromat and a chinese take-out place. It smelled like soap. “], “success_descr”:[“There was a perfect museum employee uniform sitting on the rack to the left of the entrance. “], “failure_descr”:[“After looking through the racks several times, I finally decided to ask the cashier — a wrinkled but plump old woman with a puff of curly white hair — if she carried museum employee uniforms. She shook her head, and I left, dejected. “]} world[“pass as a museum employee”]={“steal them jewels”:{“probability”:0.7, “complications”:{“heal my leg wound”:{ “probability”:0.3}, “heal my arm wound”:{ “probability”:0.3}, “heal my chest wound”:{ “probability”:0.3}}}, “go to the hospital”:{“probability”:0.9}, “reqs”:{“or”:[“get a museum uniform”]}} world[“go to the ninja supply store”]={“get a smaller gun”: {“probability”:0.4, “success_descr”:[“In the glass display case, there was a poison dart gun that looked like a fountain pen. I bought six! “], “failure_descr”:[“The cashier claimed that they had a moral aversion to projectile weapons, and thus did not carry them. “]}, “purchase a black leather catsuit”:{“probability”:0.7, “success_descr”:[“A beautiful black leather catsuit greeted me from the rack to the left of the doorway. “], “failure_descr”:[“All the black leather catsuits they had in stock were sized for literal cats. “, “All the black leather cat suits they had in stock were way too big for me. “, “All their black leather catsuits were covered in shiny chrome studs and buckles, and wouldn’t help me disappear into the night at all. “]}, “purchase a grappling hook”:{“probability”:0.7, “success_descr”:[“There was a grappling hook with two hundred feet of rope sitting right behind the counter, on display. “, “I spent twenty minutes looking through the discount bin, before finding an absolutely perfect grappling hook for thirty cents. When I went up to pay for it, the cashier waved me off — no charge. “],”failure_descr”:[“\”Are there any grappling hooks in stock?\” The cashier, impassive behind his mask, shook his head slowly in response to my question. Then, after a moment of staring at me, he threw a smoke bomb at his feet. I found myself outside the shop, which was now locked. “]}, “descr”:[“The ninja supply shop was in the middle of the second floor of the mall, between a Hot Topic and a Zappo’s. It was dimly lit, and the scuffed floors had a fake tatami-pattern print. There was a wad of gum stuck to the doorway. “], “failure_descr”:[“The shutter was shut, and a great big lock hung from the side of it. “, “The door was shut and locked, and a sign said \”Back at 2:00\”. It was four. I waited until six. “],”success_descr”:[“As I entered, a machine emitted a little beep to indicate that customers were about. The cashier appeared out of a plume of smoke behind the counter. “]} world[“purchase a black leather catsuit”]={“sneak into the museum at night”:{“probability”:0.8}, “go to the ninja supply store”: {“probability”:1}} world[“purchase a grappling hook”]={“sneak into the museum at night”: {“probability”:0.9}, “go to the ninja supply store”:{“probability”:1}} world[“sneak into the museum at night”]={“steal them jewels”: {“probability”:0.8, “complications”:{“heal my leg wound”: {“probability”:0.3},”heal my arm wound”:{ “probability”:0.3}, “heal my chest wound”:{ “probability”:0.3}}}, “go to the hospital”: {“probability”:0.9}, “reqs”:{“or”:[“purchase a black leather catsuit”, “purchase a grappling hook”]}} world[“go to the hospital”]={“heal my leg wound”:{“probability”:0.9}, “heal my arm wound”:{“probability”:0.9}, “heal my chest wound”: {“probability”:0.7}, “escape the museum”:{“probability”:0.7},”goal_reqs”: {“or”:[“heal my leg wound”, “heal my arm wound”, “heal my chest wound”]}} world[“heal my leg wound”]={“go to the hospital”:{“probability”:1}, “get a museum uniform”:{“probability”:1}, “get a smaller gun”:{“probability”:1}, “go to the ninja supply store”:{“probability”:1}} world[“heal my arm wound”]={“go to the hospital”:{“probability”:1}, “get a museum uniform”:{“probability”:1}, “get a smaller gun”:{“probability”:1}, “go to the ninja supply store”:{“probability”:1}} world[“heal my chest wound”]={“go to the hospital”:{“probability”:1}, “get a museum uniform”:{“probability”:1}, “get a smaller gun”:{“probability”:1}, “go to the ninja supply store”:{“probability”:1}} world[“steal them jewels”]={“steal them jewels”:{“probability”:1, “complications”:{}}} endGoal=”steal them jewels”
Absolutely in agreement with you, here. This is a major pet peeve.
The way I usually phrase this position is: “For any definition of creativity not formulated specifically to exclude them, computers are already capable of creativity, and have been since the 1950s at the latest.”
Why you should care about generative text
Generative text gets some press. Unfortunately, like many other technical fields that attract shallow coverage, generative text has been the subject of minor variations on the same article every few months since the mid-1950s.
The typical article on anything related to generative text (particularly generative fiction) has the following pattern:
“[Insert quote here].”
You would think that was written by a human, wouldn’t you? Well, reader, you are a dumbass, because that was written by a machine!
But don’t feel too bad, because here is an example of the machine writing something comically terrible and absurd: “[insert quote here]”.
“[Insert quote here],” says one of the three researchers in the field that we crib quotes from previous interviews from every six months.
In conclusion, I’d like to reassure you that computers won’t be writing all the novels soon. Or will they? Shock!
If you haven’t read articles about generative fiction before, no matter; they’re all like that. If you have, you know what I mean. This article is not that article. I’m going to tell you all of the interesting things about generative text that those articles didn’t cover.
You should care about generative text if you care about:
Video games
Video games have been trying to expand their clout as a narrative medium. While there are a lot of ways to do narrative in an interactive medium, one of the most obvious is to have interactive dialogue. After all, dialogue usually drives narrative in films, books, comics, and TV.
Unfortunately, dialogue trees don’t scale well: after all, they grow exponentially, and inconsistent or inadequately varied dialogue is extremely obvious. As a result, dialogue-driven interactive narrative is typically limited to big game studios, and truly complex dialogue trees are rare even in game genres that have that as their primary technical focus (such as VNs and adventure games).
The same way that, five years ago, indie game developers started looking toward procedural world generation as a way of creating large maps that allowed them to compete with large development houses’ sandbox games in terms of scale, indie game developers now are beginning to look at the various ideas in the field of text generation for ideas about automatically generating varied dialogue and dialogue trees from models of character and narrative. The next Minecraft might derive its scale from procedural expansion of NPC dialogue instead of procedural expansion of the map.
Journalism
Right now, because of the use of advertising for monetization of content, a lot of internet ‘journalism’ is clickbait — in other words, content optimized for page view counts rather than for sustained attention. Clickbait is generated at low cost by content farms; it’s poor-quality because being high-quality is a net loss, and it’s short because that’s cheaper than being long. Clickbait optimizes for two things: number of ads on a page and number of people who will click a link. However, generative text is extremely promising for content farm owners: after all, even if you’re paying content farmers pennies an hour, you’re still paying more for these humans than you would for machines, who can generate far more content far more quickly with only slightly lower average quality. The kind of A/B testing that content farms use for optimizing their headlines is, furthermore, a perfect match for existing methods by which relatively simple AIs can use feedback to improve their headline generation — and AIs are already pretty good at generating clickbait headlines. In other words, machines might well easily replace the lowest end of internet journalism.
At the same time, text generation is already beginning to supplant the lowest end of traditional paper journalism, with various organizations automatically generating minor financial and sports stories. This frees up human writers to be put on more interesting stories, or to alternately be fired, depending upon the skill of the writer and the financial situation of the news agency.
Both of these effects are truly huge potential economic shifts in these industries. And, they have the potential to be truly positive, as well. Consider Buzzfeed, which makes its money off inane clickbait content and then turns around and funds wonderfully deep serious journalism by serious journalists about interesting subjects with all that shit-click money: if they automated their low-quality high-lucre content, they could shift more of their workforce toward high-quality journalism. Alternately, the slightly lower quality of machine-written articles versus content-farmed articles might accelerate the devaluation of clickbait and cause alternatives to ad-based monetization to become more popular more quickly.
Psychology
The effectiveness of generative text is the result of an interplay between the design of the generator and the human audience. The best generators lean heavily on the human element, using rich associations and loaded structures to convince the reader to project meaning onto the text, which itself is very often structurally simple. All of this is to say that a large part of the design of text generators is psychology. Invert this, and it’s not at all surprising that text generators are being used by experimental psychologists to probe the human mind.
Just recently, there’s been press coverage of a study using a new-age BS generator to study personality traits associated with the projection of meaning , as well as of a group of older studies using joke generators to study the mechanics of humor. Text generation allows psychology experiments to scale up and to have extremely fine control over the material they use; when text generators used in psychology experiments have their source made available, later experimenters can tweak the generator in various ways in order to easily test variations on the original experiments, and text generators can be hooked up directly to systems like Mechanical Turk that allow experimenters to expand their studies outside the college campus.
Spam
The spam industry is the only segment of the tech world to really take text generation seriously. Spammers have been using techniques like text spinning to trick both humans and AI filters for more than a decade. As technology improves, spam will get better. Any advances in text generation will probably be employed by spammers first.
Ebooks
A few years ago, Amazon had a problem with machine-generated reference books of poor quality. These reference books would be produced based on search queries, which were fed into Wikipedia and the resulting pages combined, initially into a print-on-demand book but later into ebooks. It’s a little unclear to me how Amazon fixed this problem, but it doesn’t seem to be such a big deal anymore.
However, this is not the only con in Amazon’s ebook ecosystem. Today, the big money is in creating hyper-targeted erotica[1, 2, 3]. Consumers of erotic fiction often don’t care very much about prose quality, or are prevented by fear of social stigma from being vocal in their criticism of poor-quality prose in erotica. Erotic fiction is extremely popular, and hyper-specific subgenres have their own categories on Amazon, which means that it’s fairly easy to get to bestseller status in a single category (and thus have a boost in sales resulting from a listing on bestseller pages).
Again, text generation is a very good fit for erotica. It is easy to generate arbitrary amounts of poor-quality erotica. Erotica is either effective or comical: even bad erotica is good. While existing cons for Amazon erotica ebooks often involve taking public domain erotica and publishing it with a few easily automated changes, there’s no reason that brand new erotic fiction couldn’t be generated in various categories at the kind of rate that only machines can keep up with.
UI design
Partly because of the hype behind Slack and Siri, conversational UIs have become trendy recently. Those of us of my generation will recall the failure modes of conversational UIs. (Remember SmarterChild?) Understanding how to perform good text generation and take advantage of the Eliza Effect will help future conversational UIs feel less static and more lively.
Determining whether or not forming an emotional attachment to an AI-driven corporate mascot is a good thing is left as an exercise for the reader.
The arts
There’s a long history of musicians and authors employing ‘writing machines’ to help produce inspiration from old content. These ‘writing machines’ vary in details, from forms of traditional bibliomancy to dadaist or surrealist writing games to oulipo-style constrained writing to the use of computer programs for scrambling text. Phillip K. Dick used the I Ching to determine the plot of his award-winning novel The Man in the High Castle; William S. Burroughs, Thom Yorke, and David Bowie all used cutups to inspire their work (going so far as to sometimes use the text produced by cutups directly); Doctor Seuss’s unique style was largely determined by heavy constraints on his vocabulary.
Text generation technology presents a new set of ‘writing machines’ for authors, poets, and musicians to collaborate with and build upon. The difference is that, where previous mechanisms primarily created stylistic affectations or merely inspired narrative tangents, more recent text generation technologies are capable of producing a variety of engaging and interesting narratives by themselves.
Outside of the more traditional forms, pure text generation has come into its own in the form of text generator driven twitter bots. A variety of pitch bots use simple templates to produce amusing and evocative ‘pitches’ by combining familiar forms with mismatched corpora. Bots exist that automatically generate biting satire of overused, shallow, or damaging trends.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
If you find the others on the road, kill them
If you find the others on the road, kill them
The last part of Tim Leary’s famous quote is often forgotten: “turn on, tune in, drop out, and find the others”. Leary claimed it was the most important part: after all, all of the outsider weirdo creativity goes to waste if it’s stuck in your skull. Unfortunately, nothing has prepared us for the strange times.
There is no counterculture anymore, because there is no mainstream culture anymore. (Maybe there is for you, if you live in a theocracy with state-regulated media and heavily limited internet access. If you are, congratulations on reading this post, and also why are you reading this post?) The spectacle has consumed and absorbed the early-90s utopian cultural pluralism of the John Perry Barlow set, just as it absorbed the culture/ counterculture division of the 50s and 60s, which is why people like Stewart Brand are rich de-facto plutocrats now. And in the global village, no matter how freaky you are, you can surround yourself with precisely the same kind of freak and live in your little filter bubble. Finding the others, in this case, is a bad thing.
Don’t find the others. Find the Others. Stick apart. If you agree with someone, take that as a warning sign: do the two of you agree for good reasons, or are you just incidentally the same kind of freak?
The function of the lunatic fringe is not to become a comfortable space for you and your like-minded friends. There are other places for that. The function of the lunatic fringe is to thrust half-baked ideas into a violent orgy of death and copulation until they become more fully baked or die the death of warriors.
Paranoia as a design choice
Paranoia as a design choice
A lot of people have been making a lot of noise about this post about bitcoin. Even before that, people were making noise about the bitcoin community being full of goldbugs. The thing is, bitcoin is a cryptocurrency whose primary design goal is paranoia, and all other factors are secondary. Nothing about the bitcoin design or the bitcoin community will make sense if you don’t recognize the intentional placement of paranoia in its design.
Bitcoin isn’t the only technological system built explicitly around paranoia. In the crypto and computer security communities, building systems around paranoia is normal, in the same way that it’s normal in a late-capitalist environment to build systems around profit maximization. SSH is designed around paranoia (and this is why you no longer can enable ‘cipher=none’ in SSH at compile time); the WWII-era ‘double-cross’ espionage structure that underlies cold-war spy thrillers is a social technology built around paranoia; the hospital policy surrounding disposal of drugs is based around paranoia.
Systems that are intended to be secure but are not designed around paranoia tend to become security theatre if they function at all — consider the HTTPS certificate system, the TSA, gun lock technology (and indeed much of regular lock technology), community policing, credit card and check based payment mechanisms, website password protection prior to widespread access to two-factor authentication, and highway toll stations. Security theatre is not entirely ineffective — after all, most people follow the rules most of the time and security theatre hints at the idea that the rules are important enough to be enforced — but it typically combines most of the inefficiencies of a truly secure system with most of the insecurities of a truly effective system.
When we’re talking about bitcoin, we’re talking about a conception of money wherein the government is the enemy. We’re also talking about a conception of money wherein banks and corporations are the enemy. We’re talking about a system where the assumption is that hours of clear time is a small price to pay for proof that double-spending is nearly impossible, and wherein the ideal community of users is a global network of wealthy individuals who never communicate except by exchanging money for goods or services and who have no friends or loyalties. Bitcoin is a currency designed for people who think of other people as potential enemies first.
There are circumstances where that view of the world is accurate.
This doesn’t really explain why large banks are investing in bitcoin, or why bitcoin startup companies exist. Even mining pools are too communal: the moment that the interactions of the bitcoin universe fail to consist solely of pristine blocks of pure value reeking of the scent of poorly-veiled animosity and apprehension, we are on the slippery slope to collusion-based attacks to debase the currency. Mining pools have gotten close to 51% before and have backed off out of the good in their hearts; however, whenever the good in people’s hearts matters, this represents a failure of any security model based around paranoia.
Of course, at least for the startups, we can look to the california ideology for why bitcoin was adopted by actual organizations. We can look to strange hybrids like ESR. The thing about the paranoid position is that it can be easily distracted from old threats by new threats; this is why libertarianism is a right-wing ideology now while it was a left-wing one fifty years ago. If you distrust the government because they are the one with all the guns and money, you’re right — they have both those things and they may or may not be on your side; but, if you support them wholeheartedly as soon as another party attacks, you’re gonna have a bad time. Bitcoin’s paranoia is against any kind of collaboration, but the bitcoin community’s mix of the generally paranoid and those paranoid only against the bugaboos of the moment (along with various collaborations, scams, and internal politics) have caused a mix of fracturing within the community and collusion with forces from without it. Bigotry is fractal, and as soon as the policy of paranoia in bitcoin became twisted, the community shattered.
This is a pretty common failure mode. Think of mole hunts in the CIA under James Jesus Angleton. Think of Jim Jones. Think of Ayn Rand’s last amphetamine-fueled years. A failure of appropriately applied paranoia cannot be remedied with an excess of poorly applied paranoia.
To be sure, these hollywood formulations are male-centric.
To be sure, these hollywood formulations are male-centric. But, to suggest that those aspects will translate is to assume that it’ll even work. The requirements of hollywood UX are very different from real UX, and this is most extreme with conversational interfaces — to the point that the kinds of interfaces that work in films do not work IRL and vice versa.
After all, in a film, any computer interface must be immediately recognizable and understandable to people seeing it for between a few seconds and a few minutes. A real computer interface, on the other hand, can be initially a bit confusing if that makes it more usable in the long run. To be more concrete: a film’s UI must have bright colors and a giant font and only clearly show plot-relevant information (otherwise the viewers wouldn’t know where to look), while a real UI needs to show whatever information the user might find useful (life has no plot) and giant fonts and bright colors would annoy long-term users. This kind of problem extends out to conversational interfaces in films, which are driven by exposition and spectacle. People in films don’t even talk to each other the same way as they do in real life, because dialogue is optimized for showing off plot and character rather than engaging in the kind of social game-playing that dominates real dialogue.
A real conversational interface ideally resembles the interaction between two technical professionals: in other words, over time a shortened and optimized jargon is developed for more efficient communication, so that the kind of conversation that begins looking like an interaction between strangers eventually ends up resembling a command line interface.
A conversational interface based on mimicing conversational interfaces from movies won’t support the patriarchy, because it will be almost completely useless — worse in all ways than a simpler system based on individual commands. Anyone who uses such an interface is merely screwing themselves over.
The idea of a Bowie
The idea of a Bowie
I was surprised that I was affected so much by Bowie’s death. Despite being a fan, I don’t have a huge appreciation for the level of musical craft in Bowie’s music — I see him as an experimenter, and I appreciate him in the same way as I appreciate Steve Reich or Skinny Puppy: I liked the way that he wasn’t afraid to alienate his audience in pursuit of some pure expression. Nevertheless, I feel like a void has opened in the pit of my abdomen, slowly sucking my entrails out from the inside. I used to make fun of people for mourning celebrities, but I’ve been shown firsthand that the kind of connection between an artist and his audience is a real one, and the pain that occurs when it’s severed is real too.
The worst part is, Bowie isn’t even dead. David Jones is dead. Bowie was never alive in the first place.
This is not an idle distinction. David Bowie is a fiction-suit. David Bowie has no more to do with David Jones than Ziggy Stardust does, and we killed Ziggy a long time ago.
What David Bowie is, ultimately, is not a human being nor an icon but a set of patterns, practices, and behaviors. David Bowie is a way of life. Buried in the coverage of Jones’ death, clues about how to Bowie are being uncovered and re-aired: information about the creative process from old interviews, and some of the theory behind it.
I didn’t know David Jones. But, to a certain extent, I, like other fans, know Bowie, because Bowie is a construct made exclusively to be known by fans. Here is my attempt to put together the beginning of a list of things that made the Bowie fictionsuit interesting.
1. Bowie is a container for other personalities. Even as Bowie is not Jones, Bowie also constructed and discarded other identities, acting as a buffer between these identities (who are ultimately the real rock stars) and Jones himself. Each of these identities is a mythic figure with an epic arc. Each of these identities has a different angle on both the world and Bowie. Ziggy Stardust and Halloween Jack are warped Christ-figures: in a gnostic manner, they become minor messiahs for a group of disaffected youth while corrupting themselves in the process, and in the end they are bodily destroyed by invisible powers greater than themselves whose bidding they were unknowingly doing and whose goals were ultimately selfish; Aladdin Sane and Cracked Actor were more meta: a window into the fracturing of personality. An album was a story of another Bowie fictionsuit, and albums iterated on previous albums, rewriting their story: Outside rewrote Scary Monsters, which rewrote Diamond Dogs, which rewrote The Rise and Fall of Ziggy Stardust, which itself rewrote sections of Space Oddity (consider Memory of a Free Festival). Every Bowie egregore is created, set on its path up the arc of the monomyth, and killed off and discarded at the peak of its cultural power. It is in this way that Bowie (and Jones himself) avoided the rock star’s mythical downfall. 2. Bowie took creativity very seriously, and availed himself of mechanical means to expand upon ideas and styles. He used cutups for twenty years, both to expand inspiration via juxtaposition and to create actual content. References to Thelema are indications that Bowie was familiar with the english cabala and with Tarot — which is to say, he is familiar with the history of using bibliomancy as a mechanical means of obfuscating existing patterns to generate new insights. When he said that the cutup method is “a very western Tarot”, this indicates the depth of his insight: after all, Tarot is not (historically) eastern — even the people who make dubious claims as to the long history of Tarot only put it as far east as Egypt, while actual historians would say that the game of trumps originated in fifteenth-century Italy and the use of Tarot in divination originated in nineteenth century England under the Golden Dawn — so what does it mean to be a “very western” Tarot? Geographically, the cutup method in its modern form was developed in the international zone of Tangiers in Morocco — so, west of Egypt and west of Italy but southeast of England — and its predecessors in the form of surrealist and dadaist writing games were european. Instead, we can say that the cutup is a culturally western Tarot: it is a purely mechanical means of symbolic rearrangement developed by an american heir to a calculator fortune, and it eschews the kind of monastic memorization of static correspondences that Tarot relies upon. It is a more culturally western (by which I mean empiricist-pragmatist-positivist) occult tradition than the Western Occult Tradition. By the time that Bowie was writing Outside, he had graduated to an even more mechanized and even more western form of cutups: a computer program developed in California for the Macintosh that chopped sentences into five-word columns before shuffling the columns. 3. Bowie wasn’t afraid to appropriate other people’s ideas and scramble them. In some sense, Bowie himself was like a cutup machine. At various points, in interviews, he eschews any sense of intended meaning behind his words and actions. To an older crowd, this is sometimes seen as the mark of a pretentious poseur; but, this postmodern attitude toward meaning and toward the function of media is in line with bibliomancy historically. Bowie was a catalyst for other people’s growth, because he put things out there that other people had to fit together, and because they fit those things together using pieces of themselves. Sometimes, this cabalistic attitude toward art merely affected the way identities were presented; other times, it had drastic effects on the media he was working in (such as Bowie’s outsized affect upon the glam, punk, and rivethead genres). Juxtaposition of genres creates new genres while expanding those genres that have been juxtaposed. Bowie did this as a habit. 4. Bowie didn’t stop. He didn’t have to. By sacrificing his fictionsuits, he saved himself and was able to work another day. As a result, he was able to release albums at a fairly steady pace from the mid-60s to 2016. Part of his legacy is the fact that he was so productive for so long, and part of his legacy is that he never stopped innovating. 5. When it was time for David Jones to leave, Bowie made a show of casting him off too. The body that had held the idea of Bowie is gone, but in addressing directly the same way he had with Ziggy, Bowie took control of this situation. One way of reading Blackstar and Lazarus together is to consider it a fight between Jones, who wants to keep being Bowie, and Bowie, who wants to cast off Jones in order to live on in another body. (I am treating Lazarus as being from the perspective of Jones and Blackstar as being from the perspective of Bowie.)
Obviously, all of this was combined with a lot of talent, hard work, energy, and heaps of pure physical sex appeal. Invoking the Bowie godform is never going to be easy. But, neither is it impossible.
Bowie is dead. Long live Bowie.
A cutup of Palin’s Trump endorsement speech
part why holy all more  and and around works but  they of Its youll The  theyve he the a well  for keep youre hero us  has ha to hard And  jobs as more Mr to  private conservative relax isnt was  go very Well Senate try  theyre Right other code day  who our they the all  infrastructure to the rogue for  are and multibillionaire they would  to to are Not of  theyre is we much them  to that hang changey going  We so the his the  known passionately tell our he  these racebaiting Heads division would  you proves vets Iowa sides  something ethic reform and hes  down him Donald busted quiet  a important they Theyre great  you brought with but than  or to then cooks and  are goodness you Its You  right team president why of  friends a you where gravy  He some told America America  a couple go and suck  others not for every spinning  sector am of and enemy  say fulltime from our I  tried to them Greek You  Well coming week debated political  their vest being tiny Parenthood  train part for bit and  to then to blank towns  to team multibillionaire When this  deserve horse so both ISIS  Ive voters to the private  unify selfiesticks to main Trump  the refreshing gonna things you  those So establishment let supporters  its and Trump contest fabric  and jobs would under look  see to keep to finally  and that Thank take titles  man of of Iowa to  of involved really is of  on machine on that able  of movement veil he values  issues packs ready the beautiful  to clingers and people be  they success the care on  even a with You hardhats  of but captors with were  a supporters again to overseas  passionate thats is teachers Trump  only was its attack were  and get of transformation year  teamsters helps we you watching  not deals his basic one  the break can out so  hands to as just for  until are new run command  going for so respect he  were it drill kick high  Democrat a I the hes  of eating enough and must  VP leader status off able  he out you know All  political to That needed and  thats knows tells in say  establishment promise too the that  whole over to dont is  to angry off and and  Look we on let ABCs  issues the its promised can  When ready me be families  media the and would let  share us is that beat  member up hopey Now Because  theres capture and they libertarian  more House Donald were exposed  our You actually folks races  been be trade is quit  And God the able States  this strict stage would there  putting Ill will not freedom  America stage because in Not  these that to his rest  for the No on other  table their tragic again to  Chicago havent Tru deals street  what tear that theyre in  this things community Doggone competitive  behind give of is proSecond  find worked we Fighters America  work your says again elitist  that very squirmishes borders and  to well for no troops  got America frontrunner great havent  budgets rollers thing to quo  I we Donald you nations  know to weaker it the  ass Ill And and to  had theyre A all other  betcha are this a this  up changes man remember commonsense
Right on.
Basically, if you don’t need interactivity, there’s no excuse for having it. If the point of the website is to display text, it should just *display text* and neither javascript nor css is justified (let alone a CMS).
To be honest, your site isn’t quite minimal enough for my tastes. You could have gotten away with pure HTML and no formatting, and it would have been better. But, at least you’re on the right track.
I wonder if your experience is typical.
I wonder if your experience is typical. I have a lot of projects on github (and have for some time), and I’ve gotten something along the lines of four issues/ pull requests total. Having entitled trolls bothering you about your code doesn’t seem like it’s something common on github (although clearly they could be giving you the tools necessary to handle this better), and is (paradoxically) an indication that your project is extremely popular.
Part of the point of a system like github that makes forking projects easy is to eliminate pressure on upstream developers to implement features. In other words, while on an older project management system like sourceforge and google code having requests for features directed to the original authors of some code would be normal, github goes out of its way to shift the pattern such that people who want features are expected to create a fork and implement them themselves, before optionally issuing a pull request to submit the patch back to the source.
I wouldn’t blame github for this. Instead, somehow, you were targeted by a group of assholes who are too incompetent to implement the features they desire and too lacking in empathy to realize that you have more important things to do than to follow their whims. It’s absolutely appropriate to expect them to do their own homework, so to speak, and it’s a shame that they don’t seem inclined to do so (or to observe the basics of decorum).
Read Intuition Pumps by Dan Dennett.
Read Intuition Pumps by Dan Dennett. It’s by far the best book on thinking tools I’ve read. (Compare directly to lesswrong’s “sequences”, which are similar in form but ultimately much inferior in both style and content.)
I find this post significantly funnier than I should.
MMOCs/MOOCs are probably the wrong match for the kind of learning technology presented in The Diamond Age, which while fictionalized, was really a small extension of existing trends in *educational game technology* that existed in 1995. In other words, rather than fitting into trends related to the extension of traditional academic curricula with ed-tech (as MOOCs and video lectures do), it fits more closely with the Papert/Kay ‘mathland’ concept (and thus with things like LOGO, Croquet, MindStorms, and other systems built to encourage autodidacts to explore systems).
The middle ground — and a very useful and popular middle ground it is — between typical formal-instruction MOOCs and ‘mathland’-style environments is the new breed of quiz-focused gamified ed-tech services like Memrise and Duolinguo. Despite being truly useful for a fairly limited set of fields (i.e., those fields where the number of right answers tends to be limited and people can reasonably expect to learn from quick quizzes with immediate feedback in the absence of initial instruction — in this case, vocabulary, grammar, and lists of facts), they are very useful in these fields.
Both The Diamond Age and Ender’s Game reference and borrow from educational games from the 80s and early 90s, adding context-awareness and extra flexibility as their primary technical advancement; for the most part, the actual passages described from the educational video games in both of those books are cribbed from real games, and there’s no reason why such games could not still be made.
We have yet another very popular field of educational games: simulations (Sim City, Civilization) — we simulate a complex existing model of a system and the player is expected to become competent at understanding the behaviors of that model in order to execute his or her tasks. Closer to the Croquet model is Minecraft, which is educational insomuch as necessary tasks embed a great deal of fairly esoteric information about geology and about historical methods of producing common goods: Minecraft makes explicit the tech tree that Civilization uses as a progress bar, and requires players to demonstrate a familiarity with the general idea of what tools and materials are required for a given technology along with the previous technologies required to produce those tools and materials, at a much more granular level than Civilization.
Yes MOOCs rely too much on bad video. But, MOOCs are thirty years behind the curve on ed-tech and have never even attempted to be cutting-edge or maximally effective. If they had, they would at least focus on testing mechanisms rather than on methods for replacing books with AV presentations — because if engagement is sufficiently high, books are just fine.
Talking about Dribble is undercutting exactly how stupidly static the western idea of ‘good design’ is. Instead, we can look at how dominant themes in design go back to being influenced by Apple’s design standards from the early 1980s, which themselves are basically a simplified and dumbed-down version of late-1940s and early-1950s trends in modernist minimalism.
While there were plenty of interesting ideas in design from 1950 to 1980, because of Jobs’ reality distortion field we basically threw all of that out and have been increasingly influenced by this static throwback FrogDesign BS that was pretty questionable in 1979 (along with the underlying Jobsian ideology that consumers are too dumb to deal well with choices and must have decisions made for them — a necessary element in the application of extreme minimalism to ostensibly general-purpose machinery).
Today’s idea of good design is essentially: flat, spare, minimalist graphics with low information density and few user choices. Never use color if you can use white; never use two colors where you can use one; never use two buttons where you can use one and never use any buttons if you can get away with not having any; never give the consumer an option if you can please more than half of them by choosing first; if a user disagrees with a designer about usability then the user is wrong. These rules make some sense for a house or a toaster, and are defensible for a coffee machine or a toilet; they make no sense when applied to computers and even less sense when applied to computer software.
There’s a problem with people recommending ‘smart guns’: they clearly haven’t looked at the existing market for mechanical trigger locks, the best of which are still comically insecure enough that a six year old with a paper clip and two minutes can disable them. The market doesn’t have sufficient incentives to make gun locks as secure as bike locks, and it shows. Producing a more complex system for in-firearm locking, featuring electronics and biometrics, will not be profitable if people aren’t willing to pay a few dollars for a trigger lock that actually has tumblers.
There’s a system in place for gun owners who want to secure their firearms: gun safes. Gun owners who do not want to secure their firearms will not pay extra for an add-on or a special more secure gun, and people will quite reasonably resist any legislation that requires new firearms to have such features (Apple can’t keep its own products working reliably, and fingerprint readers aren’t very good at eliminating either false negatives or false positives — how could you expect Mossberg to do a better job, or for someone to want to add a battery pack to an object whose job it is to sit unused for its entire lifetime except in the rare situation that a home intruder appears?). Even if new firearms all had such a system, new firearms are not the majority of firearms on the market: unless the sale of older firearms was banned or older firearms were gathered and destroyed (two very unpopular ideas), the only result would be a sudden drop in firearm sales and a lot more import sales from overseas of older weapons; and, this is if we only account for legal sales.
One must also consider the culture of firearm owners. Firearm owners in the united states skew a bit to the right these days (although any group not firmly in the center and any group with legitimate concerns about the reliability of government-provided policing are more likely to own guns: consider the black power movement as an example of a decidedly leftist cohort with a pro-firearm stance based on a completely legitimate expectation that police would not protect their community), and are going to be suspicious of any mechanism that could get in the way of using a firearm when they decide they need to, particularly if that mechanism is coming out of a government they don’t trust. There’s also, at least within the subset of gun owners who are essentially functional, a culture of gun safety: children are taught how to be safe around firearms long before they are allowed to put their hands on them, and mistakes like poor trigger discipline or poor muzzle control are treated not merely as dangerous but as social faux pas that invite mockery (anyone who puts their finger inside the trigger guard is considered a buffoon from the hated out-group who doesn’t even know how to shoot a gun, and as a person whose presence is dangerous due to stupidity and incompetence). Legislation that interferes or duplicates the function of that acculturation comes off as tone-deaf and disconnected to people with that acculturation, even as it may protect the family of gun owners outside of gun-owning culture (who are at greater risk of accidental discharge because they are unaware of the kind of safety procedures that are drummed into infants in gun-owning culture); this legislation comes from the out-group and is for the out-group and thus seems unnecessary.
> But the reality is most people don’t want just a universal basic income.
Source, please.
Certainly, I agree that SV is biased by extremes of wealth and privilege, and that their ‘solutions’ often don’t work outside of the context of rich white/ asian males living in California and working in tech. However, a stopped clock is right twice a day: regardless of the reasoning used by one group of supporters, UBI is desirable to a number of very different groups for a variety of different reasons — not least by the precise opposite of SV, current minimum wage (or less) earners, many of whom would very much like a modicum of flexibility without the perverse incentive structures provided by current safety net systems.
The suggestion that people would reject UBI as charity is a delusion of people who understand neither UBI or actual want. After all, by definition, with UBI *everyone* would get the same income. This is not a matter of SV folks giving money to the poor; instead, this is a matter of everyone from billionaires to the homeless getting the same living wage.
I’m less concerned about actual scientists falling into this trap, because often they don’t (and those that do often don’t matter). However, science popularizers fairly frequently fail to adequately inspect their biases & disregard the philosophy of science without understanding anything about it; science popularizers aren’t always even scientists (Bill Nye is a retired aeronautical engineer, and while Dawkins and Sagan were at one time research scientists they stopped their research in favor of writing for a general audience about science), yet they have a great deal of control over the popular view of science.
There’s a lot of variation here in terms of how reasonable they are, as well — Sagan almost certainly had heard of Karl Popper, and wrote about what one could reasonably consider to be the philosophy of science with a pro-science bias but with enough emotional distance not to completely disregard important limitations; meanwhile, Nye seems to think that philosophy is all about Plato’s Cave, and Dawkins doesn’t understand that the objective truth of scripture is a tiny and unimportant part of religion. I’m not quite sure where NDT falls here, but I feel like from what I’ve seen, he’s closer to Dawkins and Nye than to Sagan.
Ultimately, if actually active scientists have foolish beliefs about the philosophy of science, it doesn’t much matter: it doesn’t impact their ability to perform lab work and do reasonable analyses, generally. But, when you replace Bertrand Russel with Bill Nye, you’re going to have a bad time: suddenly, large chunks of humanity disregard important ideas and avoid using useful cognitive tools for, essentially, theological and tribal reasons. (And, to a certain extent, even Feynman is guilty of this: the macho arrogant physics-centrism he popularized has really screwed over a couple generations of otherwise promising people by giving them an easy way to dismiss important ideas.)
I’m going to have to point out that this survey will be biased.
I’m going to have to point out that this survey will be biased. At least this is a Stack Overflow survey rather than a Y Combinator survey (which would be far worse), but even here — I use SO all the time but have never been asked to fill out this survey; many if not most people who use SO in the course of their jobs or for help with personal projects don’t even have accounts (because SO works better as a passive repository of historical good answers to common questions than as a means to have a novel question answered in a reasonable length of time — I’ve asked five or six questions and I’ve only ever gotten one response, months after I posted the question). In other words, only people who have an SO account and frequently ask or answer questions will be represented — and then, only those who feel like they have enough free time to fill out a survey.
OK, OK, but a lot of these things are still toys.
OK, OK, but a lot of these things are still toys. The mouse is still a toy, and the WIMP GUI is still a toy; the original mac was a toy for reasons related to hardware costs and hardware budget being eaten up by industrial design rather than by RAM, but the general policy that computers should be strictly limited to performing the tasks the designers thought of beforehand in the way the designers thought they should be done has continued to plague the line since 1982. Facebook was never intended to be more than a toy, and really isn’t: it’s a fun and profitable toy with very wide appeal, but it’s not as though anyone trusts it for genuinely important communications. Being profitable does not make something not toy-like, nor does becoming large in scale.
A quarter page of cliches about how bubble gum media is bad pitched at an imaginary audience who has never heard the term “bubble gum media” is the pinnacle of bubble gum media, and exactly the trend in medium posts that I hate. If you’re going to write some unoriginal content, at least make your post thirty pages long and entertaining like Cuepoint and Backchannel do.
Dear Recruiters,
Since you lot seem to be both very eager to spam me and pretty out of touch with reality, let me clarify a few things.
1. I do not live in New York City. Yes, I know my LinkedIn profile says “greater New York City area”. That’s because the entirety of New England is considered part of the “greater New York City area” by LinkedIn. I am not going to commute to New York City. I am not going to move to New York City. There is no benefit to tripling my rent in order to live in a broom closet and work for a hedge fund. 2. Emailing me out of the blue is probably not going to work out very well for you. If I’ve heard of the company you represent, I probably either dislike them or suspect that you don’t actually represent them. If I haven’t, why would I care? I have a job right now, as you can see from my profile. Sending me more emails is also not going to help. 3. What the hell makes you think I want to work in finance? Nothing about my profile mentions it. No, I don’t want to work in web design or for a startup either. 4. If I receive an email from you, I figure odds are that you aren’t actually a recruiter, or aren’t actually representing whoever you claim to be representing. Why would a big company go trawling LinkedIn and emailing random junior developers who are already employed? If you want my attention, prove to me that you aren’t just a con artist. And do it in the subject line, because I’m not going to open your email. 5. On second thought, maybe just stop? You’re wasting minutes of your time and seconds of mine.
It’s kind of funny that your way of dealing with branching narratives of this type is to play through only once and treat your path as canon, since the norm (at least in VNs, which are generally shorter and contain fewer mechanical challenges) is to get 100% completion (to the extent that, in many games, the ending the creators consider ‘canon’ doesn’t even become unlocked until all the ‘normal’ paths are visited — see Everlasting Summer, for example, whose core plot is only vaguely hinted at until you slog through all the formulaic dating sim BS, or Sharin, which does something similar by unlocking whole new dimensions of meaning in throwaway lines from previous play-throughs in what is essentially an easter-egg route that nevertheless is necessary to play through in order to get completion).
I wonder if triple-A games, because of play time and budget and aspirations towards ‘cinematic experience’, don’t play this way — but games focusing on branching narratives as their primary or sole mechanic are often built with the assumption that players will engage with multiple routes, and reflect this thematically (with plotlines involving time loops, time travel, alternate universes, and so on — everything from Steins;Gate to Higurashi does this). This is a very different way to engage with a game. It’s not precisely leaning on the fourth wall, but instead, taking greater advantage of an already existing mechanic for thematic reasons, telling a story that is much more difficult to tell in non-branching media. (There are attempts to tell this kind of story in film. Run, Lola, Run, for instance, or Tatami Galaxy. They feel more like formal experiments than direct experience because the choices aren’t directed by the audience, and this detracts from their effect; less competent attempts are even less memorable because of this.)
The bigger the bot hype grows, the more apparent it becomes that when it comes to bots, for now at least, being interesting and making money are mutually exclusive. The bots that are being productized now are just special-purpose CLIs with none of the UX effort that goes into making CLIs actually useful.
Bot capitalism will fail
There’s been a lot of hype surrounding chat bots lately. I love bots, and usually I’m all for getting excited about the things I love, but I think the recent hype is very misguided. The reason is that the people contributing to the current hype bubble surrounding bots are not natural persons but corporate persons: they are excited about bots as products. Usually, when the spectre of money enters into a domain previously commercially nonviable, a lot of people get super excited about making money and miss the point spectacularly (pun kind of intended), and this case is no different. However, it’s worth talking about this specific case nevertheless, because with bots, the commercial focus of yuppies and suits is fairly likely to ultimately convince everyone outside the core art-bot community that bots are something worse than useless: that they are supremely uninteresting.
The first thing I’m going to discuss is conversational interfaces. The reason is that the sudden increase in interest in bots is related to the fact that several companies have been shipping speech-based conversational interfaces, and a lot of current commercial bots are intended to be the equivalent of these speech-based interfaces run over existing text-based communications protocols.
I am largely unimpressed with conversational interfaces. They have a long history; many early and influential AI systems would be classified as conversational interfaces, and very simple conversational interfaces were a staple of books on learning to program for the BASIC set since the introduction of the first 8-bit home computers.
Ultimately, conversational interfaces fall into two categories: interfaces that try to keep up the illusion of intelligence and personality by ignoring most input and searching for particular keywords, and interfaces that are effectively special-purpose command lines with snarky or otherwise unprofessional error messages. The former type is epitomized by the search engine “Ask Jeeves”, which achieved its “flexibility” by implicitly inserting an OR operation between each term in the input (thus causing the term with the greatest TFIDF score to rise to the top of the results and become a de-facto keyword), although other examples include Alice and Eliza. The latter type is epitomized by the adventure game “Zork”, which had a somewhat english-like and very limited programming language it could understand and would mock you if you attempted to perform an invalid operation. Siri, Cortana, Echo/Alexa, and Google Now are all combinations of these two forms.
The thing about conversational interfaces of the former type is that they are inflexible and difficult to predict. A given input will produce some output, based on pattern-matching, but in order to prevent seeming as though the machine is not intelligent, the machine will be inclined to always respond with something — ideally, something somewhat randomized. A human being, without access to the source code, will eventually build up a folk-model of what patterns do or do not produce the desired result, but such a model has no guarantee of accuracy or completeness. A user may have some operation they desire that is built-in, but the complete list of available functions (though it is necessarily small, since each one has to be hand-written by a human being and given rules for invocation that don’t conflict with other functions) will never be distributed to prospective users because that would ruin the “magic” of a somewhat human-like interface. The ideal end-game for such an interface is for a user to memorize a handful of commonly used patterns in their most consise form and otherwise use it for its novelty value as a conversational partner — a world of people barking “MOVIE SHOWINGS BROOKLYN DEADPOOL” at their phones instead of typing the same query into google.
The thing about conversational interfaces of the latter type is that, by being english-like, the language understood by the interface will never be sufficiently minimal for a non-casual user, and by being ‘entertaining’, the error messages will never be sufficiently specific for a casual user to be able to trivially determine what he or she is doing wrong. The ideal end-game for such an interface is for a user to memorize a needlessly verbose and limited programming language and be able to type “FEED TROLL TO TROLL” with the expectation that doing so will cause the troll to eat himself and be defeated.
Taken to infinity, the ideal form of the first type is a search engine. Taken to infinity, the ideal form of the second type is a unix shell.
Commercial bots, to the extent that they are expected to reliably perform potentially dangerous operations like making purchases, editing calendar entries, and controlling home automation systems, are going to remain quite close to the second form. This is a shame, because there is absolutely nothing revolutionary about a shitty command line, and it doesn’t do justice to bots in general to imply that all of them are like that.
Consider the non-commercial bot: the art-bot. The art bot is varied in its form. The art bot, because it is a bot, is able to tirelessly perform intellectual tasks. The art bot, because it is art, focuses on tasks relating to recontextualizing ideas, words, images, and perceptions. The art bot is a meaning factory, producing brand new thoughts out of the interference pattern between a PRNG and an audience.
The art bot doesn’t buy anything, or if it does, the fact that you can’t reliably tell it what to buy is part of the point.
The art bot can write music, or poetry, or paint pretty pictures. If you don’t like the art that the art bot produces, too bad. The art bot doesn’t care. The art bot will produce a thousand other pieces while you are deciding whether or not you like that one.
Some art bots tell you to do things. Do you want to take commands from a robot? Maybe. Sometimes the things it tells you to do can’t be done.
Some art bots are funny. Some are even intentionally funny. Any image macro, snow clone, or formula joke is the potential domain of an art bot, who will scour a dictionary to produce millions of variations.
An art bot is not a shitty command line. Or, if it is, it’s intentionally shitty. An art bot might mock you for everything you tell it to do. Or, it might systematically break down your hopes and dreams. Or support them.
Freed from the shackles of needing to please a core user base and be immediately useful without ever screwing up, art bots are allowed to be interesting.
Bots are only interesting when they aren’t expected to be consistently useful or make money.
Bots are only interesting when they aren’t expected to be consistently useful or make money. The constraint of productivity saps away the only interesting thing about interacting with bots: serendipity.
The current boom in interest in commercial bots is just another example of the Spectacle trying to look hip by consuming something that can’t be effectively productized.
I’d like to point out that Project Xanadu (which, btw, is still going on) is, at its core, an attempt to solve this attribution problem permanently (at least for all good-faith actors) by focusing on quotation as a primary factor in composition (seeing editing as a form of selective quotation and rearrangement from previous drafts, for instance). Every so often I see articles like this (or articles on similar topics like music credits) that point out how necessary this tech still is — this clearly-skilled guy would still have his job if, by default, copy and paste between applications implicitly kept source attributions.
How Bots Were Born From Spam
How Bots Were Born From Spam
[1]
The first commercial spam message was sent in 1994—at least that’s the general consensus. Lawrence Canter and Margaret Siegel had a program written that would post a copy of an advertisement for their law firm’s green card lottery paperwork service to every Usenet news group — about 6,000 of them.
Because of the way the messages were posted, Usenet clients couldn’t filter out duplicate copies, and users saw a copy of the same message in every group. At the time, commercial use of internet resources was rare (it had only recently become legal) and access to Usenet was expensive. Users considered these commercial-seeming messages to be crass—not only did they take up their time, but they also cost them money.
In reaction to the “green card” incident, Arnt Gulbrandsen created the concept of a “cancelbot,” which compared the content of messages to a list of known “spam” messages and then, masquerading as the original sender, sent a special kind of message to “cancel” the original message, hiding and deleting it. Two months after the original spam postings, Canter and Siegel did it again — upon which the combined load of spam and cancel messages crashed many Usenet servers. Anti-spam measures, it seems, had themselves become spam.
[1] A Usenet client, showing message groupings. A message cross-posted to multiple groups would only appear once. Image credit: Public domain
While this was the beginning of commercial Usenet spam, this was not the beginning of Usenet spam in general. Prior to April of 1994, a poster known as Sedar Argic would automatically reply to any message containing the word “turkey” with a lengthy rant denying the Armenian genocide. This, of course, made discussions of Thanksgiving celebrations difficult.
The thing about all of these early forms of Usenet spam is that the messages were always identical. Cancelbots worked because the messages they were canceling were either identical or changed very infrequently — they could be compared to a human-maintained list of spam messages (a “corpus” of spam).
But even during this era there were Usenetters using a new technology that would upset this and future countermeasures: Markov chains, which are a popular tool among modern bot-makers. Invented in 1913 by Russian mathematician Andrey Markov, a Markov chain works by combing through text, looking at which words tend to follow each other, and assembling new sentences, paragraphs, and pages using the resulting statistics. Want to try it? Here’s a website that generates filler text from Shakespeare, Jane Austen, the Nixon Tapes, college essays, and even the Bible.
It didn’t take long for spammers to realize that cancelbots could be stumped by adding random junk to the end of messages. At the same time, spammers were moving beyond Usenet into email, just as regular people all over the United States and western Europe were suddenly learning what a modem was and signing up for web access.
By this time, people dedicated to identifying and fighting spam (a problem that had barely existed six months earlier) had already started creating “honeypot” email accounts. These were accounts that no human being would have any reason to send messages to, created for the purpose of accumulating a large corpus of spam, with the goal of researching spammer behavior and developing new spam-fighting techniques. With so many spammers carrying different messages, and random junk being newly added to the end of messages (or beginning, or middle), spam-filtering technology had to get smarter. Programmers began to look at word statistics and Markov models to identify the spammers.
But spammers quickly figured out that they could use the same Markov chain technology against the filters: By creating Markov chains out of clearly non-spammy material (usually derived from Project Gutenberg, a collection of out-of-copyright e-books), spammers could add legitimate-sounding but nonsensical phrases to the end of their messages, making the job of the filters harder. That technique is called “Bayesian poisoning” and is the origin of spam poetry.
Unfortunately for spammers, Bayesian poisoning tends to make messages too unconvincing: Long strings of unrelated words don’t sell. But there’s another way to get around blacklists based on a corpus of text—a technique that became all too common when people started including comment sections on the nascent web. In the spam community, it’s called “spinning.” The rest of us know it as “generative grammar.” Spinning uses variations on phrases in an existing message to create large numbers of semantically identical but distinct messages. Like Markov chains, it’s popular within the bot-making community, and you can try it for yourself here.
[1]
Shortly after email and web browsing became the norm, instant messaging followed. While chat services date back to the early 1970s, large-scale internet-based chat systems like IRC appeared in the late 1980s. As people started growing up with internet access in their households, commercial services like AOL Instant Messenger boomed in popularity.
On IRC in the 1990s, a lot of what had happened on Usenet repeated itself. People wrote Markov chain bots for amusement; other people wrote bots to paste pre-written diatribes in response to particular keywords. Some spambots existed that posted advertisements automatically. But the IRC community, like Usenet, quickly developed technical countermeasures.
Commercial instant messaging services, on the other hand, skewed young and nontechnical. Whereas IRC and Usenet were used and run mostly by programmers, AOL was targeted toward families. When bots appeared on AOL Instant Messenger, AOL had no incentive to stop them; when bots began sending misleading messages to AOL users, the company didn’t have enough experience with spam to be cautious of where this might lead.
Meanwhile, some AOL bots like SmarterChild and GooglyMinotaur were officially sanctioned. Despite the commercial angle, these bots wouldn’t message people without provocation and, thus, arguably were not spambots. Still, their underlying technology was identical, and their attempts to act human represent not merely a precursor to similar systems like Siri, but also a less sinister version of what instant messaging bots at the time did to trick naive teenagers.
[1] A conversation with SmarterChild. Image credit: TheFirstM
If you’ve ever used Twitter, many of the spam techniques I’ve mentioned above will be familiar. You already know that users who post links only are unlikely to be human, particularly if they have a supermodel avatar. At some point, you’ve probably inadvertently mentioned some buzzword (iPad, Bitcoin, etc.) only to be swarmed by tangentially related ads.
Other applications of these spam techniques on Twitter, however, are more interesting. Some, like RedScareBot, are subversive. Others, such as StealthMountain, are educational. Some use normally questionable time-wasting techniques for the greater good by redirecting abuse — such as an ELIZA implementation that engages people using Gamergate-related tags, causing naive trolls to flame a bot rather than a human.
But there are many other modern use cases for these technologies, too. In the academic world, in response to a series of scandals related to fraudulent conferences, a tool called SCIGen was developed that used spinning to generate nonsense papers as a way of ensuring that journals and conferences were doing peer reviews. In 2014, IEEE and Springer, two major academic publishers, adopted the use of a tool for automatically detecting nonsense papers generated by SCIGen after it was revealed that more than a hundred such papers had gotten around peer reviews.
In 2010, Amazon opened its eBook store to self-publishing only to be flooded with e-books made automatically by web scrapers. While content farms for clickbait sites are mostly run by poorly paid humans, The Associated Press is using spinning techniques to generate sports and finance articles, and others are building bots that can write clickbait.
[1]
What this all leads to is unclear. Science fiction author Charlie Stross suggests, in his 2011 novel Rule 34, that the competition between spam and anti-spam technology might drive forward future AI research. In his novel, a superhuman AI evolves from an experimental spam-filtering technology and, as a side effect, has no internal sense of self: It projects its consciousness on some arbitrarily chosen user, because its intent is to determine what that user would consider spam.
Hugh Handcock, another science fiction author, suggests in a recent blog post that the future of chatbots may have more in common with the spambots and mass-trolling of Anonymous and early-1990s IRC than with Siri. Chatbots, by design, might be more desirable to interact with than humans are — they could perpetuate rather than break down filter bubbles, becoming something to interact with without ever leaving one’s comfort zone. They might swarm around dissenting opinions. Handcock presents a world in which a human might know that all his friends are bots trying to sell him something—and simply not care.
Meanwhile, 1990s virtual reality pioneer Jaron Lanier, in his 2010 book You Are Not a Gadget, presents his concerns about current trends in publishing and media where the monetary value of artistic expression is tied to advertising. In his 2013 follow up, Who Owns the Future, he offers a possible end game for an advertising-driven society: one wherein physical spambots provide goods and services for free to people in their target market, while leaving everyone else to starve.
The second episode of the TV series Black Mirror, “Fifteen Million Merits,” dreams up a similar society—an economy based on the twin poles of entertainment and physical labor that extracts money from laborers and funnels it into the entertainment complex by using aggressive advertising that can only be accepted or dismissed using micro-transactions.
Lanier suggests that voluntary micro-transactions might be a way for artists to take back control of their work from the advertising industry and avoid an imminent fall of media from a middle-class to a lower-class position. The Black Mirror episode shows how as long as the entertainment industry is centralized, however, micro-transactions can be a tool for perpetuating class divides and systematically excluding people from participating in the creation and sale of art.
Personally, I suspect that with the new emphasis on conversational interfaces, we’ll begin to see hybrid spambots: Existing conversational interface systems like Siri and Echo, because they serve up data from third parties, might begin to be manipulated by some bot-equivalent of SEO to respond to certain queries with advertisements. In this environment, no automated methods exist for filtering out ads — and since conversational interfaces are often run by retailers, there is no incentive to do so. Rather than trying to outwit spam filters, the creators of these bots would need to be subtle enough to avoid alarming users.
As the landscape of the internet changes, and as countermeasures are put in place, one thing remains constant: So long as spambots can remain profitable, they won’t go away.
This post is part of How We Get To Next’s Talking With Bots month in May 2016, looking at how chatbots will change our lives. If you liked this story, please click on the heart below to recommend it to your friends.
Read more from How We Get To Next on Twitter, Facebook, or Reddit, or sign up for our newsletter.
If you’re going to replace regex half-way with UML and expect people to use a graphical editor, why not just use real UML and generate regex from it? Anyone who finds your visualization more accessible than regex is already more familiar with UML than with regex, after all, and probably already has a preferred UML editor; anyone who uses a console-based editor (which is to say, a whole lot of serious developers) won’t be able to use this (and will be philosophically unwilling to anyhow).
This isn’t a feature of some new revision of UML.
This isn’t a feature of some new revision of UML. This is one of the things UML was being used for in the late 90s. Most Java textbooks, for instance, use UML to describe Java syntax.
Another possibility: do nothing, and allow ad quality to degrade.
Another possibility: do nothing, and allow ad quality to degrade. Much as email spam & phishing messages are intentionally unconvincing to factor out the possibility of intelligent and savvy people caught in the net and causing trouble, one can intentionally ensure one’s ads are removed by ad blockers and then specifically target groups that are unlikely to understand the very concept of ad blockers (seniors, children, the terminally unaware).
I’m not convinced that advertising as an industry can ever be saved from its downward quality trajectory: this trajectory is built in to the concept of advertising, and while the drop can be slowed or even backed up slightly, it cannot be stopped. We are living in the era of Late Advertising (by analogy to Late Capitalism), and like the Accelerationists, our best bet is to make advertising as bad as possible in order to hasten its total irrelevance.
The author of this piece managed to write an entire article about the concept of slack without once mentioning the Church of the Subgenius, and the long history of interplay between the philosophies of half-serious irreligions and hacker culture. Hell, he didn’t even mention Slackware.
In analyzing what slack means and why people care, it’s important to understand the other aspects of the slack memeplex: specifically, it’s important to understand the Church of the Subgenius, what aspects of the world it specifically satirizes, and its interplay with Discordianism. One reason is that, if anything, the CoS’s satire is becoming more and more relevant — and was always a great deal more on-point and meaningful than more mainstream variations like pastafarianism.
Where Discordianism sets its sights on control freaks by mixing eastern philosophy with greek mythology and creating a religion venerating chaos, the Church of the Subgenius sets its sights on capitalism and The Spectacle by combining evangelical christianity with UFO cults and creating a systematic veneration of laziness (which they call slack).
The messiah of the Church of the Subgenius is L. ‘Bob’ Dobbs, an idiot-savant con-man who can “sell anybody anything”, and he was tasked with leading the descendents of yetis to a pleasure planet. Membership costs thirty dollars, and eternal salvation is guaranteed “or triple your money back”. This is the matrix of ideas from which the concept of ‘slack’ emerged, and this is why it’s important: slack is an essentially subversive concept that stands simultaneously with and opposed to all the tacky late-night-tv Ed-Woods glory of ironic ad-worshipping hipsterism that substitutes bigfoot for Chuck Norris as an icon of ancestral masculine virility.
One small complaint: you mention Moore’s Law as one of the reasons ML is having a rennaisance; while it’s true that there’s an exponential growth trend involved, the trend in question is not actually Moore’s Law (which ended sometime between 2005 and 2007 depending on who you ask). Moore’s Law has to do with the number of transistors that can be fit on a single die — a number whose hard limit we’re bumping up against (because of the size of atoms); mechanisms to get around this limit don’t involve transistors as such and so the moniker doesn’t apply to them.
In popular science writing, Moore’s law has become a shorthand for all forms of exponential growth that affect performance-per-dollar or performance-per-inch of computer hardware. CS people sometimes use it this way, but (when it matters) make distinctions — and there are a number of other “laws” that are essentially similar in nature but apply to different metrics: Kryder’s law for storage density, Koomey’s Law for performance per watt. And then, there’s Englebart’s Law (which all these other accelerating performance laws are arguably a special case of): that the performance of human beings increases exponentially in all sorts of contexts and domains.
Regarding the ‘preferred music service’ example, it’s a bit funny that Amazon is not extending the (Android/Palm) concept of ‘intents’ with simple fallback logic (“alexa, play me song X” can be handled by this list of three skill providers in preference order, so search the first and if it can’t play this song then fall back to searching the second).
There’s a difference between the Lake Woebegone effect and the Dunning Kreuger effect.
There’s a difference between the Lake Woebegone effect and the Dunning Kreuger effect. While the Lake Woebegone effect means that we tend to rank ourselves above average more often than is possible, the Dunning Kreuger effect has to do with scale: the degree to which we overestimate our abilities is inversely proportional to our actual skill level.
Novelty, Perversity, and Randomness
I recently read a blog post (never mind which one, since this is a pretty common position) railing against “rules of thumb” for writers that make generalizations about readers. This post made the argument that a readership, because it is heterogeneous, cannot be generalized about. Such an argument is untrue (one can take the mean, median, and mode of extremely diverse data sets and still get somewhat valuable information from that), but more crucially, it’s untrue in a boring way. It’s true that rules of thumb that make generalizations about readership are problematic, but this is the case for a much more interesting reason that I’ve never seen articulated.
First, I’d like to get something out of the way. These rules of thumb and generalizations about readers exist for a reason: specifically, they are useful to two different groups of authors. One group is expressly commercial: people whose primary or sole goal is to maximize their sales will want to optimize for a large readership, and will therefore want to model their readership and appeal to this model. The second group is the aspiring amateur: someone with no experience and no model at all of a readership or of the way in which one goes about writing benefits greatly from the confidence provided by any direction at all, even if the direction in question is anecdotal, misleading, limiting, or false; arbitrary advice benefits these people.
The real reason why rules of thumb are a problem is that readers read in order to satisfy a hunger for perversity. By this I mean that readers are looking for a special kind of novelty: within a constrained system (such as a fictional world, a genre, a style of argument or of writing, or a set of themes), a path through this domain is created that is plausible but surprising. The author is an engineer of subversion, creating expectations and then delighting the reader by perverting them. Rules of thumb, whether or not they correspond to tendencies desirable to readers, are best understood (as TvTropes notes) as a set of rules or expectations in the mind of the reader — in other words, as the starting point of subversion.
However much a reader may love some trope, the reader will love an interesting perversion of that trope more.
Writers, like readers, are human beings, unfortunately. Human beings are a bit too good at categorizing and finding connections: this is a liability when it comes to producing novel ideas or determining how novel an idea is once produced. Randomness-driven “writing machines” like cut-ups, bibliomancy, Cards Against Humanity, and similar techniques can introduce novelty unlikely to be produced by a human mind unaided; constraints like those in Oulipolian writing games or those used by Dr Seuss can force novelty from the reader’s perspective by warping the environment the writer is navigating: the most straightforward choice of words for a writer unable to use the letter ‘e’ will seem very strange to a reader not accustomed to playing the same game.
It is important to note that the novelty of a work is a property of the mind of the reader, not a property of the work itself. In other words, an experienced reader has more discerning taste as an inevitable consequence of that experience: anything is novel to a tabula rasa and nothing is novel to an omniscient being. As a result, the game of writing can never be won. Every set of expectations is the end result of someone subverting previous expectations. Furthermore, we can divide up groups of people by what expectations they have (and thus, what they find novel), essentially based on what they’ve already been exposed to.
A work like The Last Ringbearer is in dialogue with Lord of the Rings, but more importantly, it is a perversion of expectations created by Lord of the Rings; its value is lost on anyone who isn’t familiar with the original work. To a lesser extent, a work like Neuromancer is explicitly a perversion of the expectations set up by golden age SF: where Asimov would have given the protagonist role to someone in a role of power or authority, Gibson gave the protagonist role to someone incapable of changing even his own destiny, explicitly on the verge of being killed by minor criminals with the implication that he wouldn’t even be remembered, whose role in the story is that of a convenient tool for powers beyond reckoning; where Heinlein would have given us a wise-cracking Bill Murray protagonist and Asimov would have given us a Spock, Gibson gives us Case as Shinji Ikari; where any golden age author would have described only the important parts of the environment and shown us a gleaming, streamlined future, Gibson fixates on the tiny dents in the table of a diner and the smell of decaying newspaper piled in the entryway of a dilapidated storefront; where golden age governments are noble or evil, Gibson shows a world where government ranges from minor corruption to complete irrelevance. Similarly, Dune goes out of its way to pervert all our expectations: we have an aristocracy whose power is based on access to resources instead of an aristocracy based on the idea of genius loci and the divine right of kings as in most extruded fantasy product; we have faster than light space travel and a multi-planet civilization in a world without computers wherein most fighting is done using knives; our far-future society’s great new social and political movement is driven by an offshoot of Islam invented by an ecologist and practiced by desert nomads who ritually consume psychedelic drugs and drink their own urine. While these works can be consumed by people without the understanding of the context they are in dialogue with, the enjoyment gained by such readers is limited to that of intratextual perversion (i.e., plot twists), free-floating novelty (“what a cool idea!”), and expectations based on other domains like the real world; rather than being a lazy new trend by people trying to push cinematic universes, intertextual perversion is the primary defining factor in whether or not a creative work is considered seminal or culturally important within a genre. To use marxist dialectic terminology, a seminal work is one that is the antithesis of all that has come before it, and forces the genre thereafter to synthesize it. Every work in a genre is a reaction to each seminal work in that genre: either supporting it or reacting against it.
We can consider a genre to be defined by the parameters of its conventions: in other words, what attributes do its seminal works have in common? The domain of any interesting work in that genre, then, is in probing the unexamined assumptions of readers who have internalized these conventions: what things are these readers currently incapable of considering, and how can we surprise them by causing them to consider these ideas unexpectedly? Authors, being human, are probably also going to have a hard time considering unknown unknowns; they are advised to use machinery to aid them, since machinery has no problem being creative and original.
It is not that following the “rules” of a genre is foolishly stifling the wonderful underlying free creative spirit of the writer: any author who thinks that their underlying free creative spirit is something special never to be tamed is encouraged to look at their pile of rejection letters and reconsider a career outside the arts. Instead, following the “rules” misses the point entirely: the “rules” exist in order to make the game more interesting. Breaking the spirit of the law without breaking the letter, and vice versa, is the very manner in which writing is creative.
As much as I hate to rain on your parade, I’m afraid I can’t agree at all with your premises.
As much as I hate to rain on your parade, I’m afraid I can’t agree at all with your premises. This article itself argues against you.
You’ve written a short article that shallowly rehashes ideas that were already long past their expiration date in the 70s, when they were being summarized in Future Shock. You cite studies that don’t support your findings, and complain about trends that are mostly imaginary. Ultimately, you’ve projected your own subjective feeling of burn-out onto the world at large, channeling it into a pastiche of what is essentially now a popular subgenre of theoretically non-fiction op-eds: the anti-internet thinkpiece.
Have you constructed a media diet for yourself that negatively impacts your life? Clearly. Is this common? Sure. Is it new? Not remotely. You’ve taken perfectly good tools and used them to cut your limbs off, and now you’re complaining that the tools are dangerous.
The only part of this article that is non-trivially correct is the part about multitasking & unfinished task cues. But, the idea that this mechanism should be used as an excuse to do some kind of internet detox is absurd. Instead, be mindful of your own cognitive biases, and take advantage of this effect as incentive to perform the tasks you intend to perform.
Easy access to information makes shallow understanding possible but does not encourage it; after all, it also makes much deeper understanding possible. If this access makes you tend toward shallowness, that’s an indication of your own intellectual laziness, not a reflection of the innate tendencies of the tools at your disposal. Practice some discipline, and these tools will help you gain more depth. (Without much discipline and with only a little foresight, it would have demonstrated to you that there is nothing original in this essay; in Medium’s daily recommendation email alone, I see three of this type a day, though they are often shorter and have fewer reviews or recommendations. You could have saved time by recommending some of those, rather than writing this yourself.)
There’s another path here that you haven’t mentioned: take advantage of content generation technology and combine it with your own efforts. (This isn’t a new idea: a few months ago, at least two or three ‘creative autosuggestion’ projects were floating around; Burroughs edited the output of cutups, as did Bowie; after Deep Blue became the world’s best chess player, top-tier players started playing ‘centaur’ or ‘cyborg’ chess wherein they collaborated with chess programs.)
That said, being a part of the generative writing community, I think you slightly overstate the state of the art. Generative poetry is pretty good, in part because poetry is pretty flexible and unusual stylistic choices are typically seen as meaningful: generative writing does well in forms that are either extremely experimental or extremely formulaic. Machine-generated short articles performing numbers-driven reporting of factual events (such as sport or financial stories) are more or less indistinguishable from human-written stories of the same type, but also represent a fairly uninteresting application of this technology (in part because, while the AP and other news wires started using this technology only recently, it’s been possible for even a mediocre programmer to implement this kind of thing since the 50s). Long-form narrative-driven works don’t tend to be readable, although in the past few years there’s been a lot of progress (for instance, there was an entry in National Novel Generation Month in 2015 called MARYSUE that generated pretty convincing pastiches of bad Star Trek fanfiction, most of which is not much worse than human-written bad Star Trek fanfiction).
There are certain things that generative writing systems are very good at. These systems are capable of being intensely creative (in the sense that they can generate juxtapositions of ideas that could never occur to a human being), but lack any good approximation of human taste: as a result, writing systems can be used as a creativity prosthesis, where human judgement isolates good ideas from bad ideas while the machine performs the heavy work of producing ideas. Likewise, these systems are good at being exhaustive: it’s trivial to take a couple corpora and a format and produce every possible variation on a theme, if someone is willing to wade through the output looking for the interesting material. Human editing makes machine-generated writing much more feasible for even long-form work.
Have you determined how much non-book content you are reading during the day?
Have you determined how much non-book content you are reading during the day? During a typical day, I end up reading several books worth of articles, not to mention emails. I’m loath to value books over other forms of written content impulsively: after all, books are often crap and articles are often quite good.
Is it valuable to be self-aware about the way in which notifications and consumption of short-form content performs operant conditioning on you? Sure; this is why I have notifications off on my devices, avoid broadcast television entirely, and try to avoid reading any article that takes less than five minutes (with articles taking 20 minutes or more to read taking precedence).
But, a “book” has a lot of cultural and structural baggage: books are associated with traditional publishing pipelines (meaning that a book is written about a year and a half before anybody who isn’t a professional author, editor, or reviewer reads it), with a particular structure (several multi-page sections called chapters, organized either by topic or chronology, numbered) and attributes (even ebooks are usually paired with paper equivalents, and so features like internal hyperlinks, interactive sections, and direct feedback should be avoided; updates, to the extent that they exist at all, are limited to later editions which must be bought separately; a book is rarely less than 90 pages long and rarely longer than 500, because otherwise binding it would be prohibitively expensive for an approximately seven USD price point). How many of these attributes of a book are, in of themselves, valuable? How many of them, for a particular subject, are desirable constraints rather than undesirable ones? When we consider that books might also need to be audio books, we add more constraints: no diagrams, no extensive use of homophonic puns or homoglyphic puns, write nothing that is inherently unpronounceable.
I love books. But, when we uncritically value the book format we sacrifice ourselves to a kind of shallow reactive traditionalism that ultimately runs counter to precisely the kind of attributes that book-loving people ascribe value to: deep consideration and intellectual bravery. A book is a tool, and to consider it preferable for jobs for which it is unsuitable for reasons of habit and social signalling is to devalue it.
Bots are wonderful.
Bots are wonderful. But, for a variety of reasons that should be so obvious as to not merit mentioning, if you’re trying to make money by creating a bot you’re making a huge mistake. (Because people haven’t managed to get the memo, I’ve gone over these reasons three or four times on Medium alone. But, spending a few minutes thinking about the situation should suffice.)
Of course, if somebody expects to make money off of phone apps they’re already laboring under a huge set of delusions. Rather than disabuse them of specific ones, I would recommend they instead switch to a more lucrative career in problem gambling and/or attempting to be struck by lighting for the insurance money.
A lot of the ideas about OO that are mentioned in Mr Scalfani’s essay were pushed and popularized by academic materials for teaching Java and C++ as first languages; to the extent that they are true in those languages, they are mostly true of toy examples. These materials hammer home these benefits as ideals while also pushing OO as a panacea, & are aimed at naive beginners, who lack the experience to argue coherently against them.
Using OO when and where it is appropriate, and using languages like Smalltalk that do OO ‘right’, is fine. It’s not fine to tell 15 year old newbies that OO is the best tool for every job, and that all Java code is OO — and that’s what practically every introductory Java textbook will do, because it’s literally part of the standardized APCS curriculum & part of the accreditation process for university computer science programs.
It’s not that OO should be attacked because it’s inherently terrible. Instead, there’s a fantasy idea of OO promoted by a whole industry, and that fantasy needs to be attacked because it is systematically producing incompetent programmers who write bad code.
General purpose, in my view, is an overstatement.
General purpose, in my view, is an overstatement. (That said, maybe I work with an atypical set of problems.)
OO is appropriate when the least complicated way of modelling a problem is in terms of agents with internal states communicating. In other words, something like a physics engine is a natural fit for OO.
I mostly work with processing large amounts of text data between formats, occasionally doing analysis. (This is, as far as I can tell, a pretty typical programming job.) This kind of work lends itself well to multiple parallel stages with relatively little state, much of which is short-lived. In other words, it fits well with the UNIX pipeline model, and both OO and FP would be a poor fit. (I’ve had to work with other people’s attempts to shoehorn this kind of process into an OO framework; when I can get away with it, I replace a few thousand lines of Java with two or three lines of shell.) In other words, this is the ‘general case’ for my line of work.
I occasionally come across a circumstance where OO makes sense. I wrote an OS in D, and because D’s object system is similar to C’s structs and unlike C++’s objects, I was able to make excellent use of inheritance & polymorphism for wrapping abstraction around memory-mapped devices like VGA memory; I also made heavy use of object orientation when modeling some novel data structures for Project Xanadu. However, in none of these cases was it convenient or sensible to go full-OO — all of these cases were ‘mixed paradigm’ with a lot of procedural and functional code, and even when I wrote OO, I violated encapsulation whenever I needed to & barely took advantage of inheritance.
I understand why pedagogy around OO is so popular. After all, human beings deal with objects with internal state in real life all the time, so these concepts are familiar to non-programmers. Likewise, inheritance hierarchies mimic the form that every naive cataloguer produces. But, like John Wilkins in his Essay Towards a Real Character and the imaginary Chinese philosopher in Borges’ Celestial Emporium, we quickly find that reality doesn’t neatly conform to either a hierarchy of relation or a conception of objects with attributes.
In other words, OO is a convenient stepping stone to other paradigms, and is useful in of itself when a mapping of the problem space to a set of objects is either immediately obvious or (in the case of physics engines) provided in the literature, but its most useful attribute is the way in which it demonstrates its own limits. Despite this, because most formally trained software engineers have only ever had serious experience with so-called OO languages & with material that promotes OO to the exclusion of other paradigms, there is the (false, but widespread) belief that the problems introduced by trying to shoehorn the wrong problems into OO are problems of programming in general and are irreducible. The idea that there is such a thing as a general purpose programming paradigm that is ‘good enough’ for anything, while convenient for casual developers who would like to avoid learning more than the basics of a single language, supports this essentially artificial set of problems.
The current boom in interest in functional programming is hardly unexpected: you teach people that the one true paradigm is a bastard mix of OO and procedural programming for twenty years and that it’s the end of history, and as soon as they are exposed to an alternative they’ll jump all over it: most problems introduced by OO are artifacts of OO and dissolve as soon as you break out of it, but FP has similar artifacts.
Poor CS pedagogy is a bit of a hobby horse for me. OO is as it is, and I don’t have anything against it as such, but the idea that OO is generally applicable (or, more generally applicable than some other given paradigm) is getting in the way of professors and authors realizing that all serious developers need to have a full toolbox. The current miserable state of the development world is a result of every developer having a pair of vice grips & nothing else — and while you can solve pretty much any problem after a fashion with a pair of vice grips and a few hours of fiddling, you end up with a lot of mangled screws and bloodied fingernails.
I always make this argument, and it usually falls on deaf ears.
I always make this argument, and it usually falls on deaf ears. But, this is a cultural and industry-norm-driven pattern — and Netflix is in prime position to upset industry norms (as it’s already done by releasing whole seasons at once).
In the anime industry, outside of a handful of long-running high-profit and usually every episodic shows aimed at younger audiences, there’s a very different attitude toward extending series. In the US, we tend to make new seasons of a show until it gets so bad that it gets cancelled; in Japan, because cancellation is such a big deal (it’s so rare that I’ve only ever seen one cancelled show), you have a very different pattern: a single season is planned out and made, and if it’s sufficiently popular and the source material allows for it, a decision is made about whether or not to create a sequel, usually several years later. The average show is between 12 and 26 episodes long; most shows have exactly one season, and when new seasons appear, whether or not they are a continuation of the same story depends heavily upon whether or not the show is an adaptation of a continuing story in another medium. As a result, a season is like a miniseries: good shows are finely crafted single-season stories that are never revisited, and a show with more than three seasons is almost always considered mediocre at best.
If Netflix adopted this style, they could push a change in the industry. After all, television has already changed: serious character-driven stories with complex ongoing narratives have become valued only in the past fifteen years or so, displacing the normal highly-episodic series well-adapted to syndication by channels that only license a small portion. If the expected form of television changed in this way, we’d see fewer shows that follow the pattern of Buffy — you know, an excellent first season followed by increasingly mediocre and convoluted follow-up seasons designed to pander to an obsessive core fanbase.
I don’t know where you got any of the ideas in this article.
Copyright is not a protection against communism. The concept of copyright predates communism by a few centuries, and was initially formed as a means of mediating conflicts between publishing guilds and thus making censorship of printed materials easier. When the intellectual property framework of the united states was being designed, it became reframed in the same terms as patents: as a means of guaranteeing a temporary monopoly in order to encourage people to donate ideas to the public domain. This would be the 1790 formulation you were talking about — fifty years prior to the first ideas we could reasonably consider part of ‘communism’. In fact, we can consider copyright (along with other time limited forms of IP like patents, as opposed to trade secrets) to be a tool in support of ‘communist’ ideas in the sense that it is specifically intended to feed the pool of publicly owned ideas.
Copyright has definitely changed. For instance, in countries that are signatories to the Berne Convention (which the United States has been since 1957), all copyrightable materials are automatically implicitly under copyright — copyright registration is purely for the sake of expediting a suit by providing evidence from a trusted third party. During the middle of the twentieth century, implicit copyright was introduced to the United States in several different ways, and there was a short period wherein unregistered works were only implicitly copyrighted if they had a copyright statement attached (which is why Night of the Living Dead is in the public domain); this is no longer the case. In 1998, the Digital Millenium Copyright Act introduced the notion of safe harbour provisions in order to grant partial protection to websites hosting arbitrary third party content; this is a major break from all prior copyright legislation. When anti-circumvention clauses were added to the DMCA, that’s another major change. The idea of the FBI actively investigating breach of copyright is another recent change not in line with the historical spirit of IP law (which, since it is civil law, should theoretically only be investigated in the case of a suit).
I understand that you’re trying to push a product here. But, before trying to push a product relating to copyright law, try doing at least a modicum of research. I agree that copyright is currently benefitting mostly large corporations; however, this is not because of the cost of registration (because registration is not necessary) but because of a general lack of understanding on the part of regular people about the nature of IP law; this post perpetuates the worst of these misunderstandings.
It’s important to note that Wikileaks itself doesn’t choose what information to grab & host; in terms of editorial control, it sits somewhere between pastebin & the New York Times: it mostly receives unsolicited leaks, and it will spend extra effort cleaning up & making accessible things that have wide appeal, but will more or less host anything. Looking at the raw list of leaks makes this clear: there are a lot of items along the lines of the membership list of Condoleeza Rice’s sorority (which is fairly uninteresting).
Does the wikileaks administration reject some submissions? Probably — after all, there are plenty of people who, given a place that will host anything, will use it to pirate hollywood movies & child porn. But, if somebody offers wikileaks an email dump, whether or not they will host it is not even a question: of course they will host it, because that’s what they see as their mission.
tl;dr verison: original article is a dumb rehash of stuff that was proven wrong thirty years ago, and the author’s shallowness has nothing to do with the internet & everything to do with being lazy.
Spoiler alert: I don’t hate you for being immune from spoilers; in fact, to the extent that this has actually been empirically tested, the general consensus appears to be that spoilers improve the enjoyment of media.
What is lost to spoilers? Only the shock value of sudden plot twists — in other words, the cheapest and most evanescent emotional effect of the laziest narrative constructions. Media that can be “spoiled” by spoilers is media that isn’t worth repeat viewing, continued thought, or discussion. For everything else, spoilers improve enjoyment.
Caffeine takes about 20 minutes to metabolize to the point where it can suppress adenosine reuptake. This is how people do “coffee naps”.
Twenty minutes is plenty of time to write down 10 original ideas. Why not chug your coffee (or use caffeine pills), then spend the following 20 minutes writing your ideas? Alternately, sip your coffee while writing your ideas — and take advantage of the much slower ramp-up in the effects.
I don’t think Wikileaks is choosing their timing.
I don’t think Wikileaks is choosing their timing. They are not, and have never claimed to be, a journalistic outfit: if they were, they would assert control over their material and choose particular material to release (which they do not).
They are a repository for leaked material. The value of the leaked material is occasionally useful as a PR strategy to get more support, but they have no particular problem hosting uninteresting leaks — which is most of what they supply.
Don’t compare them to the New York Times; instead, compare them to MegaUpload.
It’s a mistake to consider Wikileaks as a media organization, because they are generally anti-curation. There’s little rhyme or reason to the leaks they host, because they put up anything that’s submitted that they have reason to believe is legitimate. This makes them closer to a repository for leaked information — which is, of course, what they’ve been claiming to be the whole time. To the extent that they perform any kind of extra work — from redaction to supporting internal search (as they did with the cables) — it’s because they had something of such great importance that they felt the need to improve accessibility even if it damaged the correctness in minor ways. This should be seen as a divergence from their goals, rather than part of their core goal set.
I’ve always been suspicious of how much companies that brand themselves as “tech companies” are drinking their own kool-aid, and how much is a calculated PR/branding move chosen specifically to provide an “in” for avoiding regulations. Uber can hardly be considered anything other than a taxi service — but by claiming to be a tech company they have an excuse to use this Randian mythic form to reframe any circumvention of labor regulations as a battle of smart renegade heroes of industry against an oppressive government, rather than the narrative of a large corporation systematically abusing its employees (which would be more natural if Uber identified as a tech company).
Given the trend toward creating service companies employing mostly contractors and claiming them as “tech companies” because of their use of employee management software, the suggestion that this is some kind of organic misunderstanding rather than a canny manipulation seems increasingly absurd — although it’s not unusual for intellectually incestuous communities to become prone to episodes of truly monumental mass delusion, so we shouldn’t expect the silicon valley VC industry to fare any better than the judicial system of eighteenth century Salem, nor for those forced by circumstance to implement those delusions to fare any better than the poor of any community in the middle of a witch- or werewolf-hunt.
Saving the magic system of Familiar of Zero from itself
Saving the magic system of Familiar of Zero from itself
Familiar of Zero isn’t just a 2006 anime — it is the most 2006 anime I’ve ever seen. It’s the kind of uninspired extruded-fantasy-harem-crap we always complain about, in its purest and most unadulterated form. Its protagonist is without any particular traits, other than an over-the-top lecherousness that while it sticks out now was more or less par for the course a decade ago for harem shows. The romantic lead is the kind of poor-little-rich-girl tsundere we wouldn’t see properly fleshed out and given characterization until Toradora. There’s a large supporting cast of generic archetypes. The most interesting and original character may be Colbert, a plot device of a balding, kindly but boring professor who is also prone to chasing after myths — kind of like if your high school social studies teacher spent his free time trying to find Atlantis — but he is mostly notable for how little he resembles Stephen Colbert. It’s a spectacularly forgettable show, notable mostly for how stupid every single character appears to be. But what it does have is an interesting and well fleshed-out magic system.
That is, until the plot starts.
You see, the way magic works in this world is that each spell works with one of the four classical greek elements. Even simple spells that are theoretically identical are completely different between elements: the spell to illuminate the tip of a wand would have nothing in common in the mechanics of casting between a method using air and a method using earth, even though the result would be identical. A ward or seal created by fire magic can only be undone by fire magic. This creates a pretty stable set of parameters. Imagine this from the perspective of game design for some MMORPG: even if every player character is a mage, you would expect a party to by necessity contain a member specializing in each type of magic. To the extent that attack spells are used, they are undone by attack spells from an opposing element, usually — fire can be defeated by water or redirected by wind, etc.
And then, we’re introduced to the fifth element, void, which is incredibly powerful and can beat anything. Because that’s the only way our protagonists can be put into interesting situations: by deus ex machina.
Ignoring void magic, we have the possibility of multi-classing. The idea is that extremely talented people can reach the limit of advancement in one form of magic and then start from the bottom in another form. If you can master three of the four forms of magic, you become a triangle mage, and can cast spells that weave together the three elements in such a way that it would be very difficult for three high level mages of the individual elements to undo; only a triangle mage specializing in the same three elements, or a square mage who has mastered all four, can undo it.
Void magic is in-born, can’t be properly studied, operates mostly by intuition, and is incompatible with all the other forms. So it breaks everything.
The world in which this show takes place is a kind of extension of feudal europe, in which all those capable of magic are considered a part of the aristocracy, and where rival kingdoms are in politically precarious situations, with webs of secret alliances, spies infiltrating each other’s kingdoms, and aristocrats trying to foment populist uprisings among the peasantry. In other words, a fairly realistic depiction of a society where a whole class of people have magic powers passed on along the bloodline. We have plenty of call-outs to actual historical events, including a revolution in the britain-expy called Albion wherein a guy named Cromwell kills a prince named Wales and turns the country into a dictatorship. Adding intrigue are various magical devices that can temporarily animate corpses or cause people to act against their will while in line of sight.
And then, the void magic is powerful enough to kill all the conspirators. Accidentally. With the help of a world war two era fighter plane. This show keeps generating promise and then screwing it up.
Without the main characters, we have the setup for a really interesting game or story here. We have a complex but internally consistent set of rules for magical warfare — one that rewards teamwork at the lower levels and rewards achievement at the endgame, encouraging people to max out their level up to four times. We have interesting sets of alliances and conflicts: nations at war with each other and themselves, spy networks, the double-cross system. We have devices that have interesting abilities and limitations: the ability to temporarily control a corpse, the ability to control a living being at short range and under line of sight. We have an interesting combat system, with familiars tanking for mages and an interaction between elements. I could imagine something similar to a cross between WoW and Planetside — an ongoing war of small-scale conquest along the edges of domains, powered by a steady supply of fresh blood from strongholds where mages in training follow quest lines to develop their skills.
All we need is somebody to take advantage of this, instead of breaking it in the name of making a generic hero’s journey story.
The trademark issue is pretty cut and dried: a trademark holder must show due diligence in protecting its trademarks or else it loses control of the trademark permanently; as a result, a trademark holder has every incentive to send nasty messages to anybody they might have reason to believe might be seen by future courts as infringing upon their trademark, lest they be suspected of not protecting it well enough. (Basically, the law incentivizes trademark holders to be assholes & go overboard, which is why Disney sues day care centers for having Disney-owned characters on murals.)
As for linking: you may not remember, but the court case regarding the legal status of links was a *big* deal, and old media companies are likely to still be pretty sore about it. It was only twenty years ago that the question of whether or not deep linking was a form of infringement; similar mechanisms (like the use of frames to wrap ads around other people’s sites) were ruled against. It doesn’t really matter if the NYT is planning to make a bot: they didn’t want to give up on suing Google & anybody else who does deep links, and now that they have to, they have every intention to prevent that verdict from being generalized to slightly different circumstances.
Furthermore, you can make the argument that use of the API might, in certain cases, lower the number of ad views they get or screw up their progressive paywall system. While I’m not convinced that either system is making them bank in the way that they might hope (and both are easily & regularly bypassed), they have a financial incentive to label and shun anything that might conceivably be used to circumvent these mechanisms — particularly if the product is not commercial.
Zizek (along with a handful of other figures in media criticism that are full of ideas but short on consistency, like McLuhan) are, I find, best viewed from the same lens as divinatory bibliomancy or generative or surrealist writing-games. In other words, the juxtapositions they create can help externalize and elucidate insights that come within us, but we should not attribute those insights to the ostensible authors, because for every ‘true’ insight we get from those texts, the direct opposite can be supported equally well by the same material. Rather than a string of words containing clear meaning, I see these authors as creating a string of vague associations with strong resonances — and we as readers can cherry-pick particular ideas and complete them ourselves, just as Burroughs did with his cut-ups, or a tarot card reader does with his spread.
OK. There’s a pretty big gap between scheduling the announcement of a leak to maximize press (and thus maximize donations to your non-profit organization) and trying to manipulate the election of a foreign country in favor of your ideological enemy, though.
Why would a collection of mostly european anarchists want a Trump presidency? I don’t see any traces of accelerationist rhetoric going on; if anything, Wikileaks appears to take the attitude that transparency in general will improve governance, rather than the idea that discrediting the idea of government in general is desirable. Julian Assange is not Nick Land; his outlook is a lot more utopian.
An alternate wizard/cleric distinction
I’ve been reading Playing the World, Jon Peterson’s scholarly history of the origins of Dungeons & Dragons. Peterson elucidates the circumstances around the wizard/cleric distinction well, explaining that such a distinction had very little basis in fantasy literature (and none in Chainmail, Gygax’s predecessor to the D&D ruleset) but instead appears to come from Gygax’s religious background & various complaints about the absence of religion in medieval & fantasy wargaming. As a result, while magic users are fully irreligious, we have a separate class that is essentially a specialist magic user whose domain of abilities is based on a credulous reading of christian miracle-work — along with a dynamic involving power level being limited by a fall from grace. This is wholly at odds with the essentially pagan attitude elsewhere in the extruded fantasy product tradition that D&D in many ways codified: a universe where a thief can be lawful good is not in any way a christian universe, even in terms of the secularized pseudo-christian morality that binds clerics. Instead, since we’re talking magic systems, we might look at historical occult traditions and classifications: specifically, the thelemic conception of left-hand versus right-hand path.
Talking to magick practitioners about the left hand and right hand paths is like talking politics at a family gathering: it’s a recipe for broken hearts. For the sake of this essay, I’m going to use a strict and simplified division: left-hand magic is performed using the abilities of the magic practitioner directly, and comes down to a set of rules and mechanics; right-hand magic, on the other hand, relies upon no special abilities in the practitioner, but instead a knowledge of means by which the practitioner can convince supernatural entities to perform tasks. This is certainly not the only way to interpret left-hand vs right-hand traditions, but the major alternative (which applies a moral gloss, considering left-hand to be synonymous with black magic) is less useful for the purpose of role playing systems & ultimately redundant; I would furthermore claim that conflating the two (as many traditions do) is at best an indication of anti-secular bias.
Untangling left-hand and right-hand mechanisms in historical occult practices is difficult, in part because non-secular traditions perform theological gymnastics to reclassify seemingly left-hand practices as right-hand practices, thereby avoiding anti-left-hand bias. This is particularly common in monothesistic contexts: the use of kaballistic formulae whereby the magician manipulates letters or numbers representing the state of the world in order to manipulate the world appears fairly left-hand, but by identifying the material world with an aspect of the divine and invoking predestination, we can treat it as a form of prayer — that most representative form of right-hand magic. For the sake of this essay I would like to avoid these kinds of tricks entirely, and consider only those magical techniques that directly interact with a supernatural entity as right-hand.
And so, what does this give us? It gives us a richer domain for the cleric, reallocating some of what would otherwise be the domain of wizards. In a clearly polytheistic context, the low-level cleric also takes on some aspects of bards, being capable of granting boons to particular attributes by prayer or sacrifice to gods associated with those attributes. Contracts with gods, as in voodoo, are the domain of clerics; control of elementals, summoning of & contract with demons, and invocation and evocation of any non-humanoid ‘monster’ likewise. Rather than needing to keep up a kind of pseudo-christian morality, a cleric would need to merely act in a way in line with the gods he or she actually interacts with. (Many polytheistic pantheons contain a god of communication who acts as a gatekeeper: Legba in voodoo, Ganesha in hinduism; any cleric would need to act in a way that pleases the gatekeeper deity, or risk losing access to the whole pantheon, but what this means is very different between pantheons: what is fine with Mercury is unlikely to be acceptable to Ganesha.)
This would require moving many of the healer & support roles to another class; after all, the cleric redefined in this way would be significantly more powerful as an offensive class at higher levels and would not have special healing abilities at lower levels.
I have to agree with all of your points here.
I have to agree with all of your points here. But, at the same time, on the scale set by the other TOS reboot films, this one is the closest to Star Trek — in the sense that it has an interesting and complex plot, ensemble cast drama, & elements of the utopian subtext that characterized TOS. The other two have leaned much closer to generic-sci-fi-action — though Star Trek movies have always been a lot more action-oriented and a lot less talky than the shows.
Was the jokeyness a bit out of character for a Trek film? Sure, particularly for the reboot franchise; on the other hand, the other films in this group have been particularly over-serious grimdark fare (even as they used absurd plotlines that made “Spock’s Brain” and “Move Along Home” look normal), and so expanding upon the only redeeming quality of previous scripts — one-off jokes — is justified. (This is hardly out of line for Trek films; after all, the best — or at least most interesting — Trek film is The Journey Home, which is also the silliest.)
I more or less agree with MovieBob’s review here: Star Trek Beyond is, as a sci-fi action film, passable; as a Trek film it’s mediocre; as a Trek film in the reboot franchise, it’s surprisingly good, and would have been indicative of a worthwhile series had it been the first film. Because it’s the third, and because we’ve already lost some of the cast, the reboots as a whole probably won’t be able to recover, because even if the general improvement in quality continues, it doesn’t have enough remaining installments to become worthwhile as a whole.
“ASAP as possible” is redundant.
“ASAP as possible” is redundant. (I’m surprised you didn’t notice this typo when you went to stick it in a pull quote.)
ASAP stands for As Soon As Possible.
I’m unfamiliar with Schiffer.
I’m unfamiliar with Schiffer. Her post seemed typical of tech-related posts on medium with the exception of a fairly common typo being emphasized in a pull quote, so I figured it was an honest mistake — many authors on medium are not native english speakers.
Nope! Changing ‘responsive’ to ‘reactive’ doesn’t explain why we’d specify a server-side Microsoft stack in the context of a client-side problem on a blog about CSS, nor does it explain why we would drop words & punctuation in this context. It still seems more likely to me that the author meant “ASAP” in its conventional sense. Since she appears to be active in this thread, perhaps she can clarify.
On second thought, the article is slightly above the average stupidity of a tech article on medium.
On second thought, the article is slightly above the average stupidity of a tech article on medium. The epoch should have tipped me off :)
So, you think the tail end of that pull quote was intended to be “we need to stop building responsive web pages active server active pages as possible”? If that was the intent, then there’s still a typo here — we’re missing a conjoiner & some punctuation. In other words, “we need to stop building responsive web pages using ASAP, as possible” would make sense (even if it’s very awkward phrasing), but it would imply that only one set of technologies (the Microsoft ASP stack) is at fault. This is a very strange interpretation, considering the rest of the article is complaining about reactive web design in general.
The gatekeeper idea you mention is one of these elements that I feel is self-reinforcing.
The gatekeeper idea you mention is one of these elements that I feel is self-reinforcing. Despite filmmaking equipment getting progressively cheaper, even indie budgets are growing, & the whole industry is getting more and more insular, with low acceptance for spec scripts & a lot of big-budget low-risk franchise flicks. From the 70s straight through the 90s, even though film cameras were an expensive specialty item & editing equipment was difficult to use & to afford, we had people dropping lots of money trying to make (mostly awful) films. Today, when it’s possible for someone in the lower middle class in the west to make a feature length film with only the phone & the computer they already have (paying nothing for software), we still have big-budget theoretically-indie youtube channels, rather than an explosion in amateur narrative films.
I used to run something called the “No Budget Film Contest”. We’d take submissions of films, rank them, & give small (less than $10) cash prizes to the top 3. For a film to be eligible, the creator of the film could not have paid for anything specifically for the film, with the exception of camera rental — no paid actors, no set pieces or props, no expensive editing software. The goal was to demonstrate that one could make reasonably interesting movies using no resource other than time: in other words, that small scale productions can be valuable, and that you don’t need to get studio involvement or a successful kickstarter to make a film. It’s been a while since I hosted one of those, but it might be time again.
What?
It’s difficult to tell if you’re serious. Are you honestly unfamiliar with the colloquial use of “ASAP”, or are you just failing to make a joke?
That’s sort of my point: there’s no technical reason why feature length film budgets should be increasing rather than decreasing; instead, it’s a purely social reason: specifically, people (falsely) believe that a big budget is necessary to make something with any kind of audience. Demonstrating that a worthwhile feature-length film can be made with a budget in the tens of dollars instead of in the millions is probably the best way to counter this.
In the 90s, plenty of pictures were made for ten thousand dollars or so. That number has stuck in people’s minds as the low-end estimate, but in reality, that refers to the cost of a low-end film camera, plenty of film, and film editing equipment — or a low-end camcorder, a bunch of tapes, and a video toaster. But, today, most americans have video cameras in their pockets that knock the mid-range 90s camcorders out of the water. We need to redefine the idea of the “shoestring budget” for film from $10k to twenty bucks.
A Qualified Defense of Jargon & Other In-Group Signifiers
This essay is in part a response to an article I read this morning. I won’t link to the article, in part because I don’t want to diminish it by talking over it, and in part because the story that this article tells is pretty common — plenty of people have complained about exactly the same thing, myself included. The article in question told the story of a self-taught programmer whose unfamiliarity with certain terms and historical figures theoretically irrelevant to her work marked her as an outsider & made her uncomfortable in a professional environment. I sympathize — after all, I too was once a self-taught developer in over my head, and I’ve gone through the process of trying to work in an environment where my unfamiliarity with the shibboleths marked me as an outsider. I also don’t want to imply that her situation is justified — accidents of biology & history like gender, ethnicity, and childhood cultural background can make the bar for fitting into particular groups much higher than it should be. What I’d like to avoid is the shallow idea that we can just reject shibboleths altogether and solve all problems.
Jargon is a clear cut example of when a tool that’s instrumental for competency also acts as a shibboleth. Most fields with complicated forms of technical knowledge also have specialized vocabulary or specialized meanings for particular words; the general consensus surrounding the meaning of a particular word stands in for a vast chunk of shared knowledge that would be complicated to explain in non-specialized language. When someone says “O(n²)”, they are making a claim that, in order to be understood, implies a familiarity with the calculus of derivatives, instruction counting, how loops work, and the idea of worst case versus expected case outcomes. Working with someone whose familiarity with foundational concepts in your field is at the level of an outsider is nearly impossible: you need to act the part of professor more often than you do real work; if someone lacks familiarity with the terminology, your handle for figuring out their knowledge level is no longer usable. Autodidacts are already at a disadvantage here: the set of things they know is not the same as that taught by a degree program, and they can have large conspicuous gaps in their knowledge even when it comes to relevant material. When an autodidact is unfamiliar with a term, it’s hard to tell whether or not that indicates a gap in their competence that needs to be filled, taking up time and effort that could otherwise be applied to the ostensible end goal.
Even when a piece of jargon isn’t of direct relevance, familiarity with jargon is indicative of a shared cultural experience that reflects shared values and habits — some pieces of which can be highly relevant. Using a term like “dot file” when referencing a hidden configuration file indicates familiarity and comfort with the unix environment, which might extend to a familiarity with pipes, standard unix tools, shell scripting, basic unix system administration, the history of the free software and open source movements, unix-style development toolchains, a preference for classic text editors like vi & emacs over graphical IDEs, an understanding of the unix philosophy of small modular programs working together on text streams, a familiarity with and preference for irc over other similar communication systems, a fondness for beards, and the ownership of toys shaped like penguins or tennis shoe wearing demons. Someone who just says “config file” will not open the floodgates of spurious cultural associations. (Likewise with “home directory” versus “user directory” or “my documents”.) All of these associations are fuzzy: not everyone has equal immersion in the cultural ephemera related to some technology. However, there’s a set of expectations associated with use of terminology: kinship with a set of tribes with clear centers and fuzzy boundaries.
Similarly, familiarity with certain historical figures has cultural relevance that extends to philosophy with meaningful applications in work environments. Somebody familiar with Djikstra who only learned about him in school probably only knows about him in the context of graph traversal, but someone familiar with Djikstra’s mythology is likely to be familiar with a couple quotes: “asking if a computer can think is like asking if a submarine can swim” and “premature optimization is the root of all evil”. Even if misattributed & misunderstood, both those quotes have impacts on the way someone develops software that go far beyond familiarity with graph traversal algorithms. Someone who quotes Postel’s Law, similarly, has a particular philosophy that’s easy to identify. All of these philosophies have impact, though the value of this impact depends on the application: Postel’s Law was instrumental in both the widespread adoption of the world wide web and the horrible security shitshow that the world wide web can’t extract itself from.
This is all to say that culture matters and signifiers matter. If you’re working alone, it’s fine to focus only on the precise technical ideas that stand between you and your immediate goal; as soon as you start working with a group, an inability to code switch means your coworkers can’t predict your behavior, communicate with you efficiently about important things, or figure out what you need to know in order to do your job effectively. Hiring an autodidact is a big risk, which is why lots of places don’t bother — it’s not that a degree implies a high level of competence, but instead that a degree is a hedge against a low level of competence that would cost the company a lot of money; hiring an autodidact with big gaps in their understanding of jargon and culture is not merely a risk but an almost-guaranteed cost, even if the person in question has significantly above-average technical skills and work ethic.
All of this is a huge problem, because the tech industry has a number of problems related to a lack of diversity — particularly cultural diversity. We need to shake this industry up with outside ideas, and part of that is bringing in autodidacts who have experiences unlike those systematically manufactured by degree programs or accelerators. We need people who can point at the inconsistencies and stupidities that are passively accepted as normal by the monoculture. But, we need those people to speak the language.
If we don’t throw away shibboleths entirely, what are we to do?
My recommendation is: outsiders, know your enemy. If you’re learning to code on your own, pay attention to the mythology. Read the Jargon File. Read The Devouring Fungus. Read Hackers & Where Wizards Stay Up Late. Make sure that you have enough understanding of the history and culture to pass as a precocious newbie.
As for insiders: be conscious of the way shibboleths are being used. Just because someone doesn’t know the term you used doesn’t mean they are totally unfamiliar with the concept. Make sure you aren’t leaning too heavily on spurious associations: playing Tempest doesn’t really have anything to do with understanding depth-first searches, Stallman’s association with Linux isn’t much more than a series of accidents that don’t meaningfully impact the syntax of awk, and LISP’s vast mythology is mostly the product of it being really popular at MIT fifty years ago rather than some deep truth about lambda calculus. Make sure that, to the extent that you are using shibboleths as a proxy for competence, you are not doing so in a way that unfairly prevents competent outsiders from contributing meaningful work. And, especially, give the benefit of the doubt to anyone who isn’t a cultural fit for reasons beyond their control: assume that women & people who are neither white nor asian are competent, because everyone else in their lives will assume they are incompetent.
A device and method for escaping the universal truth
ABSTRACT
A device and method for escaping the universal truth. The devices comprises a panoptic system, an inverse mirror, a universal system, a whole edifice, a whole system, an original book, an uninterrupted circuit, an artificial mosaic, a nervous system, a single gadget
BRIEF DESCRIPTION OF THE DRAWINGS
Figure 1 schematically illustrates the critical obsession with its aura.
Figure 2 is an isometric view of The other aspect of this process.
Figure 3 schematically illustrates no strategic stakes at this conjuncture.
Figure 4 is a block diagram of an unforeseen twist of events and an irony.
Figure 5 is a perspective view of The machine-readable form of the input.
Figure 6 is a block diagram of any other symptom in classical medicine.
Figure 7 is a block diagram of the discrete charm of the gravity.
Figure 8 is a schematic drawing of the logical evolution of a universal system.
Figure 9 is a schematic drawing of the second sentence of the epigraph.
Figure 10 illustrates the same time by the cartographer.
DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS
The present invention witnesses the end of the negative form. The present invention separates one pole from the very swing. The invention seals the end of the abolition. The invention is the slope of a false problem. The invention is the truth of this new age. The device signifies a setback in the sense. The invention makes any sense of a conventional , restricted perspective.
The present invention is an escalation of the whole edifice. The present invention produces nothing but the discrete charm of the gravity.
What is claimed is:
1. A device for escaping the universal truth, comprising:  an original book; and   a universal system.
2. The device of claim 1, wherein said original book comprises the discrete charm of the gravity.
3. The device of claim 1, wherein said universal system comprises an unforeseen twist of events and an irony.
4. A method for escaping the universal truth, comprising:  a single gadget;   a universal system;   an inverse mirror; and   an uninterrupted circuit.
5. The method of claim 4, wherein said single gadget comprises the discrete charm of the gravity.
6. The method of claim 4, wherein said universal system comprises the logical evolution of a universal system.
7. The method of claim 4, wherein said inverse mirror comprises The other aspect of this process.
8. The method of claim 4, wherein said uninterrupted circuit comprises The machine-readable form of the input.
A method and device for becoming a property owner himself
ABSTRACT
A method and device for becoming a property owner himself. The devices comprises an old system, a mere instrument, a militant working, a Polish edition, an international product, a present system, a big factory, an other working, an eight-hour working, a Danish edition, an indispensable cloak, a first edition, a democratic newspaper, a English edition, a German edition, a whole system, an entire surface, a European working, a feudal system, a social stronghold, a revised edition, a chief load, a German working, a whole article, a bribed tool, a last resort, an own working, a transcendental robe, a modern working, an essential product, a 23-page pamphlet, a Russian edition, a mighty weapon, a second edition, a collective product, a present edition, a respective working, a subsequent edition, a great factory, an other article, a mechanical loom, a poor stock-in-trade, an own house, a sentimental veil, a parliamentary stronghold, a new machine, a representative system, a heavy artillery, a previous edition, a gigantic broom, a common platform, a European system, a capitalist system, a rough sketch, a whole working
BRIEF DESCRIPTION OF THE DRAWINGS
Figure 1 is a diagrammatical view of the progressive historical development of the proletariat.
Figure 2 is a schematic drawing of the first revolution in which the working class.
Figure 3 is a schematic drawing of a certain stage in the development.
Figure 4 is a schematic drawing of a corresponding political advance of that class.
Figure 5 illustrates the first conditions for the emancipation.
Figure 6 is a diagrammatical view of the influential bourgeoisie during the French revolution.
Figure 7 is a cross section of an independent section of modern society.
Figure 8 is a perspective view of all political action on the part.
Figure 9 is an isometric view of an exaggerated form of the ancient struggle.
Figure 10 is a diagrammatical view of the main consequences of the abolition.
Figure 11 is a block diagram of the same pace at which the progress.
Figure 12 is a perspective view of the working-class parties of every country.
Figure 13 is a cross section of the rural producers under the intellectual lead.
Figure 14 is a cross section of the vanished status of the workman.
Figure 15 is a schematic drawing of the great factory of the industrial capitalist.
Figure 16 is a diagrammatical view of the inevitable impending dissolution of modern bourgeois property.
Figure 17 is a cross section of the necessary offspring of their own form.
Figure 18 is a schematic drawing of the miraculous effects of their social science.
Figure 19 is a cross section of the historical movement as a whole.
Figure 20 is a diagrammatical view of the old system of manufacture or industry.
Figure 21 is a block diagram of the miserable character of this appropriation.
Figure 22 schematically illustrates the whole range of old society.
Figure 23 is a schematic drawing of the political form at last discovered.
Figure 24 schematically illustrates the icy water of egotistical calculation.
Figure 25 is an isometric view of a national bank with state capital.
Figure 26 is a diagrammatical view of the holy water with which the priest.
Figure 27 schematically illustrates the positive form of that republic.
Figure 28 is an isometric view of all other branches of the administration.
Figure 29 is a diagrammatical view of a rough sketch of national organisation.
Figure 30 is a perspective view of The essential conditions for the existence.
Figure 31 is a perspective view of a new proletarian line during the discussion.
Figure 32 is a diagrammatical view of The first direct attempts of the proletariat.
Figure 33 illustrates The proper form of their joint-stock government.
Figure 34 is a perspective view of the proletarian differ from the serf.
Figure 35 is a cross section of a fantastic conception of its own position.
Figure 36 is an isometric view of a powerful coefficient of social production.
Figure 37 is a cross section of a clear understanding of the character.
Figure 38 illustrates the actual position of first class.
Figure 39 is a cross section of a comprehensive formulation of the proletarian movement.
Figure 40 is a cross section of the rapid development of Polish industry.
Figure 41 is a block diagram of the momentary interests of the working class.
Figure 42 is an isometric view of the physical force elements of the old government.
Figure 43 is a perspective view of the senile mountebank at its head.
Figure 44 is a cross section of the healthy elements of French society.
Figure 45 is a diagrammatical view of the ultimate general results of the proletarian movement.
Figure 46 is a schematic drawing of the classical works of ancient heathendom.
Figure 47 is a perspective view of a mighty weapon in its struggle.
Figure 48 is an isometric view of The first , fundamental condition for the introduction.
Figure 49 is a perspective view of these philosophical phrases at the back.
Figure 50 is a diagrammatical view of an agrarian revolution as the prime condition.
Figure 51 illustrates no other nexus between man and man.
Figure 52 is a block diagram of the whole superincumbent strata of official society.
Figure 53 is a perspective view of the economic and political sway of the bourgeois class.
Figure 54 schematically illustrates the same course as its predecessor.
Figure 55 is a schematic drawing of every other employer in the search.
Figure 56 is a block diagram of the disastrous effects of machinery and division.
Figure 57 is a diagrammatical view of all numerous vanguard of scientific socialism.
Figure 58 is a perspective view of the social power of the nobility.
Figure 59 is a cross section of the first time in the introduction.
Figure 60 illustrates the individual workers so that the worker.
Figure 61 is an isometric view of the social character of the property.
Figure 62 is an isometric view of the economic progress of the country.
Figure 63 is an isometric view of the last great reserve of all European reaction.
Figure 64 is a schematic drawing of a new proof of the inexhaustible vitality.
Figure 65 illustrates the oracular tone of scientific infallibility.
Figure 66 is a diagrammatical view of the necessary condition for whose existence.
Figure 67 illustrates the civilized countries of the world.
Figure 68 illustrates the French criticism of the bourgeois state.
Figure 69 schematically illustrates the immediate demands of the movement.
Figure 70 illustrates the first class of the country.
Figure 71 is a diagrammatical view of the scattered state of the population.
Figure 72 is a diagrammatical view of a considerable part of the population.
Figure 73 is a diagrammatical view of the entire bourgeois society on its trial.
Figure 74 is an isometric view of the intellectual development of the working class.
Figure 75 is a schematic drawing of the first radical attack on private property.
Figure 76 illustrates the free movement of , society.
Figure 77 is a diagrammatical view of the latter stands at a higher stage.
Figure 78 is a schematic drawing of a new collection of the work.
Figure 79 is an isometric view of the Communistic abolition of buying and selling.
Figure 80 is a block diagram of a supplementary part of bourgeois society.
Figure 81 is a diagrammatical view of the economic conditions for its emancipation.
Figure 82 illustrates The political rule of the producer.
Figure 83 is a block diagram of the special privileges of the nobility.
Figure 84 is a schematic drawing of a revised edition of this earlier draft.
Figure 85 is a cross section of the full development of every previous revolution.
Figure 86 is a diagrammatical view of the true originators of the war.
Figure 87 is a diagrammatical view of the necessary consequence of the creation.
Figure 88 illustrates a 23-page pamphlet in a dark green.
Figure 89 is a block diagram of the revolutionary element in the tottering feudal society.
Figure 90 schematically illustrates the bombastic representative of the petty-bourgeois.
Figure 91 schematically illustrates an undeveloped state of both agriculture.
Figure 92 is a diagrammatical view of no other conclusion that the lot.
Figure 93 is a block diagram of a new guarantee of its impending national restoration.
Figure 94 is a schematic drawing of the socialist movement spreads among them and the demand.
Figure 95 is a perspective view of the entire surface of the globe.
Figure 96 is a schematic drawing of a decided progress of Polish industry.
Figure 97 is a perspective view of the true conditions for working-class emancipation.
Figure 98 is a block diagram of the other countries of the world.
Figure 99 is a perspective view of the same pace as the growth.
Figure 100 is a block diagram of the bourgeois sense of the word.
Figure 101 is an isometric view of the gradual , spontaneous class organisation of the proletariat.
Figure 102 is an isometric view of the programme document in the course.
Figure 103 is a block diagram of the political and intellectual history of that epoch.
Figure 104 is a schematic drawing of a full and substantial exposition of the new revolutionary.
Figure 105 is a schematic drawing of The undeveloped state of the class.
Figure 106 illustrates the first instinctive yearnings of that class.
Figure 107 schematically illustrates a progressive phase in the class.
Figure 108 illustrates the bold champion of the emancipation.
Figure 109 is an isometric view of the right man in the right place.
Figure 110 is a perspective view of the intellectual development of the mass.
Figure 111 is a perspective view of the same time markets for the sale.
Figure 112 illustrates the absolute monarchy as a counterpoise.
Figure 113 schematically illustrates these first movements of the proletariat.
Figure 114 is a diagrammatical view of the Italian proletariat as the publication.
Figure 115 is an isometric view of the little workshop of the patriarchal master.
Figure 116 is a cross section of the working class of the 19th century.[vi.
Figure 117 is a perspective view of all coercive measures against the working class.
Figure 118 is a diagrammatical view of the continued existence of bourgeois society.
Figure 119 schematically illustrates the agricultural population on the land.
Figure 120 is an isometric view of the real action of the working class.
Figure 121 illustrates a necessary condition of communist association.
Figure 122 is a perspective view of the practical absence of the family.
Figure 123 is a cross section of The rural communities of every district.
Figure 124 is a schematic drawing of the leading body of the Paris circle.
Figure 125 is a schematic drawing of The productive forces at the disposal.
Figure 126 is a perspective view of a final stage in the reorganisation.
Figure 127 is a schematic drawing of the misty realm of philosophical fantasy.
Figure 128 is a cross section of the ultimate disappearance of private property.
Figure 129 is an isometric view of the exact contrary of its real character.
Figure 130 schematically illustrates the big industry of our own day.
Figure 131 is a diagrammatical view of the self-conscious , independent movement of the immense.
Figure 132 illustrates the savage warfare of Versailles outside.
Figure 133 schematically illustrates the further consequences of the industrial revolution.
Figure 134 schematically illustrates a national bank with State capital.
Figure 135 is a diagrammatical view of the sleeping partner of the capitalist.
Figure 136 is a cross section of the final approval of the programme.
Figure 137 is a cross section of the ultimate form of the state.
Figure 138 is a cross section of the great struggle of the day.
Figure 139 is a cross section of all local markets into one world.
Figure 140 is a schematic drawing of the first step in the revolution.
Figure 141 illustrates the upper stratum of the working class.
Figure 142 is a schematic drawing of the international union of the proletariat.
Figure 143 is a diagrammatical view of every villainous meanness of this model.
Figure 144 illustrates the various wards of the town.
Figure 145 schematically illustrates the last remnants of their independence.
Figure 146 is a perspective view of the hallowed co-relation of parents and child.
Figure 147 is an isometric view of a full member of a guild.
Figure 148 is an isometric view of a vast association of the whole nation.
Figure 149 is a perspective view of the political liberation of the proletariat.
Figure 150 schematically illustrates the political conditions of the Continent.
Figure 151 illustrates The new draft for the programme.
Figure 152 is a block diagram of the peaceful abolition of private property.
Figure 153 is a cross section of the front the common interests of the entire proletariat.
Figure 154 is a perspective view of the peculiar mysteries of the Picpus nunnery[xxii.
Figure 155 is a schematic drawing of the immediate consequences of the industrial revolution.
Figure 156 is a schematic drawing of the same way in which a foreign language.
Figure 157 is a schematic drawing of the social and political aspirations of the European working.
Figure 158 is a block diagram of the first Paris Revolution in which the proletariat.
Figure 159 is a block diagram of the second paragraph of point 9 and the last sentence.
Figure 160 is a cross section of the secret societies of the time.
Figure 161 illustrates a vague aspiration after a republic.
Figure 162 is a perspective view of the direct or indirect dominance of the proletariat.
Figure 163 is a diagrammatical view of The individual members of this class.
Figure 164 schematically illustrates the proletarian differ from the handicraftsman.
Figure 165 is a diagrammatical view of the full consciousness of their historic mission.
Figure 166 is a cross section of a bribed tool of reactionary intrigue.
Figure 167 is a block diagram of the monarchical form of class rule.
Figure 168 is a diagrammatical view of the common cause with the party.
Figure 169 schematically illustrates An oppressed class under the sway.
Figure 170 is a cross section of the motley feudal ties that bound man.
Figure 171 schematically illustrates The inner organisation of this primitive communistic society.
Figure 172 is a cross section of the historical conditions for the time.
Figure 173 is a cross section of the feudal organisation of agriculture and manufacturing.
Figure 174 is a perspective view of the great mass of the proletariat.
Figure 175 schematically illustrates the hostile antagonism between bourgeoisie and proletariat.
Figure 176 is a diagrammatical view of The bourgeois clap-trap about the family.
Figure 177 is a block diagram of the commercial crises that by their periodical.
Figure 178 schematically illustrates the other political parties of our time.
Figure 179 schematically illustrates the basic conditions of this society.
Figure 180 is a schematic drawing of the old cries of the restoration.
Figure 181 illustrates the reactionary character of their criticism.
Figure 182 schematically illustrates a new onslaught upon the power.
Figure 183 is a block diagram of any distinctive social validity for the working class.
Figure 184 is a perspective view of the first part of the book.
Figure 185 is an isometric view of the central seat of the old governmental power.
Figure 186 is a schematic drawing of a shameless display of gorgeous , meretricious and debased luxury.
Figure 187 is a diagrammatical view of the proletarian differ from the slave.
DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS
The present invention is a 23-page pamphlet in a dark green. The present invention grows emancipation struggle of the proletariat. The present invention examines the movement of the working class. The invention constitutes the last great reserve of all European reaction.
In accordance with an alternative specific embodiment, the present invention found their only salvation from the proletariat. The device becomes the signal for a proletarian revolution. The device constitutes the foundation for the political and intellectual history. The present invention prepares the way for a more complete insight. The present invention brings out a book with the name. The present invention reflects the history of the modern working-class movement. The device is the act of the working class. The present invention attains its emancipation from the sway. The invention has again gathered sufficient strength for a new onslaught.
According to another embodiment, the device reflects the history of the modern working-class movement. The device is the task of the working class. The device is no better witness than this day. The invention regards their position as the working class. The invention indicates a decided progress of Polish industry. The invention is a necessity for the harmonious collaboration. The invention is the work of the working class. The device achieves the international union of the proletariat. The invention is itself the product of a long course. The invention remains no other nexus between man and man. The present invention finds its fitting complement in the most slothful indolence. The invention creates a world after its own image. The invention puts the existence of the entire bourgeois society.
In accordance with an alternative specific embodiment, the present invention endangers the existence of bourgeois property. The invention becomes an appendage of the machine. The present invention is the self-conscious , independent movement of the immense. The present invention layers the foundation for the sway. The device abolishes the right of personally acquires property. The present invention is the groundwork of all personal freedom. The present invention is the miserable character of this appropriation. The invention is the non-existence of any property. The invention deprives no man of the power. The invention finds its complement in the practical absence. The invention alter the character of that intervention. The device keeps even pace with the dissolution.
The present invention is the most radical rupture with traditional property. The device is the condition for the free development. The invention comprehends the march of modern history. The device is the necessary offspring of their own form. The present invention conceals the reactionary character of their criticism. The present invention has Clerical Socialism with Feudal Socialism. The invention is the head of this school. The device reversed this process with the profane French literature. The present invention expresses the struggle of one class. The device presupposed the existence of modern bourgeois society. The present invention is the object of the pending. The device serves the government as a weapon. The present invention proclaims its supreme and impartial contempt of all class. The invention secures the continued existence of bourgeois society. The present invention keeps even pace with the development. The present invention improves the condition of every member. The invention is a final stage in the reorganisation. The invention is the result of the whole. The invention prepares the way for your community. The invention does the proletarian differ from the slave. The device is the property of one master. The invention does the proletarian differ from the serf. The invention has the use of a piece. The invention does the proletarian differ from the handicraftsman.
According to another embodiment, the device joins the movement of the proletariat. The invention is the theory of a liberation. The device defend the cause of the proletariat. The device is the political liberation of the proletariat.
In accordance with an alternative specific embodiment, the present invention guarantees the subsistence of the proletariat. The present invention prepares the way for its transformation. The present invention is that stage of historical development. The invention is a revised edition of this earlier draft. The device draws up a programme in the form. The invention drafts a programme in the form. The device does not draw profit from any kind. The invention reduced the activity of the individual worker. The invention does this sale of the labor. The present invention accomplishes the liberation of their respective working. The present invention makes itself the first class of the country. The device annihilated the power of the aristocracy. The present invention is a lack of the necessary capital. The device takes the form of constitutional monarchy. The present invention renders the condition of the proletariat.
The present invention follow the same course as its predecessor. The invention characterizes the revolution in the whole social order. The invention is the necessary consequence of the creation. The present invention is the course of this revolution. The device is the victory of the proletariat. The present invention ensures the livelihood of the proletariat. The device requires an entirely different kind of human material. The device controlled by society as a whole.
In accordance with an alternative specific embodiment, the present invention is the influence of communist society. The present invention is the stage of historical development. The device establishes the rule of the aristocracy. The invention makes the common cause with the party.
The present invention facilitates the unification of the proletariat. The present invention popularise this programme document during the revolution. The device become the property of the state. The invention regulates the credit system in the interest. The device substitutes paper money for gold and silver. The invention become the property of the state. The present invention serves nascent bourgeois society as a mighty weapon. The present invention intensified the class antagonism between capital and labor. The invention marks a progressive phase in the class. The present invention is a regime of avowed class.
According to another embodiment, the device opens an abyss between that class. The present invention professed to rest upon the peasantry. The device upholds their economic supremacy over the working class. The device transfers the supreme seat of that regime.
According to a beneficial embodiment, the invention is the positive form of that republic. The invention is the suppression of the standing. The device becomes a reality by the destruction. The device is the embodiment of that unity.
According to a preferred embodiment, the invention serves every other employer in the search. The present invention puts the right man in the right place.
According to a preferred embodiment, the invention clogs the free movement of , society. The device supplies the republic with the basis. The present invention regulates national production upon common plan. The device takes the management of the revolution. The device is the first revolution in which the working class. The invention have put enlightenment by the schoolmaster. The device solves in favor of the peasant. The present invention stops the spread of the rinderpest. The invention display their patriotism by organizing police. The present invention is the abolition of the nightwork. The present invention hampered the real action of the working class. The device retraces this dissolution in The Origin. The present invention carries brazen historical irony as a result. The device is the seat of the emigr� government.
What is claimed is:
1. A method for becoming a property owner himself, comprising:  a previous edition;   a 23-page pamphlet; and   a German edition.
2. The method of claim 1, wherein said previous edition comprises the sleeping partner of the capitalist.
3. The method of claim 1, wherein said 23-page pamphlet comprises the further consequences of the industrial revolution.
4. The method of claim 1, wherein said German edition comprises the special privileges of the nobility.
5. A device for becoming a property owner himself, comprising:  a feudal system;   a mechanical loom;   an own house; and   a parliamentary stronghold.
6. The device of claim 5, wherein said feudal system comprises a national bank with State capital.
7. The device of claim 5, wherein said mechanical loom comprises the political form at last discovered.
8. The device of claim 5, wherein said own house comprises the bold champion of the emancipation.
9. The device of claim 5, wherein said parliamentary stronghold comprises the main consequences of the abolition.
The political post I’ve been avoiding writing
The political post I’ve been avoiding writing
This is a post about the current election. I didn’t want to write it, for several reasons. One is that none of my insights are my own, and I expected to see lots of other people making them. Another is that I don’t particularly like making or seeing political posts — even political posts that I generally agree with make me irrationally angry, because they stir up all that stressful tribal instinct. The third reason is that posting about american elections doesn’t and can’t really shed any light on my genuine political positions, because I’ve never seen a political system that really lent itself to the way I feel about politics. However, because somehow nobody is making the arguments that seem obvious to me, I’m going to have to make them, with the caveat that readers probably shouldn’t take this as an indication of my alignment outside the particular and very unusual circumstances that constitute this election.
My position is that, no matter your opinions on any political issues, Clinton is more likely to be a better candidate than Trump in terms of implementing them. No matter where you fall on the various political spectra, you have a better shot with Clinton. If you’re a marxist, or a neocon, or a neonazi, or a randian libertarian, or you really want the united states to become a zoroastrian theocracy, you’re better off voting for Clinton.
Here’s the thing. Trump isn’t very good at *doing* things. No matter what he tries to do, he’s pretty likely to fail; not only that, but he’s unlikely to be able to realize that he’s failed. His mental model of the universe is so disconnected from reality that he believes himself to be a successful businessman. So, if you already agree with his positions, you can’t expect him to successfully implement them. On the off chance that he succeeds in some task, it’s difficult to determine which task it will be, and once he starts something, he can not be dissuaded from continuing. In other words, voter opinion can not influence him. His positions are arbitrary and change often, but they are quite importantly not based on any kind of outside influence: if he accidentally stumbles upon some policy that’s, say, a war crime (as he has), he won’t be convinced out of it; likewise, if he stumbles upon a highly unpopular policy, its unpopularity won’t convince him to abandon it. A Trump presidency is like electing a random number generator: it’s unpredictable, can’t be reasoned with, and although it’s technically unbiased the best possible case is that it will be entirely ineffective.
Compare this to Clinton. Hillary is extremely effective as an administrator, and seems to want power for the sake of demonstrating her ability to wield it rather than for any particular end. While effectiveness in of itself is a double-edged sword in a politician, Hillary has a couple other attributes that make her power much more likely to be wielded for good. Specifically, she has a tendency to make her positions mirror that of the general population — in other words, to flip-flop in order to mirror popular sentiment (if you want to paint it negatively) — and she’s concerned in a fairly realistic way with how history will remember her. By following public opinion, she’s unlikely to perform actions that are terribly unpopular based on some kind of flawed personal conviction; however, her concern with posterity lowers the rate at which she would advocate things that may be popular in the moment but will almost certainly end up seeming terrible to future generations. To use a concrete example, she’ll publicly support gay marriage even if she privately disagrees because she knows that gay marriage is only going to continue to grow in support, but she will never support legislation like that proposed by Trump to ban the immigration of muslims (even though this would not be illegal per-se & has precedent in the Alien Act) even if it’s highly popular with the electorate because such bans will always look bad at some point in the future.
While people criticize the candidates for being weak in either confidence or implementation, there’s really only one circumstance in which a leader that has a set of strongly held positions that they implement effectively is desirable: when you agree with all those positions to a greater degree than the leader does. A strong leader with unpopular positions is a disaster of a dictator; we would rather a leader with positions with which we disagree be ineffective, because then his or her decisions would be irrelevant. Alternately, a leader whose policies are flexible but whose ability to enact policy is good can be bent to the will of the people, and becomes a tool of the people: an even better result. In a sense, to the degree that leaders are strong, we wish them to be corrupt in a very specific way: weak to the forces of popular sentiment now and in the future, but strong against the kind of organizations that exist to warp politicians’ sense of what positions have popular support (industry lobbying groups and such).
Trump doesn’t know what he wants, or how to get it.
Trump doesn’t know what he wants, or how to get it. Best case, he gets nothing done; expected case, he does something by accident based on a half-considered remark and it turns out to be something nobody wants.
Clinton, on the other hand, is likely to be “more of the same” — in other words, a general extension of the right-center policies that characterized the Bush and Obama administrations (and indeed Bill Clinton’s). She’s a known quantity, with enough experience to avoid stumbling into a big dumb war.
Clinton is a strong enough leader to execute the plans she has, which are, essentially: avoid pissing off voters, and otherwise change as little as possible. This makes her “conservative” in the traditional sense, and hers is likely to be an uninteresting and unmemorable presidency (like Carter’s).
Trump will either be considered a failure, or will succeed at something that will be considered a huge mistake. A failure would be preferable — but that just brings him down to slightly below Clinton’s expected behavior.
I completely understand the desire to make big changes.
I completely understand the desire to make big changes. I don’t find the current system sufficient. (The term closest to describing my political sentiments is “anarchist”.) That said, I can’t stand by the idea that a “middle finger vote” is justified, particularly when it’s unlikely to result in real change; breaking down a complex and resilient system like a government requires a strategy.
If you want someone to tear the system down, you will probably want to elect someone like Gary Johnson: he has a clear idea of which parts of the system he would like to remove and how to remove them.
Trump is unlikely to tear it all down. He has no reason to want to perform major restructuring; he doesn’t have enough understanding of the current structure to know how to disassemble it; even if he did, he probably couldn’t keep his attention focused on the task of tearing down government for long enough for it to happen. A Trump presidency will be comparable to that of Bush: more of the same, but with more mistakes.
To tear a system down requires more than bringing in an unpredictable wildcard. While someone like that is unpredictable in the short term, the distribution of their behavior is predictable at scale: a random walk seldom goes very far and often returns to the center, not despite but because of its randomness.
With regard to the negative sides of the status quo, I think even this favors Clinton. She will avoid war and torture whenever possible — after all, they have bad PR — and to the extent that she condones them at all she will do so very carefully. Trump will not avoid war: he will act in accordance with momentary displays of dominance, rather than in accordance with risk.
I was taking executive orders into account, here.
I was taking executive orders into account, here. I don’t doubt that Trump would issue plenty of executive orders, but I doubt that they would be effective in doing whatever he intended them to do, and I suspect that many of them would do things he didn’t expect.
Ignoring the difference between legislation & executive order in terms of how much cooperation is involved, we have two candidates neither of whom have any particular deeply held convictions by which we could predict their behavior. One of these is a highly efficient and effective administrator whose desire for power and recognition drives her behavior in very predictable ways, and who can easily be manipulated into effectively administering the country in ways that are desirable if minor. The other has behaviors that cannot be influenced or predicted, but, luckily, he’s never been terribly effective at giving orders or convincing people to do things, so to the extent that he is likely to make sweeping changes, it will be in completely arbitrary, unpredictable ways.
Good vs iconic art
Good vs iconic art
When it comes to narrative forms of art (film, literature, comics), there’s often a great disconnect between works that are highly influential (and thus become ‘classics’) and works that are well-executed. I don’t think this is an accident.
Well-executed works (hereafter, “good”) don’t particularly need to be novel. Whether they are high art or low art doesn’t matter — someone working in a “low” genre (say, harem anime) can be skilled enough to produce a truly shining example of that genre. Like good design, good art disappears: it is a complete and flawless implementation of expectations, and only upon close examination does the skill and effort involved in executing the expected attributes of the genre become visible.
As an example of unambiguously “low” art being extremely well executed, consider Monster Musume — a harem show involving young women who are mythological creatures. It seeks to sate essentially puerile desires: the goal is to have sexually-charged slapstick humor involving attractive women who are part animal. It is not the first show to do this by a long shot, but it may be the best: nearly all elements outside those that add to that goal are invisible, and it reaches its goal admirably, but at the same time it has a good and well-planned justification for nearly everything that happens (by combining the idea of diplomatic relations, corrupt/lazy officials, and a largely hostile population, you get an interesting take on the fantasy harem genre that can be seen as a stealth satire of racial politics and international relations), and it furthermore satirizes its own genre admirably (the protagonist’s name is never spoken in the show and rarely in the source material, and his facial features are often left blank; in other words, he is an exaggerated form of the self-insert character).
While “good” art may be popular in the short term, it is rarely influential: mostly, it subsumes itself in its own influences. Iconic works are by definition influential; they often come from outsiders and break best practices. While iconic works are memorable, it is rare that an iconic work is superior to those works that copy it. Consider Dracula and Frankenstein — both extremely iconic works, both widely adapted, and both (in their original form) nearly laughably incompetently constructed by the standards of both our time and theirs. It is not the high standards of craftsmanship that make Dracula and Frankenstein iconic; it’s not their popularity, either, although both were popular. Instead, each brought into being a particular collection of ideas that fit into a missing slot in the culture; this undigested piece of mental matter was sized upon and as these works were adapted or influenced other works the core interesting idea was progressively isolated from its trappings, until the point at which it becomes fully assimilated and no longer numinous. Dracula is no longer numinous to us: we have a mental image of Bela Lugosi in a cape, and we forget that he was supposed to have hairy palms, and we forget about Doctor Von Hellsing being a blood expert, and we forget about Lucy being obsessed with wax cylinder audio recording; while all of those elements are more interesting to us now, the iconic elements of that story are sufficiently captured by Bela Lugosi in a cape, to the point that this view of vampires as nobility (which did not originate in Dracula but really had its purest representation in the 1933 Universal Studios adaptation of Dracula that has become the most iconic one) has become dominant. The idea of permanently young and beautiful blood-sucking aristocrats afraid of the sun is one that resonated with the early twentieth century American culture very strongly, even as Max Schreck’s portrayal of Orlock in Nosferatu is a more accurate representation of how Dracula was portrayed in the book. Likewise: Frankenstein’s Monster originally looked far more human, and was highly intelligent (speaking several languages); we have made an icon out of an ugly Frankenstein’s Monster incapable of speech or complex thought and a Victor Frankenstein with a god complex and boundless ambition, rather than a beautiful but slightly unnerving monster and a Dr Frankenstein who wouldn’t be out of place at a My Chemical Romance concert because the former was a closer fit for exactly what was (and no longer is) unnerving about the story. Iconic works allow us to identify the unheimlich and integrate it into ourselves and our society in a disarmed form; or, to be more cynical: the Spectacle uses iconic works as an early warning system telling it what to consume next. Of course, as highly iconic figures become fully integrated, they cease to have the impact they otherwise would: recent Godzilla and King Kong movies flopped for the same reason that new viewers wouldn’t watch the originals, which is to say that these monsters have been integrated and are no longer monstrous.
Iconic works don’t need to be bad in order to be iconic, but even if they display technical excellence, they will face initial rejection. Consider Neon Genesis Evangelion: certainly iconic, and hardly poorly made, this show garnered very little interest during its initial run. Part of the reason is that it aired on an incorrect slot: this show, with its dense references to media from the 70s and its complex character dynamics and psychosexual undertones, was airing in a time slot that normally was geared toward ten year old boys. But, it’s more than that: Evangelion remains highly divisive, and remains relevant more than twenty years after it first aired, because its iconic elements have not yet been fully assimilated. Evangelion didn’t introduce the unwilling soldier (in fact, this element is part of why Gundam was iconic); it didn’t introduce the idea of a complex and incestuous conspiracy between a private high-tech defense organization and various government and religious authorities. But, Evangelion took upon itself the task of examining attributes of the mech genre realistically, and did so by taking a bunch of characters who border on archetypal and spending a great deal of effort trying to make their behavior and characterization realistic. Evangelion is still relevant because we haven’t figured out what makes it relevant yet: every post-Evangelion mech show is in some way a response to Evangelion in the same way that every post-Dracula vampire novel was a response to Dracula, yet even as some individual creators have done several generations of responses, none are a sufficient substitute for the original. For instance, Yoji Enokido, after working on Evangelion, went on to work on Utena (a non-mech show with certain very visible Evangelion references), Rah Xephon (a mech show that has been seen as an Evangelion clone), Star Driver (a mech show very similar to Rah Xephon with a more typical mecha protagonist), and Captain Earth (another mech show with a more typical mecha protagonist, which has some scenes directly lifted from Rah Xephon); while his vision has diverged from Evangelion proper, he’s still chewing a piece of the same cud.
How to shoot yourself in the foot with good worldbuilding: two methods
How to shoot yourself in the foot with good worldbuilding: two methods
Sometimes, in the course of telling one story, you make changes to the imagined world you’re creating that opens up the possibility of telling a much more interesting story. This story can’t be told without moving focus away from the characters you’re currently involved with. No matter; there’s no way that these main characters are more interesting to your reader than they are to you, and if there’s demand for them, you can explore what’s going on with them later.
Always tell the most interesting story going on in your world.
Sometimes, you are telling the most interesting story going on in your world, but readers are losing interest: a formerly rabidly creative fandom has stopped writing fanfiction, complaining that the new installments feel hollow. You told the most interesting story in your world, but you limited your world so that the story you were telling was the only interesting story to be told.
Always make sure that someone smarter than you can tell a more interesting story in your world than you can imagine.
I agree that “For You” used to have a much better signal to noise ratio.
I agree that “For You” used to have a much better signal to noise ratio. I’m tempted to blame the increased size of Medium’s user base — after all, with a small user base it’s a lot easier to tweak the algorithm for good results, a small sample set is a good enough proxy for recommendation quality, and few people are going to be trying to game the system, while all of those things grow worse at at least a geometric rate as the number of users grows.
That said, I’m seeing a particular kind of bad post pop up much more frequently on Medium in the past ~2 years than previously: the short post that should be much shorter. I see a lot of medium posts that are two paragraphs long, making a point that would be clearer as a tweet, along with a couple huge and irrelevant images and somebody’s affiliate link.
I used to see these mostly as HN links (since I browse HN’s “newest”), and chalked it up to the culture of shallow self-promotion that’s all too common on HN (combined with the fact that, like Blogger a decade ago, Medium provides people who have no technical skills with a free blog that looks relatively professional). Because I rarely interact with such posts other than viewing them, I doubt that my feed is full of them due to this early exposure; instead, I suspect that this behavior has become normalized — it’s now expected to post “one minute reads” that are actually ten second reads, and my darling “twenty minute reads” and “eighty minute reads” are seen as bad for business, creating a low view to read ratio — nevermind the fact that there aren’t any ads on Medium (other than native ones) so impression metrics don’t really matter.
Another possible source is following people who have lower standards for recommendations than I do. Such people may typically write nice, long, well-considered posts, but yet recommend all the crap they agree with, whether or not it’s worth reading. Medium doesn’t distinguish between following somebody for their posts and following them for their recommendations.
If your graphic design is optimized for impact at a distance without scrolling, your use case is advertising (or something like it).
The typical use case for the web should not be an ad.
Maximizing the amount of legible text on the screen encourages people to create informationally dense content — it means that a short article like yours looks lonely and empty, while longer articles seem like a more natural fit. This seems like a pretty good thing to encourage; I certainly prefer long, in-depth writing to shorter, shallower stuff.
To the extent that designers should be allowed to control default text size, for primarily written content, ten pixels is probably the upper limit for what I’d consider acceptable; any more artificially inflates the perceived size of a piece of writing. However, web designers have a lot more control over the way that sites look and act than they probably should for accessibility reasons: if I set my browser’s default text size to six, websites should accept that, the same way they should if someone with poor vision sets their default text size to thirty.
An increased default size for body text points to a set of values that are already far too common on the web: a preference for flashy showmanship over well-delivered content. Good design steps aside and becomes invisible; the exhibitionist design-masturbation of huge body text never can.
I wish I could recommend this article multiple times.
Here’s the thing. I read a lot of news articles on technical subjects, and so I’m extremely aware of all these patterns (so much so that I expected, before reading this article, that I would have a lot to add); but, I can’t possibly be reading as many of these as the people who write them for a living. Occasionally I see an author whose work I respect move to a different context and start making these mistakes. Are they being enforced? Are they just popular because they are easy?
Please badmouth CSS more in developer talks
A response to a popular article.
The appropriate response to a perceived competence gap between web development and application development is not to fake admiration for the worst tools of web developers, but instead a concerted effort to improve tools and knowledge in both communities. If someone’s complaints about CSS make you feel like your skills are being belittled — well, that’s probably an indication that you need to improve your skills, and learning why your preferred tool is bad is a good first step.
The fact that some people are capable of making impressive things with a tool does not make the tool good. Making impressive things with bad tools (or with good tools that are intended for a completely different purpose) is a tradition in the tech community; it’s called hacking. Writing a text adventure in postscript is impressive only because doing so is a terrible idea. Likewise, modern web development is impressive because HTML and CSS are limited enough to make most things that would be easy in other domains very difficult in a browser. This is not a point in favor of CSS; it is a point against it. A tool is good if easy things are easy in it and hard things are only slightly harder; CSS fails this test.
Normalizing the use of a poor tool in which a great deal of effort is necessary to solve common problems has knock-on effects. If an absolute beginner can’t perform extremely common tasks (in other words, if new users are buried under an avalanche of gotchas), those tasks are pushed onto intermediate users; more difficult tasks are relegated to advanced users; difficult tasks that need to then operate consistently and reliably — well, that’s just something nobody has time with. And, if you cut corners and bring on somebody who is slightly less skilled than is necessary, you’re more likely to get inconsistent and unreliable results even for simple tasks, because the difficulty curve is all screwed up and beginners don’t know the snags they haven’t researched yet. This is a pattern that will happen with any poorly-designed, over-complicated, inconsistent tool: anything created with the tool will be systematically slightly worse than anything created by someone of similar competence with a well-designed tool.
Pretending a bad set of tools is good lowers the bar for good tooling. It encourages an environment where bad tools are the norm, and encourages people to learn only bad tools. Just as the web is an absolute horror show (ultimately just because Tim Berners Lee cut a bunch of corners in 1992), we have big groups of people who think using hadoop & hive is a good idea when a single unix command line running on one core will do the same amount of processing in 1/80th of the time, and we have academic fields where significant numbers of statistical errors in published papers are resulting from bugs in Microsoft Excel. Bad tools should be shamed, and use of bad tools should be limited and careful.
Computer programmers have spent a lot of their history in the sandbox. In the 60s and 70s most of the interesting things being done on computers didn’t have to be stable or reliable; our modern programmer culture derives mostly from the group that “shot from the hip”, rather than from the serious and conservative professionals who were crunching numbers on IBM boxes during this era. From the late 70s through to the mid 90s, personal computers were mostly not networked, and for part of that time permanent storage was limited — the cost of a mistake was that the end user had to reboot the machine, usually, and even though hardware memory protection facilities existed on PCs after 1987, they remained unused for the next ten years. Meanwhile, those who used the internet were universally technical and could expect to fix their own problems. Sloppy development, and development tools that made non-sloppy development difficult, became normal. But, we aren’t in the sandbox anymore; poor decisions made for toy projects in the early 90s are coming back to bite us daily. Poor tools and sloppy decisions are no longer acceptable.
You had me up until “short but complete”. Lessons of less than ten minutes? Come on.
Typical video lessons, like typical classroom lessons, contain a bit at the beginning that’s a review of previous material relevant to the current lesson, and a bit at the end that gives an overview of the current lesson and a teaser for the next. While the teaser portion can be omitted, an overview of previous relevant material cannot be eliminated: people frequently view these things out of order, or zone out through part of them, or misunderstand which points are central. Ten minutes isn’t nearly long enough to cover enough content to be worthwhile if we also have these overviews.
In domains with practical application, like the programming-related domains you focus on in your article, there’s a place for extremely short chunks of information. Specifically, after finishing a course or initially learning the material some other way, a person may need a quick reminder while actually applying the material. A video or audio lesson is a terrible match for this use case: finding and skipping to the relevant information is slow and error-prone. For short chunks of information, text is ideal. Audio and video based courses, however, have the edge in introducing new material to a partly or mostly passive audience.
Your other points generally make sense. But, I’d argue in favor of 1–2 hour audio/video lessons plus textual review/summary sheets.
You missed one *big* factor in why SV isn’t good to emulate: SV is mostly “show business” — big phantom valuations for vaporware, etc. SV’s startup culture is systematically absurdly prone to being taken in by BS, compared to all the places where a “tech startup” is just called a “small business”.
It’s one thing to mimic a system that’s truly been shown to work once. It’s another thing to mimic the simulacrum described in the marketing materials of a system that, like a ponzi scheme, pulls in wave upon wave of fresh meat to dash against the rocks in order to produce the raw material used to prop up a handful of names that can be passed off as “success stories” in later brochures. Because of the enormous amount of churn and hype, it’s often hard to tell that SV has in many ways an abnormally high failure rate, with even its successes still failing to make money; after all, real success is boring and undramatic: real success is the family laundromat on the corner that’s been running for eighty years, not the social network for dogs that gets a six billion dollar valuation on paper because the VC had the hots for one of the presenters and then crashes six months later because nobody wanted it.
The way I think of it, trying to learn computer science without learning C is like being a medieval scholar trying to learn medicine without first learning Latin: whether or not you *like* it doesn’t matter, because it’s simply the language everything has been written in since the 70s; while you can get by without learning any one specific minor language (read: pretty much everything but C is a minor language), no matter what language you prefer to write in you will need to be able to read C.
Upending the system requires effort and planning.
Upending the system requires effort and planning. Electing a moron is going to produce business-as-usual-with-extra-drama, which is not an improvement over business-as-usual-with-extra-efficiency.
Actual improvement has to come from someone with a knowledge of the mechanisms of government and how they can be subverted. We don’t have time for a fuzzing attack on government — by the time such an attack has any meaningful results we’ll be long dead. We need someone to exploit known vulnerabilities.
You’re struggling with some of the same conundrums as I am.
For some years, I’ve been playing with automated text generation. After seeing the coverage of the gaming of short Kindle erotica, I verified that erotica was easy to generate. But, while I would have very little problem “scamming amazon”, I definitely would like to avoid scamming amazon’s customers. So, I was wondering what I could possibly do to separate machine-generated erotica for people who get a kick out of the idea of machine-generated erotica from machine-generated erotica designed to be passed off as human-generated erotica until after the sale. Does clear disclosure in the book summary and author summary constitute sufficient cover? Does clear disclosure mean I get banned by Amazon due to their mysterious content policies? I’m enough of a tightwad myself that I’d be mortified if somebody spent $0.99 on an ebook of mine without knowing beforehand that it was 300 pages of algorithmic churn.
Interesting work keeps getting done in the margins, and some gems (like Tingle) appear in the thieves’ quarter, so distinguishing oneself from the thieves in at least intent is very important. But, if you do a thing that scammers do, does disclosing it make you no longer a scammer? And, in whose eyes?
I think often, the people who pay for content farm extruded erotica aren’t themselves being scammed: they got what they expected to get, and considered the gonad-tickling suitable for what they paid; instead, the scammed party is perhaps Amazon (who would prefer to have a better reputation), or the workers on Mechanical Turk (who decided to accept this but maybe should be making at least minimum wage), or nebulous other parties even less directly affected.
I switched to NaNoGenMo because of similar commitment problems.
Pros:
• You will probably produce much more than one novel. (I usually produce one during the first half-hour of November 1st just to get it out of the way, and then work on more interesting / complicated projects afterward) • There’s a pretty active community, with deep and precise discussions of things like structure and themes, because computer generation of long-form narratives that remain interesting to humans over the course of 90+ pages is a hard problem.
Cons:
• You need to know how to code • The novels you generate will be even less likely to be salable than the hurried work of an amateur human novelist • Explaining the concept to people who aren’t familiar with it is even harder than explaining NaNoWriMo, because a lot of people are somehow unaware that computers can write books
In defense of contempt
In defense of contempt
A response to a popular article
The article in question suggests that the habitual tribalism & combative style in communication within the tech community is toxic, particularly to minorities; I do not dispute this point. The article in question also suggests that criticism of languages and technologies should be avoided because it often discourages community diversity, and this is where the author and I part ways.
The state of the programming community is poor, with regard to diversity, and this leads to all sorts of systematic problems that are self-perpetuating. However, the state of the programming ecosystem is also poor, and the perception of acceptability given to bad tooling and bad habits leads to systematic and self-perpetuating problems of its own. The way to increase acceptance of outsiders into the community is not by sacrificing the very worth of the enterprise the community exists to engage in; indeed, it’s entirely unnecessary to do so.
The author decries the tribalism of the community with regard to tooling, but differences of opinion when it comes to preferred tools is not a meaningless aesthetic distinction. The prevalence of overflow-related vulnerabilities in real software ultimately comes down to the popularity of C over Pascal historically; as many exfiltrations of password files and other sensitive data are owed to the use of outdated PHP best practices as can be attributed to SQL injection (and thus, lack of input validation); the poor state of Windows security prior to 2001 when compared to competitors at the time ultimately comes down to the decision to avoid taking full and proper advantage of hardware memory management, setting up a proper system of user privileges, and other common practices in the domain of network-connected multi-user OSes — in other words, Windows was a leaky sieve and prime target for over a decade because lazy habits that were acceptable for single-user isolated machines with no real multitasking were being applied to a huge number of interconnected boxes.
The results of using a poor tool or using a good tool poorly are a lot like the results of ignoring modern medical science: in isolation, they might be acceptable for a handful of people who don’t have it rough, but in aggregate they result in epidemics. Someone who writes ostensibly production-ready code in PHP or Perl should be treated like someone who refuses to vaccinate their children: their behavior should be considered acceptable only if they are extremely careful and they have a very good excuse. Someone who promotes the use of tools that encourage the production of bug-prone insecure code outside the context of isolated personal experiments should be treated the same way we treat antivaxxers: as a perhaps well-meaning but deluded person whose misinformation is resulting in major destruction.
When someone has different aesthetic preferences, it’s natural to accept that. But, when a group that is already marginalized disproportionately adopts a set of tools that are well-known to be destructive and then dedicates enormous resources to the use of those tools, we don’t decide that those tools must be acceptable on aesthetic grounds despite their known destructive potential: we instead try to discourage that group from associating with those tools and figure out what forces are creating that association.
Poor tools are often the domain of beginners, and those who dedicate sufficient time and effort eventually graduate from those poor tools to better tools. (I first learned to program in QBasic.) That time and effort isn’t free, so people who are already under other extra constraints (including people who have extra social or financial pressure) often never move on.
There’s another factor here, however: good tools in some ways often become poor because they become popular with beginners. Most tools are optimized for a small set of problem domains, work acceptably in some others, and work horribly in every other domain. A beginner, having experience with only one tool, will apply this tool to every domain; if problems in some domain are harder to solve with this tool, the beginner, unless properly instructed, will believe the problems in this domain are simply inherently harder to solve. As a tool becomes popular with beginners, experts become difficult to identify in the crowd, and slightly elevated beginners begin to become treated like experts simply because there are many more slightly elevated beginners than experts; these pseudo-experts will popularize poor habits in the community, and these habits beocme associated with the tool itself. An expert who uses many tools will have less say in the community surrounding one tool than the many enthusiastic beginners who are unaware of or reject all other tools. To some degree, the most toxic tribalism is that of beginners who don’t think of programming languages or techniques as tools and identify themselves with their preferred tools.
We should separate criticism of tools based on legitimate concerns from criticism of tools based on tribal or class issues. Plenty of tools can be used well but largely aren’t because most of their devotees are beginners (see: Java, C, C++, Python). Other tools are fundamentally flawed, and while using them well is not impossible, it is a trick that takes a great deal of experience and is beyond the scope of nearly all of its audience (see: PHP, Perl, Javascript). Some tools have lost a great deal of respect because most of their ecosystem is populated by tooling that’s orders of magnitude worse than their original design, compounding flaws (see: Java, Javascript, Ruby). Other tools are perfectly fine for what they were designed to do but are almost always used for things they’re terrible at (see: Perl, Javascript, Lua, TCL). The popularity of a tool with beginners can certainly negatively affect the suitability of that tool in genuine and valid ways if the beginners are given sufficient control over the tool’s later evolution, so it’s not as though a tool’s popularity with beginners is inherently irrelevant, but a good tool can be used well even as most people use it poorly.
There’s another interesting tendency with regard to the popularity of certain tools with beginners, and it’s one that’s wrapped up with institutions and politics. This is the matter of pedagogy. Java is currently extremely popular, but its popularity owes little to its attributes and much to the fact that it has become part of a standard; there is a curriculum surrounding Java focusing on a Java-centric view of object orientation, and this curriculum forms the basis of both the AP Computer Science curriculum in the United States and various certification and accreditation rules for university programs. In other words, if you live in the United States and you are not an autodidact your first programming experience (barring a bootcamp) will probably be in Java, combined with a curriculum that focuses on UML, inheritance, and the details of Java-style encapsulation, while completely ignoring performance concerns and systematically denying that some problems are not easily represented in an object oriented model. Prior to Java, these programs centered on C++, with a similar set of foci. In other words, for several decades, students with no prior programming experience have been taught that there is one language (Java or C++) and one technique (Java-style OO) that is the best at everything, and as they filter into industry they work with other people who went through the same indoctrination and continue to produce huge ugly monoliths of inefficient Java and C++ “enterprise” code. This is the end-game of letting an echo chamber of like-minded beginners dictate the state of an industry.
So, what do I recommend, with regard to the problem of balkanization in tech pushing out minorities?
I consider this really to be an issue of beginners graduating to higher levels of understanding (and systematic pressure making it harder for certain groups to graduate out of the beginner classification), and one way to help this is to be extremely clear in your criticisms about the nature of the problems you criticize — in other words, rather than saying “PHP users are dumb”, say “PHP is a deeply flawed language, and PHP users should be extremely careful when using these particular patterns”.
Another way is to make it clear that using a single language is not acceptable in a professional context: any serious developer has a large toolbox already, and if beginners understood that language preference is not a reasonable basis for long-term tribal divisions because any professional belongs to multiple tribes, the toxic identity-based hostility between programming language communities would mostly go away, allowing concrete and issue-based critiques to become more visible.
Also, seasoned developers who frequently work in many languages and have a deep understanding of the positive and negative aspects of many tool designs should become more vocal about tooling: even-handed discussions about this subject make it easier for beginners to graduate into well-rounded developers and avoid making common mistakes that lead to wide-scale disaster.
Finally, standardized programs for computer science education should include language survey courses earlier and feature them more prominently, while removing some of the pro-OO bias that currently characterizes them: nobody should be able to graduate with a CS degree without being truly competent in at least five or six very different languages, rather than the typical gamut of Java, Javascript, and SQL, and they shouldn’t graduate without non-trivial exposure to twenty or thirty more.
Javascript won’t save the web. Javascript is part of the problem.
The existence of HTML (and any embedded markup) is part of the problem, and generating all elements with Javascript won’t help. CSS didn’t save the web from HTML because the CSS/HTML division misunderstood the problem: content chunking is inherently part of presentation, not content, and any presentation layer should be an external (rather than embedded) markup that can style and rearrange arbitrary spans of content (presumably based on byte or character indices).
HTTP is part of the problem. HTTP doesn’t distinguish between dynamic and static documents, and while it has facilities for representing redirects, moved files, and other potentially useful features, nobody implements or uses any parts of HTTP other than the behavior of codes 200, 404, and (occasionally) 500; even very useful features like partial download requests and file time requests are inconsistently supported.
HTTPS is part of the problem. Hierarchical certificate signing chains will always be vulnerable to leaked top-level certificates, and poor support for certificate revocation will continue to slow adoption of improved hash algorithms.
DNS is part of the problem. The association between hostnames and IPs is only useful from the perspective of a machine (or a programmer thinking at the machine level); the association that is useful to users is one between names and chunks of data, or sometimes between names and services.
Javascript in a web context will never save the web, in the same way that a tumor will never cure cancer. The problems with the web go a whole lot deeper than the front-end concerns that Javascript can address.
If you want to know what *might* save the web, take a look at IPFS & IPNS, then take a look at Project Xanadu.
I for one appreciate your resistance to cutting down your articles.
I for one appreciate your resistance to cutting down your articles. I like to read deep, informative writing, and your work here on medium is some of the best.
There’s a trend toward short (~500 word) articles here; it’s a trend that won’t survive, since it alienates people who want a less casual approach to learning.
Intellectual property is the strongest it’s ever been, more or less worldwide.
Intellectual property is the strongest it’s ever been, more or less worldwide. Large, powerful organizations with enormous IP portfolios are active in the machine learning space: IBM has a huge number of active patents, and Google and Apple are both busy stockpiling them from third party sources. Even if learning models are legally determined unpatentable, that doesn’t keep people from attempting to (and often succeeding at) filing patents for them; the nonexistence of such patents doesn’t prevent any company with lawyers and brass balls from attempting to defend them (and any other nebulous or imaginary claims) in civil court.
Even if, somehow, these extremely powerful forces don’t manage to get their way in terms of ensuring learning models are protected by some form of strong IP (copyright for individual models or patent for novel formulations), and somehow the IP litigation system that has for decades been systematically favoring IP holders and ignoring strong fair use cases reverses tack, and somehow these companies forget that they could make a trade secret claim — in other words, even if somehow our dysfunctionally overpowered IP system suddenly started working properly — learning models are hardly the most common forms of potentially protected work, and they are years away from being capable of producing work of equivalent quality to most protected work. In other words, the end of protection for learning models is insignificant compared to the scale of IP.
Theft of learning models, of course, is both trivial and unprovable. A system intended to produce certain outputs for certain inputs can be trained on the same data or can be trained on API calls; as scale the result is the same, but the innards will be uncomparably different even for a very close match in behavior. Much like other behemoths of tech, the factor that would keep competitors out of the race with machine learning based API services is not the (public) concept or the (trivial and novel, mostly off-the-shelf/open-source) implementation but the cost of scaling to meet demand — anybody can write a facebook knock-off in a weekend but only facebook and a few others can afford the server cost to host facebook’s audience. Similarly, anybody can download tensor flow or torch, but few people can afford the cycles to train it on the entire google books corpus and add new books as they are released.
We don’t call facebook knock-offs (even very close ones, like those used for phishing) copyright infringment and consider them subject for suit, even though they definitely are using image assets against TOS; instead, we treat them as either legitimate attempts at competition doomed to failure or as cheap knock-offs indended to trick us. Likewise, trademark law is rarely applied directly against parasitic industries like that of mockbusters — the legal risk of loss of protection is low, and large film companies are mostly fine with allowing the parasites to continue preying on people with poor vision or damaged faculties of judgement who can’t distinguish between “Transformers” and “Transmorphers”; Universal is happy knowing that Asylum will never be able to compete with them head to head, and once the current generation of executives dies off and is replaced, they will treat internet piracy the same way.
I was once on cleanup duty, in a particularly unfortunate situation.
I got a chance to work for a long-time hero of mine: someone who was influential in computer science circles in the 60s and 70s, who (while fairly non-technical himself) has a lot of ideas that haven’t really been given the chance they deserve. Along with a friend I had roped in, we embarked upon trying to ‘finish’ a ‘prototype’ that had been provided several years before by a self-taught programmer who had never worked on a large project before. He had told the man we were working for that he had gotten it very nearly finished, and it did demo nicely, but he burned out so badly that he ended up (from what I understand) in a mental hospital for a while, and had been out of touch and unable to work on that or any code for several years by the time we saw it.
Initially, the problem was that we couldn’t find any of the code. We were told that it was a combination of C++ and python, wherein python was being used as a plug-in language; we found no C++ or python code in any of the “source” zip files we were given, and while revision control had been provided, this guy never used it except once, right before he quit, to check in all the windows binaries and a whole bunch of screenshots and videos of various prototypes, along with a couple pieces of out of date documentation and a pdf copy of Dive Into Python.
Eventually, after much digging around on the part of various parties who at one time or another had copies of this, we got a large zip file that contained a nested series of smaller zip file copies of the same directory structure with progressively earlier dates. On the third or fourth level down, we found a single C++ file.
We discovered that this single C++ file contained all of the C++ code for the entire project. Nearly all of it was commented out, using line (not block) comments. It wouldn’t build on any platform — it had a bunch of typos and wasn’t actually valid C++; in other words, the only copy we had of the C++ source was a messy, old, in-between version. But, inspecting it, we discovered that it had partial C++ implementations of several functions that were supposedly “done” (such as loading and parsing a proprietary file format), which were mostly disabled. It was difficult to determine which of these were disabled, because there would be several functions with almost the same name that had nearly exact duplicates of the same code, and various caller functions would use various version. Often we discovered that some function that was closest to complete-looking couldn’t possibly work, only to discover later that the only call to the whole chain of operations had been disabled and replaced with some hard-coded value. This single C++ file was several megabytes in size.
Eventually, in order to make it easier to debug, we separated this file into about ten, by categories laid out in this (obsolete) documentation, and determined which blocks of code were definitely not close to functional, removing them. After this streamlining we got the size down quite a bit, but discovered that nearly all of the functionality was missing. Complicated mechanics behind drawing, file parsing, object placement, and structure were nowhere to be found, but it built and worked — on windows, at least. (It had also been sold as cross-platform despite being build in visual studio; it turned out that it was windows-only, but mostly because the author had used a windows-specific sound library to play notification sounds instead of using the one that came with SDL. We quickly fixed that.)
Combing through the code, we discovered that there was a single line early in execution that loaded a hard-coded arbitrarily-named text file (“abiowy2222.txt” or something) as a python script. We found a directory full of strangely named text files, and while some of them were full of junk (copied and pasted pieces of documentation or forum discussions, lists of error messages), about half of the 100+ text files were partially overlapping versions of a big chunk of python code.
It turns out that this text file contained a bunch of python code that performed a bunch of calls back into C++ to perform draw calls on some large chunk of hard-coded data. This programmer hadn’t bothered to write code for loading that ugly file format he designed; he hardcoded the content of the one file he was using for the demo. He had skipped writing the logic for determining layout, and instead had hard-coded the positions for the objects described in this file. And then, this python file had its own main loop and exited at the end of it — in other words, nearly all of the C++ code was entirely disabled.
We endevoured to rewrite pretty much all of this logic, and we did, at least twice. We wrote an actual implementation of the file format loader (and discovered that most of the examples we had were subtly corrupted) and an actual implementation of the layout logic, both in C++. While trying to debug a (semi-independent) module that implemented a kind of non-relational database based on a graph of arrays of pointers, we decided to attempt a pure python implementation, in the hopes that it might be fast enough to be a good comparison. (This original author was obsessed with premature optimization and with using obscure features of C++, and had comments next to each function calculating — typically totally incorrectly — how many bytes per object were being transferred. The database was implemented in an overly complicated way for documented speed-related reasons, but it turned out to be both slower and more bug-prone than the straightforward and naive approach we took in python, across many varied tests.) Having determined that this database in its C++ form was essentially beyond salvaging, we used the python version instead, and spent a great deal of time trying to square the fact that initial draw time was so fast with the extreme slowness of interactivity. We ended up rewriting most of that draw code, before rewriting all of it from scratch in python in a single all-nighter. This all-nighter occurred about two years after we initially started working on this project.
We did this for free, since we were doing it for a mutual hero, but it really put us off the idea of playing the role of code doctor in the future.
The legality of the provenance of the material is irrelevant to the ethics of reporting.
The legality of the provenance of the material is irrelevant to the ethics of reporting. After all, leaks like this are not being submitted directly to outlets: they are being handed to the public. To the extent that journalists have to worry about reporting on leaked material, they have the same concerns about publicizing any other effectively non-secret information: do they do more harm than good by repeating something to a wider audience?
The question becomes more complicated when an outlet receives the content of a leak directly, as happened with the Snowden documents: in that case, the material is still effectively secret, and releases must be carefully vetted, because in a sense the outlet is conspiring with the leaker, and is the party providing damage control.
Reporting on public leaks should be treated the same way as reporting on suicides or other sensitive yet non-secret events: they should be news if they are newsworthy, and they should be reported on in such a way that minimizes damage that might come directly from the manner of reporting rather than from the facts being reported.
To “compete” in web search results is somewhat absurd.
To “compete” in web search results is somewhat absurd. One-off retailers are rightly viewed with suspicion, and making a living off web advertising is something available only to sites at the scale of BoingBoing.
Make good content, ignore monetization, and you may end up with the self-satisfaction of a job well done. Engage with any form of monetization and you lose even that. Good content doesn’t need to appear high in search results, and doesn’t require monetization: if you post things that are of interest and aren’t already on the web, you will attract an audience; break either of those two rules and you are not posting good content but instead are at best acting as an aggregator (and then you’re competing with reddit). If you have an audience and a community, you can get support when you need it.
An Alternate Medium Style Guide
An Alternate Medium Style Guide
I’ve seen several “medium style guides” promoting habits I find irritating. Here’s what you should do instead, if you (for whatever reason) want to attract readers like me.
Web design over time has gotten less and less connected to accessibility & utility, and more and more connected to what might look good in a screenshot. Use of color is but one egregious example.
Of course, low-tech web pages with pure, unstyled HTML (and no formatting tags) continue to work just fine, and continue to load quickly. It seems like, as features become available, designers are using them to optimize for their own experience, not realizing that they should probably be optimizing for screen readers, twenty year old computers connecting over dialup, console-based web browsers without javascript support, and people who need to override styling with their own choice of typefaces, colors, and sizes.
There are real, accessibility-based reasons to make sure all of these things are configurable and the configuration sticks, but there shouldn’t need to be. When a web designer decides that his own sense of aesthetics matters more than the decisions of the user, a false division is created. The web designer should not be the master in this relationship; the web designer serves the user, and should bow to whatever any user prefers, to the extent possible. This is not the way things work now; instead, web designers act as arbiters of taste, with major consequences for large groups of people.
Most websites are essentially unusable for the blind or those with major vision problems — and for those of us with vision within the normal range, these websites are merely irritating. The average size of a website is approaching the size of the original shareware release of DOOM — in other words, if your machine is old, or your connection is slow, most sites are again unusable. A dyslexic person may want to set their font to one of the many fonts designed to change bilateral symmetry in order to improve the ease with which letters can be distinguished; CSS tricks are used to disable user-selected default fonts, and when user-selected fonts happen to show through, the sizing, layout, and behavior of the website is negatively affected because the designer has home-brewed a fragile system for implementing controls and layout rather than using standard widgets and sensible defaults.
A website is not an art project; a website is a piece of machinery that people accept into their lives, like an appliance. Just as nobody would accept a microwave that exploded if you tried to heat up pizza in it rather than baked potatoes, nobody should accept a website that ceases to function when the font is changed. Just as nobody would accept a stool that is nailed to the floor and cannot be repainted, nobody should accept a website that doesn’t respect a user’s color and size preferences. Just as nobody would accept a vaccum cleaner the size of an elephant, nobody should accept a website that takes ten megabytes to serve up 500 words of text.
A false idea of expert difficulty prevents people from demanding these things from the sites they visit. This idea is false not because modern web design isn’t complicated — modern web design is very complicated, and you really do need to be an expert to practice it. Instead, this idea is false because the only things in web design that are really difficult to do are things that should only be done rarely, if at all. The web is optimized for transmission of large chunks of minimally-styled text; using it to simulate native applications, while impressive, is a terrible idea and should never have become normalized.
Ham sandwich for president
Putting Clinton in office is like giving a gun to a police officer: potentially dangerous & problematic in an array of well-understood ways, and best seen as a continuation and extension of a questionable standard.
Putting Trump in office is like giving a gun to a dementia patient: dangerous and unpredictable.
Putting Johnson or Stein in office is like giving a gun to a raccoon: a raccoon has no well-defined concept of what a gun is or does, and doesn’t have any thumbs, and so the level of danger is low but non-zero.
Putting Vermin Supreme in office is like giving a gun to a ham sandwich.
There’s something strange going on with Russian intelligence & the internet security / civil liberties axis
Julian Assange & Wikileaks have been criticized for distributing pro-Trump Russian-generated propaganda by Edward Snowden. Snowden, despite receiving Russian asylum, has been pretty critical of Putin, even as Assange, whose asylum is provided by Equador, has strangely not been. What is going on?
Snowden and Assange are one of these political odd-couples: they have come to the same matrix of behaviors and assumptions from different political corners, and have ended up in the same precarious position by leaking important enough material to get people in positions of power to want to lock them up or kill them (even though ostensibly this is not the proximate cause of Assange’s asylum request). Snowden comes at this from the direction of the kind of right-reactionary libertarianism shared with Eric Raymond and Robert Heinlein — a distrust of government if and when it is the most obvious threat. Assange comes at this from the perspective of left-anarchism, filtered through the beginnings of cryptarchism that Assange lived through. While they have little in common outside a distrust of states and state secrets, they both have a kind of western-style civil-libertarian stance that’s pretty common but directly conflicts with Putin and his administration.
The only thing that all three of these figures (Assange, Snowden, and Putin) have in common is a deep familiarity with spycraft. Snowden was a contractor to two different american espionage agencies; Putin was in the KGB and brought KGB veterans and KGB tactics to his time in office; Assange, in addition to having a particular interest in exposing spycraft and the internal communications of espionage agencies, comes out of a culture of tech-savvy civil libertarians that since its inception in the clipper chip era has had an obsession with spies and espionage.
If there’s a figure that has even less to do with Assange and Snowden, it’s probably Trump. Putin might have an interest in a Trump presidency over a Clinton one only insomuch as the United States, if competently run, can be a major competitor to and impediment in Russian plans, particularly when such plans clash with those of international organizations in which the United States is a powerful member; Clinton, despite her flaws, is an effective politician and bureaucrat, while Trump is not. Perhaps Assange might dislike Clinton — after all, Clinton is of a piece with the current morally questionable state of governance — but Assange is not stupid and neither a North-Korea-like America nor America as a Russian puppet state aligns with his interests. Snowden has reason to worry about Clinton, considering that if he returned to the United States during a Clinton administration it is clear that she would not step in to save him, but Trump’s Nixonesque strongman stance doesn’t bode well for him either; no matter what happens in November, Snowden is unlikely to be able to return home during the next eight years.
The only explanation I can think of for the current strange behavior of these figures is that each has his own plan and believes himself to be using the others for it. Of course, in such a situation, all three plans will probably fail and all three figures merely add to the entropy of the social universe. Nevertheless, it’s very strange to see the civil libertarian axis of the infosec community take a hard right-hand turn and its anarchistic major figures show solidarity with secretive and famously corrupt authoritarians.
With the procedural-OO hybrid, we see it applied in industry a little more often because it’s taught. That said, I think we often use more OO than is justified (I work for a large company and we have a large, essentially monolithic system for loading things from flat files into databases that’s megabytes upon megabytes of java code, most of which consists of less-than-100-line classes with deeply nested inheritance — to perform a task that would be more reliably performed by a single line shell script). When procedural or proper OO is the Right Thing (or, when a procedural-OO hybrid is the Right Thing), we have a leg up on the problem, but until recently most CS program graduates would have very little familiarity with functional or declarative languages.
The tasks I see at work are rarely good matches for OO: I process large streams of data, mostly, so pipes are the appropriate abstraction. The tasks I see in my personal projects sometimes require OO, and other times are a better match for a functional style; only the simplest pipe-component type tasks end up being something I’d want to use a purely procedural style with.
That said, we do the tasks we know how to do, and we solve them with the tools we’re familiar with. Very useful tools of the past have been totally forgotten by the industry: how often do you see a junior dev who knows what PROLOG is, or FORTH, or MUMPS, or tumbler indexing of enfilades? I’m very happy that certain useful tools are becoming much more common (asymmetric key encryption and hashing used outside of the context of communications security, for instance, and combining functional programming with implicit parallel execution), but when tools become subject to fashion everyone suffers.
I don’t deny that there are social norms surrounding the elevation of bad design.
I don’t deny that there are social norms surrounding the elevation of bad design. Of course people who don’t have to eat their own dog food will have worthless and shallow opinions! But, there’s an ethical aspect to design: would you rather improve the world with good design or get paid more for bad design? There’s a didactic aspect to this as well: these semi-competent people in positions of power have the ideas they do because they have internalized trends that were promoted by actual designers or by people who have placed a heavy hand on the fashions of design; since warped ideas about what constitutes good design among designers caused these norms, corrections to these ideas (along with system-level changes like improved diversity and dogfooding) can correct these norms, albeit on something like a twenty year delay.
It takes resources to make a stand, so I can’t recommend everyone do so. But, if you’re a designer and you can afford to reject exceptionally bad norms, I recommend doing so. After all, accepting the awful state of design allows it to improve more slowly.
The ideal chatbot is not a butler or a puppy, but an elder god
The ideal chatbot is not a butler or a puppy, but an elder god
The commercial industry popping up around chatbots worries me.
There have been chatbot communities for a long time. I’ve been involved with several. I love chatbots, and I love chatbot communities: they intersect with experimental writing, performance art, and ‘punk’ communities to a much greater extent than traditional AI communities, and constitute melting pots. Chatbots are interesting to these people because they are a tool for playing with language and identity in an interactive and public way.
Another kind of chatbot community has appeared, only in the past few years. This community bears more resemblance to that surrounding Hacker News than to the other chatbot communities. These are commerce-driven, mostly clueless pseudo-entrepreneurs who heard someone say “bot is the new app” and decided to start writing bots, without looking at the history of the form. As a result, the state of the art in commercial bots looks a lot like it did twenty years ago: it looks like AIML.
I don’t think that this is an accident. Instead, commercial pressures make it impossible to produce interesting bots. Entrepreneur-types talk a lot about “joy”, but when “joy” is only possible in service to commerce it’s necessarily limited. A commercial chatbot must be not only useful but more useful than alternate methods for performing the same tasks — and since most of these bots are essentially text-based front-ends for existing web services, the only way they can get close to the productivity of the existing services is to simulate an inflexible command line interface. A commercial chatbot can contain only pleasant surprises: it cannot confront us with challenging ideas, because challenging ideas are not profitable; since things that are pleasant to some of us are challenging to others, a commercial chatbot is limited in how many of us it is allowed to surprise at all. Being “smart as a puppy” or acting like a butler, in addition to bringing in questions about the culture of servitude that these representations build upon and taking advantage of questionable levels of surveillance in order to implement these features, limit the range of behaviors of the bot to the domain of “cupcake fascism” — leading to situations like Siri telling users not to swear when they request resources about dealing with sexual assult. A bot that is limited to being as conservative as its most conservative user will be little more than a censor in the way of easier-to-use services.
When bots don’t need to be useful in the normal case, that is when they become exceedingly useful in the exceptional case. Bots don’t need to reason the way humans do; bots lack the creative limitations implied by a human consciousness, and while this produces mostly noise, accidental signal has a special value when we find it. Bots, freed from caring about humans, can become alien and impart alien wisdom to us. Such bots can synthesize novelty from vast corpora — this is what bots are good at, and it doesn’t take much human intelligence on the part of a programmer to produce very striking results. Bots can implement dumb ideas endlessly, and by implementing them and making them concrete, change our perspectives.
Yeah. (I haven’t recommended this for McDonalds before, because I think that the case for food is a lot more complicated, but I’ve definitely recommended refusing to write bad code to developers, by the same logic.)
Mr. Rheingold, thank you for reposting all of these.
As an afficianado of computer history, I’m frequently surprised at how little things change, and how often history repeats itself. The field has an amazing institutional forgetfulness; people today are still trying to attack problems that were solved properly by NLS and Xanadu fifty years ago. Ignorance of the history of computer-mediated communication is all too common even among people who are focusing on it, and so we keep making the same mistakes.
Does innovation have anything to do with commerce?
Does innovation have anything to do with commerce? Not any moreso than anything else. Innovative ideas are not necessarily commercially viable, and commercially viable ideas are rarely innovative; the space of commercially viable ideas is small, isolated, and nearly fully mined.
That said, I can’t agree at all with the idea that we live in a particularly innovative time. Most of the businesses that currently get marked as “innovative” are attempts to revive business plans that failed in 1999 (like Uber); the remainder are businesses that produce shoddy copies of the products of their technically superior competitors but make more money because they spend a bigger chunk of their budget on advertising to tell everyone how “innovative” they are than they spend on actual R&D (like Apple).
Genuine innovation cannot be easily productized, and as a result, it isn’t really compatible with consumerist capitalism. At the same time, it isn’t easy to mistake genuine innovation for the kind of imaginary pseudo-innovation that is produced by the con-men who dominate most industries. If you can’t tell the difference, you aren’t looking.
There’s a difference between constructive cynicism and edgelordism.
There’s a difference between constructive cynicism and edgelordism. It’s one thing to have and promote a cynical view in the face of utopianism; to jump onto the tail end of an existing backlash is far less useful.
Black Mirror, while well-made, tends toward repeating media criticisms that were already cliche in the early 90s. When your complaint is twenty-five years old, repeating it without adding anything is of limited utility; Black Mirror doesn’t introduce new and interesting twists to its complaints, and it doesn’t reproduce anything *but* the complaint.
(Now, I don’t hate Black Mirror. It’s like rewatching The Twilight Zone: everything is predictable, but there’s really excellent atmosphere.)
Mirai was appropriately named
Those of you who have been paying attention to the news already know that the major DNS outage last Friday is probably related to Mirai, a piece of malware whose source was released recently. You’ll also know that Mirai targets low-cost internet-connected embedded devices, and that it’s a comically incompetently written piece of code.
The idea that embedded devices would be vulnerable to attacks doesn’t even count as an open secret: the idea that major websites of the near future would be DDoSed by smart fridges was a cliche in some corners of the computer security world in 1999. Prior to around 2010, the dominant term for what we now call the Internet of Things was “ubiquitous computing” — a reference to Phillip K Dick’s novel Ubik, whose description of a group of appliances conspiring to extort and blackmail their human owner, now used as a parody of the “internet of things” concept, actually initially inspired it. The method that Mirai uses to get into these nodes is again an old one, familiar to war-dialers from the BBS era: Mirai iterates through a list of default username and password pairs until it gets a hit. Such lists are easy to find, and have been circulating on the internet since before it was called “the internet”.
In the early 90s, members of a hacking group called the L0pht made a public statement claiming that they could easily take down the internet, and unless security measures improve, someone with less impulse control would eventually do so. They weren’t bluffing; various methods of making the whole internet essentially unusable for long periods of time have been available for decades now, largely unpatched — it turns out that until now, few people have wanted to bother with such blunt instruments.
The general consensus in computer security that the human element is typically the weakest link isn’t without merit: in a competently secured system, humans are the most difficult element to lock down, and exploiting the human biocomputer requires less cleverness than exploiting a computer system. However, even when high stakes are involved, competence is not the norm: until recently, easily broken short PIN codes dominated online banking, and much banking infrastructure still relies upon things like account numbers and SSNs that conflate identification, authentication, and authorization; few systems implemented multi-factor authentication and nearly all systems will waive mutli-factor authentication in the face of a sufficiently convincing phone call; modern security practices have yet to penetrate industries like web dev and embedded systems development, where hardcoded authentication defaults, debugging backdoors, passwords stored as plaintext or unsalted hash, weak xor encryption against some arbitrary byte, and other awful behavior is tolerated or encouraged. We see from leaked documents that even the NSA is engaging in absolute idiocy, using Microsoft Word & its macro system for dealing with confidential documents and allowing those macros to contain commands that interact with the network.
Mirai, created by weebs, is full of in-jokes refering to chan culture and anime. Some people have taken this to mean there’s an association with Anonymous; however, the lesson of Mirai is that an association with Anonymous is totally unnecessary. The situation that Mirai takes advantage of is an old one; the only thing new about it is that even the dumbasses have realized that even a dumbass can take down the internet. The release of Mirai’s source has allowed script kiddies of an even lower skill level than Mirai’s authors to take advantage of the collective ignorance that end users have been allowed to partake in.
The name “mirai” was probably chosen because it sounds cool, but it’s very appropriate. “Mirai” means future, and Mirai is representative of our future: one where you don’t need to be 4chan to take down large chunks of the internet, but can get away with just being the junior high glee club.
There’s a potential link to folie au deux here.
Most of what we as human being believe we accept as the result of social proof, rather than other kinds of proof. In other words, large portions of our mental model of the world are socially constructed: we accept things that our peer group accepts, even if they contradict our experience. Strange and delusional beliefs can easily take hold in social groups that are isolated from the greater population or have no impulse to share the same ontologies as the outside world (say, cults, conspiracy theory groups, and insular societies with enough power to avoid having to kotow to public opinion). The smaller the group, the easier it is to diverge further and further from consensus reality.
The halo effect also feeds into this. A group often feels the need to believe the opposite of whatever its opposing group believes, regardless of the relationship these predicates have to reality; a group will handicap itself in order to show group solidarity and such handicaps often come in the form of a ritual show of spite to some imagined outgroup, sometimes as a clearly absurd belief. The absurdity of the belief is proportional to the strength of group membership. Believing that Obama is a secret muslim is the right-wing equivalent of the Yakuza cutting off their own fingers: it’s a sacrifice that is capable of only symbolic utility but very concrete pain, and so the symbol gains strength from the pain.
If you want a professional quality development machine, buy a 2008 Thinkpad on ebay (cost is about $80) and install Linux on it. It’ll remain perfectly good for the next decade, and perform all your development needs.
OSX is only marginally better for development than Windows is, and a professional should use neither, except for testing and packaging — and don’t you have access to build farms and test farms for that?
Microsoft is no longer actively bad, the way it was under Ballmer.
Microsoft is no longer actively bad, the way it was under Ballmer. That doesn’t make it good, though. There’s forty years of dirty tricks that MS has to make up for just to get back to neutral, and it’ll probably take forty years, for those of us with long memories and a familiarity with history, to decide that this new direction is something more substantial than a PR move or a short-lived utopian optimism.
Package dependency hell is *primarily* a function of unfamiliarity, and it’s not like it doesn’t exist on other platforms. Getting a functional development environment up and running on a Mac is just as hard as getting one up on Linux, so long as you’re OK with typing — the difficulties with homebrew are almost exactly the same because homebrew is basically just a unix-style package system! If you’re trying to get XCode to work properly and you’re used to vim, you’re going to have a lot more trouble with that than you would with installing a full development toolchain (minus IDE, because an IDE isn’t useful or necessary) on another unix.
Maybe you have a fat wallet and your idea of marginal value is different from mine, but I’d rather pay a few hundred dollars less and install my own unix on a machine that will continue to work if I drop it.
This kind of thing has been Apple’s MO since 1982.
What I blame Steve Jobs for, more than anything else in the industry, is a strange and pervasive false minimalism where power and flexibility are removed in favor of one-off features. This starts with the original Macintosh.
The Macintosh team was originally working under Jef Raskin on what at the time was slated to be the Macintosh but eventually became the Swyft, and later the Canon Cat. It was a genuinely innovative design: a device that worked primarily as a word processor, but that had the capacity to perform programming operations in-place in the context of a document, like a combination of literate programming and spreadsheet macros. Steve Jobs, having had become obsessed with the Alto he saw demoed at PARC, had started the Lisa project, and when it became clear that Lisa was going to be a flop (it was expensive and underpowered, totally unusable without an aftermarket hard disk that cost as much as the computer, had no applications to speak of, and cost as much as a new car), Jobs got rid of Raskin and took over the Macintosh project, pushing it away from the original concept and towards being a lower-budget version of the Lisa. As a result, he pushed for lower specs: a last-generation CPU, not very much RAM, no support for TV output or external monitors, and a small monochrome (not greyscale) display. In the end, it debuted alongside the Commodore Amiga 1000 and the Atari ST, both of which had double the RAM, impressive high-resolution color display on standard color televisions, multitasking, and faster current-gen CPUs, at half or a quarter of the price. Somehow, the Macintosh sold better.
During development, Jobs was very wary of the ‘mistakes’ made with the Lisa, and so he forbade the developers from adding new features. Famously, he didn’t allow the hardware team to add any expansion ports; someone secretly added a single expansion port to the board design, however, and the original Macintosh shipped with a board set up such that an intrepid user could solder a connector onto the motherboard and get an expansion port (which couldn’t be used because the OS didn’t have any support for it). Removing expansion ports is, however, typical of Jobs’ attitude here, and it’s typical of his later work at Apple when he came back. (He was forced to resign over the failure of both the Lisa and the Macintosh, and took most of the Macintosh team over to NeXT, which doesn’t seem to have this particular problem somehow!) His idea was that the Macintosh was a computer for “regular people”, who he defined as people who would not only be incapable of using an expansion port but who would be incapable of learning what one is. In the end, plenty of cost on the Macintosh went into fancy beveling and other purely aesthetic aspects of the outer case, advertising, and other marketing concerns. The lesson was: a mediocre product, if it looks pretty and is advertised very well, can become the basis of a successful brand.
The lessons of the original Macintosh became a strange kind of warped “worse is better” ideology that affected Apple only occasionally during Jobs’ 17-year exile and almost constantly after his return. As soon as Jobs returned, he cancelled most ongoing projects, shifted focus from the pretty servicable beige boxes that represented macs in the mid-90s to the technicolor form-over-function blob of the first generation iMac, and ordered that floppy drives be excluded from all future devices (in 1998). Excluding floppy drives probably removed some cost, but margins for Apple products have always been huge in order to allow for big advertising budgets: owning Apple products, even in the early 90s, was as much a conspicuous display of wealth as it was a utility (outside of schools, which had deep discounts on Apple products as another marketing technique). With software developed under Jobs’ reign (like iMovie & iTunes), even the limited flexibility and configuration of earlier mac products mostly went away or became more limited: as part of the marketing push, Apple products were set up to be configurable in only limited ways pre-screened to be in line with the style approved by Apple’s designers, and Apple software was designed to work primarily with Apple-proprietary formats even when open formats existed and were superior (in a mirroring of Microsoft’s extend-embrace-exterminate strategy from the same time period). The unofficial slogan for Apple was “it just works”, with the hidden caveat that “it” was limited to a small set of tasks Apple had chosen to focus on — while Apple made it easy to do the very specific things they wanted people to do, doing anything else was much more difficult than on competing platforms. This kind of shallow marketing-first strategy was very successful, even though most people more or less recognized it.
This strategy continued (even though the UNIX base inherited from NeXT made it difficult to lock-down software extensibility on macs and systems like homebrew became common). The original iPhone wasn’t supposed to have third party apps; later, the app store was added but in a way existed as a way to funnel money into Apple moreso than as a way to actually add third party apps to the platform: Apple retained complete veto rights to apps, had long evaluation periods, required third party developers to pay $100 a year to even be allowed to submit their code, and forced these developers to write everything in a language that, while not strictly Apple-specific, is an obscure early C++ competitor that only every achieved any kind of traction at NeXT and later within Apple for compatibility reasons. Apple used this mechanism to remove: apps that criticized Apple, apps that competed with Apple’s own services, apps that violated Apple’s current design guidelines, and apps that were reviewed by somebody who was in a bad mood that day. The iPhone has no distinction between desktop and application drawer, and has no task-switching capability. The iPod, unlike the mp3 players it initially competed with, only worked with macs, only worked with iTunes, only worked under firewire, and had extremely limited controls and configurability; while some of these limitations were changed later, this was only done as a means of bringing this overpriced luxury object to the masses of people who still ran Windows. Apple laptops started the push toward being thin and light, and removed useful things like disk drives and expansion ports using the excuse that thinness and lightness were legitimate goals; of course, while laptops benefit from not being heavy, after a certain point being functional becomes more important than being lighter, and being thin only means the device is easier to break: Apple had essentially invented an excuse for charging more money the less they actually invested in their device, so that whenever they removed a feature or a piece of hardware they could add both the cost of the hardware and their manufactured percieved value of thinness to the cost of the end product. More recently, Apple has pushed for the removal of both the function keys and the headphone jack, as part of this general push towards smaller and lighter devices.
Now, this would be one thing if Apple could be ignored. My complaint isn’t just that Apple has the kind of abhorrent paternalistic attitude towards its users that used to justify the rush for Africa; after all, individual people and individual companies have all sorts of abhorrent attitudes and ideologies, and they largely don’t directly affect me because I can avoid putting money into those organizations. My big complaint about this is that other hardware and software companies, faced with Apple’s success, have tried to emulate their objectively awful hardware and software design decisions (thus perpetuating this strange design sense that users are children who need to be protected from choosing ugly color schemes by Big Daddy Ive), never realizing that the key to Apple’s success in everything since the end of the Apple II line was to produce mediocre products, sell them for many times their actual value, and spend enormous amounts of money on ad campaigns to convince people that mediocrity is amazing and that even really common things were secretly invented by Apple. In other words, Apple, since the early 80s (but especially since 1997), has been the Kim Jong Il of the tech industry, and their ideas are being gobbled up by lots of people who really should know better.
The existence of function keys on a new-model laptop is really sort of a non-issue in the scheme of things. Apple has been torpedoing third party development in much bigger ways for a long time. Function keys are only used for development by users of IDEs, and the intersection of IDE-using developers who also use brand-new Apple laptops is a group whose situation and opinions in the grand scheme of things shouldn’t matter much. These people are already shooting themselves in the foot paying four or five times what they should be for an ultimately less functional machine than what they’d get if they bought a used Thinkpad on ebay and stuck Linux on it; it’s not the end of the world if they also have to use a crappy mouse to navigate ill-designed XCode menus sometimes. Maybe these people are developing on a mac because they are trying to develop for the iPhone; in that case, their slower rate of development may hasten the death of that device as well, and it would probably be a net good.
Like a lot of people, when Steve Jobs died it made me hopeful. Lots of legitimately smart people work at Apple; maybe one of them would take charge and reverse some of the most damaging policies, the way that many of Microsoft’s worst policies changed after Ballmer got ousted. I expected that after a handful of things that were still in the pipeline from the Jobs era managed to get pushed through, we’d start to see some good decisions: laptops with thick protective cases, metal hinges, and locks to keep them closed; three-button mice; hardware that’s actually up to date; machines that ship with homebrew already installed; iPhones you can run Android on. But, I’ve lost that hope: I now suspect that Apple, like RIM and Oracle, will keep to its current course until it finally screws itself over enough to actually die — and, like Atari and IMSAI before it, will probably become a free-floating brand to be placed on arbitrary hardware based on whoever buys the rights.
While I agree with the sentiment, I think you misplace the blame.
I understand the desire to have more readers. My problem with style guides is not this motivation. Instead, most style guides for medium focus on attracting casual readers and gaming the metrics in ways that actually produces inferior content. In other words, they assume the motivation in writing for medium is to get attention for its own sake & make those numbers go up, rather than clear communication.
For very short content, medium can’t easily distinguish between a ‘read’ and a ‘view’ anyway, so the metric hack of writing short articles is basically meaningless anyway. Likewise, misleading titles lead readers to be tricked & (often, unless the article is of very high quality) to feel tricked.
All of these techniques make sense in an ad-driven system where authors are being paid per-impression or per-read, but while Medium provides these statistics, it does not pay users (and paying publications on Medium typically pay a flat rate). Clickbaity dark-UX tactics are bad enough when they’re paying the bills; there’s no reason to apply them when there’s no money involved & all that’s happening is that readers are having their trust & good will abused.
As an author on Medium, I’d rather write a meaningful essay that takes 2 hours to read and see it get four readers and ten views than write a tweet-length piece of content buffetted by images and affiliate links and get ten thousand reads. As a reader, I’d rather read something substantial than something anemic: Medium’s user interface isn’t optimized for short articles the way Twitter’s is, and every one-paragraph article is a waste of my time and effort, particularly when it doesn’t contain any new information.
As a follow-up, I’d like to reiterate that, as Charlie Stross says, “bigotry is fractal”.
As a follow-up, I’d like to reiterate that, as Charlie Stross says, “bigotry is fractal”. Power relations are complicated because they contain their own inversions (and may run in complicated loops in certain communities), so what constitutes punching “up” versus “down” is often unclear and very environment-dependent.
Satire at its best highlights through emphasis behaviors and systems that are malfunctioning and, if eliminated or repaired, would be a net positive. For instance, consider satires of government corruption in democratic republics: the power relationships are complicated since while elected officials are often wealthier and have more direct power than their average constituent, they are at least theoretically under the thumb of those constituents & both the pressure to remain electable and various anti-corruption restrictions provide the impetus behind corruption in the first place; something like Yes, Prime Minister, by focusing on government, highlights a set of very strange circumstances that can force someone like Prime Minister Hacker to behave in the way we see actual officials behave despite his best efforts, and mostly criticizes the system. Ultimately, every character in that show is flawed, but the person with the greatest claim to ostensible power is the person we in the audience identify most with, because while he is vain and conniving and foolish, he is also naive, optimistic, and genuinely has the best interests of his constituents at heart after a fashion, and each episode shows how his tiniest motions toward exercising his power get quashed beneath the great weight of a government system designed to preserve the status quo. The humor comes from seeing one of the most powerful men in the world reduced to a petulant child by a dysfunctional system and empathizing with him.
Books that should be made into movies, but never ever will
Books that should be made into movies, but never ever will
We Can Build You by Philip K. Dick
Summary: A company that makes electronic instruments (something like a cross between a Mini-Moog and a Melotron, based on the description) decides to branch out into animatronics, and for an anniversary of the civil war, decides to produce autonomous androids designed to look and act like Lincoln and his secretary of state. Our protagonist, a salesman for this company, falls in love with the artist hired to build the faces; the artist starts off being represented as a manic pixie dream girl. (She sleeps with him, decides she doesn’t like him much, and disappears; he doesn’t get the hint.) Since these androids are autonomous and trained on the writings of the figures (along with records of their habits), they have no idea that they are robots, and they proceed to act as though they have been transported through time, leading to a plot where the salesman and robo-Lincoln go on a cross-country road trip looking for the robot version of the Secretary of State, during which robo-Lincoln tries and fails to give romantic advice. In the last scene, our protagonist is drunk in a bar with robo-Lincoln, coming to terms with the fact that he was dumped, while robo-Lincoln sinks into a deep depression and becomes essentially catatonic.
Why it should be made: This would make an excellent counterpoint to modern rom-com fare like Scott Pilgrim, in that it does a good job of subverting the manic pixie dream girl progression: an artistic and damaged woman ends up rejecting the protagonist who is obsessed with her, and that result sticks. By having animatronics with just enough AI to be unpredictable, this ties in thematically with the Westworld franchise, which of course has recently been rebooted to some acclaim. Lincoln biopics had a sudden popularity a few years ago, as well. But, the most interesting part about this story is that it doesn’t do what pretty much every other story about AI does: it never bothers to touch upon the idea of whether or not these machines are “really conscious”. The machines are clearly machines, because our protagonist’s friends built them, and the story doesn’t make them out to be particularly advanced or clever; at the same time, they act like people and are therefore treated like people by our protagonists. When robo-Lincoln goes into a deep depression, nobody questions whether or not the depression is “real”, because of course it’s real: he’s so sad that he can barely move. It’s a book that substitutes the turing test for the eliza effect, and succeeds.
Why it will never get made: Hollywood is really fixated on removing the lumps from PKD adaptations. Outside of the original Total Recall & a couple scenes from Minority Report, PKD adaptations basically reach for a streamlined hollywood ideal of what twelve year olds in 1995 would consider a mind-blowing sci-fi movie. This ignores the kind of fuzzy weirdness PKD embraced in his writing, and which characterized much of the draw of We Can Build You. If you took this and removed the lumps, you’d get a really uninteresting result. For proof of this, compare Do Androids Dream of Electric Sheep (the book written immediately before We Can Build You, and one that is in many ways inferior) to its loose adaptation Blade Runner (which, while iconic for its cinematography and sound design, has removed so many lumps that it’s pretty much the closest thing to a science fiction cliche since Fritz Lang’s Metropolis).
Gun, With Occasional Music by Jonathan Lethem
Summary: A hardboiled detective story in a strange science fiction universe. In a world where the government supplies citizens with memory-loss drugs, uplifted animals form a servant underclass, and police enforce politeness with social credit chips, a private investigator tries to solve a brutal murder.
Why it should be made: A good adaptation will introduce audiences to the true power of science fiction to play with novel ideas. The plot proper is nothing special: it’s frosting on top of the world-building, but because it’s so cliche, it does a great job of leading readers through this world. A proper adaptation would be visually arresting and weird: uplifted animals retain their usual size but talk and walk on two legs; the same technology is being used on the infants of the wealthy, who develop into perpetually drunken and misanthropic superintelligent infants with oversized heads and a pathological inability to avoid making puns. Imagine if Jupiter Rising was actually twice as smart as it thought it was instead of half as smart, and you’re imagining a Gun With Occasional Music adaptation.
Why it will never be made: It’s unclassifiable. The only way we’d ever see this done justice is if Terry Gilliam or Don Coscarelli directed it; even then, with or without producer intervention, it’s an even bet whether this would end up being great (a la Brazil) or a top-heavy flop (a la Jupiter Rising). A lesser director would be tempted to try to play it straight and tone down the lumps. Unlike with We Can Build You, where the lumps are few and big and integral to the plot, Gun With Occasional Music has a million tiny lumps and an easily separable plot proper, but the only reason to bother with it is the lumps.
Snow Crash by Neal Stephenson
Summary: In an anarchocapitalist future where nation states have been replaced with franchises, a cable magnate tries to reintroduce an alien mind-virus to a swarm of refugees, and a master-swordsman journalist races against time to prevent a whale-riding knife-wielding sociopath from distributing it all throughout virtual reality.
Why it should be made: Snow Crash mixes big ideas with the most interesting excesses of VHS-trash spectacle. A proper adaptation would do for the 90s what Quentin Tarantino did for the 70s.
Why it will never be made: Snow Crash deals heavily in exploring weird political and racial dynamics, often explicitly. It uses the conflation of nationality with franchise preference to compare and contrast ethnic identity with branding. To do this, it engages in and plays with racial stereotypes. This is hard to do well, and risky even when it is; movie audiences will pretty much always contain somebody who takes things at face value, and with so much else going on, we’d probably see the same kind of missed-the-point fandom around a Snow Crash movie as we do around Scarface, Fight Club, Watchmen, Goodfellas, and Robocop.
Other things happening in November:
• NaNoGenMo: write a program to procedurally generate a novel in a month. • NaBoMaMo: create 31 twitter bots in November. • ProcJam: write a game using procedural generation in November.
The Lazarus Pose
The Lazarus Pose
February 23, 2025
Two weeks ago, when the first manned ship to Mars exploded, even if nation states wanted to cover it up, such a thing would be impossible. The scale and suddenness of this event exposed the the internet cranks’ claims of international conspiracy as wishful thinking. Much speculation about the evidence caught on radar and sattelite imagery has occurred, but to my knowledge, I am the only one to directly investigate the black squares.
Let me first reiterate the obvious: the Heart of Gold was the largest, fastest space vehicle humans have ever built by a large margin, and took a great deal of time and capital. It was one month into the three month trip. The things that intercepted it were of comparable size, and the time from when the squares in Siberia and the Outback first appeared and impact was only twenty minutes. This was not part of the abandoned Soviet Dead Hand interception system; the Soviets could not have built a device this size.
I got lucky: I was on Twitter when the news hit. I immediately booked a red-eye flight to Siberia; after all, the Australian square would be much harder to travel to. I bought every drone for sale in the airport mall.
The Siberian square, like the Australian one, was about a mile across. When it opened, it caused a building above it to collapse. I imposed upon the landlord of this building; luckily, there were no tenants at the time, though the landlord suspects that some squatters may have been lost.
With a hole a mile across, it’s reasonable to expect quite a bit of depth. The scale of the things launched from here gave me a bit of an estimate: at least 800 feet. I modified the drone firmware so that each drone could act as a signal repeater for the next one in the chain; I also added a ping function as a means of estimating depth, although with slow microcontrollers like those in the drones there’s a margin of error of about 2%.
My daisy chain of drones demonstrated that the square extended straight down about 200 feet, with some indication of shear stress on the edges for the first 25 feet below the rockhead. After 200 feet, the space opened up: my drone’s cameras, radar, and sonar all couldn’t find edges. There wasn’t much to see, other than a lot of dust. About a hundred feet lower, some debris (presumably from the collapsed buildings) sat atop a blunt cone made of a material that resembles smooth concrete. I had my lead drone land on the cone to get a closer look.
From the perspective of the tip of the cone, I could see a vast grid of similar cones in all directions. However, the lead cone (and others) began to move; tremors made the building I was staying in become unstable, and I had to flee. Unfortunately, my recordings were lost when the building collapsed on top of my computer, during the square’s closure.
From public sattelite imagery, it seems that the other square closed during the same two hour window; I suspect that it actually closed simultaneously.
With regard to the mystery of the squares and the tragic demise of our Martian colonists, it is my position that history has repeated itself. The Soviet dead hand system accidentally recapitulated, in some small way, an antedeluvian drama that once occurred between Earth and Mars, long before the age of man. We have finally reached the level of technological development necessary to become pitted against our forebears, who while not human were also people of Earth. However, their defenses are far beyond what we can reasonably wish to escape. How long will we be trapped on this planet by the nervous twitches of a long-dead race? The thousands of cyclopean missiles beneath the Siberian tundra are our formidable jailers, along with similar stockpiles who knows where else. We cannot leave the cradle of earth until we outwit them; but, even something as simple as the door mechanism for this defense system is centuries beyond our technology, and the very existence of a race who could build such things is beyond the current reach of our archaeology.
Who were the figures in this ancient drama? We have been presented with a mystery whose clues will be inaccessible for the forseeable future.
Yeah. It’s useful as a rule of thumb for who you should probably be extra careful not to mock for no good reason, but satire falls into the category of legitimate criticism layered in humor. Nobody should be immune to legitimate criticism, and nobody should be mocked for no good reason, so as a rule it’s only useful when you’re already failing to follow other, more important rules.
I don’t think the idea of calculating punch vectors — i.e., trying to figure out power relationships — is not worthwhile, though. Systematic inequality tends to be easy to ignore, so encouraging people to look for it is important. But the thing about legitimate criticism is that it’s not an attack: the target of criticism benefits from understanding and accepting it, by definition. In other words, calculating power relationships should probably not be a part of determining who to criticize (although it may be part of determining how to criticize them), because criticism is a positive-sum game.
Early attempts at implementing transcopyright (essentially, a system by which quote attribution is treated as a mechanism of remixing & is built-into the editing and publishing system with fees, the idea being that long discussion threads could become a kind of economy where tiny amounts of money are transferred) such as TokenWord failed because there was no community, and other systems that attempted to adopt some of the ideas of transcopyright didn’t go all-in and thus ended up falling back on their alternative revenue systems in lieu of actually explaining the mechanics of transcopyright to users. However, Medium seems like an almost ideal environment for a transcopyright-type system: long conversation threads emerge around long original posts, most posts aspire to something resembling “journalism proper” rather than the kind of blog-like or social-media-like content that dominates on Tumblr or similar, and many old-school publications have moved to Medium and integrated with its systems. Not only that, but Medium already has a licensing menu & complicated licensing mechanisms. In other words, if Medium were to build a mechanism for pasting highlights into a new post while making them link to the original context & hook that system into a default license that explicitly allows these highlighted sections to be put into a new post in exchange for redeemable credits, Medium would immediately have a transcopyright-style system (although they would need to hook into some sort of payment framework in order to allow people to put money in and take it out; probably, they’d have to set aside a bit of money for people to start off with, so that users wouldn’t need to pay something in first).
Ads don’t pay much at all. Any replacement for ad-supported systems would need to let people pay as much as ads would, and were that payment truly automatic many people would: we’re talking a fraction of a cent for hundreds of views. (I subscribe to Google Contributor, which basically turns Google’s own ad system into a micropayment system: I rarely see ads, but every site gets the kind of money it would if I saw every ad. The problem is that for certain ads, Contributor is disabled, and these are the most irritating ones, so I still have to use ad blockers on desktop — only sites I view on my phone end up going through Contributor in the end. However, if Google were to charge me $4 instead of $3 and properly block *all* the ads, I’d happily turn off ad blocker and pay the money.) The ad ecosystem wastes money by involving a bunch of third parties; direct automatic micropayments to all involved web hosts is better, because anybody calculating how much showing me an ad is worth to them is drastically overestimating: I’m never going to buy something by clicking through a pop-up ad because it’s insecure, so they’re never going to make that fraction of a cent off me that they paid the web host, and as soon as they realize how many people out there think the same way, ad impressions will be worth even less.
I’ve never believed that clickbait had anything to do with audience preferences.
I’ve never believed that clickbait had anything to do with audience preferences. The popularity of longreads shows that people would well and truly prefer to read proper, in-depth journalism.
The thing is, low-content reporting (short listicles that are mostly images, 500-word “articles” that are mostly pull quotes from other people’s coverage, reposted press releases) is super cheap and easily outsourced to content farms. If you’re being paid by the click and you can’t easily maximize the clicks (because you’re in a red queen’s race) then you can at least make sure you’re spending as little money as possible on each ad-hosting page of “content”.
On communities
An extension & refinement of the MOP theory.
The function of any given community is social noise reduction: a community allows groups of people with certain sets of attributes to access each other with fewer “misses” — i.e., without accidentally interacting with a person lacking any of these attributes. (Whether or not this is a good thing depends pretty heavily on the circumstances; however, ultimately, a country club, a Bilderberg conference, a birdwatching forum, and a hackerspace all have in common this basic definition.) Gatekeeping is therefore a huge part of the function of the community.
In a sense, the gatekeeping rules of any community define that community. After all, gatekeeping rules ideally act as a test to ensure that members approximate the ideal community member, but functionally, gatekeeping rules specify who gets into the community and therefore what the range of attributes are of community members. Gatekeeping rules are rarely explicit, and many are inherently implicit: after all, community norms, tolerance for particular community members, and desire to be part of the community all have major gatekeeping ramifications despite never being written or systematically enforced.
Communities are not the result of people coming together so much as they are the result of people seeking a temporary separation by some arbitrary category. They happen naturally, as people congregate toward other people who can fulfill needs and desires. When a community becomes diluted such that different sub-groups within it have different needs, it fractures into multiple communities, whether or not anybody recognizes or points this out for any particular group. Community fracture is always present and fractal to some degree: cliques appear within any community among people who get along better together, and this is part of the community fracture gradient and caused by the same impulse toward sorting. However, all of these organizations are temporary and desire-driven: someone who ceases to be interested in hot rods has left the hot rod community, and communities don’t organize along racial lines in the absence of racial tension. The rich and powerful form cliques among themselves because they share attributes that are only accessible to them (nasty gossip about foreign dignitaries isn’t available to people who aren’t presidents; discussion about how to prevent your kids from murdering you for your inheritance isn’t useful for people who have no estate), and pooling power is a side effect rather than a goal.
Prior to complete fracture, a cultural change within a community changes the community’s ideals. When somebody leaves a community and calls it “dead” — it’s not dead, but it no longer serves the purpose that person needed it for. Communities are tools for solving particular social/interpersonal problems (usually, a desire to communicate about a particular subject), and when community norms change, it’s as though someone’s screwdriver has been replaced with a hammer.
The MOP theory explains a very specific special case of this phenomenon — one that happens pretty frequently, particularly in “geek” circles. Specifically, it explains what happens when an affinity group organized around a particular subject becomes subverted for the sake of commercial interests. (When we talk about this in terms of Punk, or Star Wars, or comic books, we’re really talking about a set of established commercial interests that had already taken over partial control of a social group being subverted by another, larger and more powerful set of commercial interests with different norms.) Whether or not this is a good thing basically comes down to which norms align more closely with the ones you value. While typically the group that takes over has values closer to the notional “mainstream”, it’s not as though communities haven’t been taken over by commercial interests that are even more fringe — as with Palmer-Luckey-funded alt-right trolls infiltrating 4chan. That said, the typical pattern is as mentioned in the MOP theory: a community with a focus on detail and quality has its gatekeeping process subverted by a second group whose focus is on commerce, in such a way that an influx of less-dedicated members become involved; the average dedication level of the group plummets while the most dedicated group members leave the community to form their own. Without the most dedicated members, the infrastructure involved in gatekeeping and in keeping conversations going disappears, leading to most of the casual members also leaving, but enough money has been extracted from the group by commerce-centric outsiders to make this tactic a success.
When a community that has been infiltrated becomes self-sustaining (usually because the new gatekeeping mechanism is sufficiently exclusive that most members still find something valuable in each other’s company), it’s essentially a new community with new norms: obviously most of the old guard will find this new community less to their liking than the old one, because the old community was based more closely on their own desires and values. It also becomes vulnerable to being infiltrated, split, or subverted by some other commerce-centric group. Whenever a commerce-centric group infiltrates a detail-centric group, the group norms become more lax, because commerce works best at scale and details work best with strong gatekeeping.
While toxic community norms (as mentioned on the Status 451 piece) can become part of gatekeeping, they are rarely truly valuable as such. Communities with toxic norms can become stable so long as they consist primarily of people who can’t easily split off. Having been a long-time member of several autism-related internet communities, I can verify that schism doesn’t take a huge amount of emotional intelligence or social capital; more typically, toxic norms dominate in groups where confidence in one’s ability to split off is low & desire to avoid a toxic community is low — in other words, toxic communities are a result of learned helplesness, not a calculated tactic.
Stories I won’t write (but you can)
Stories I won’t write (but you can)
This is a list of pitches for stories I won’t write. You absolutely may write any of these. The full list (which gets updated occasionally) is here.
Clearly, the only thing that can defeat Trump now is the veneration by chaotes of an even older deity. (I’m thinking maybe Innana, patron goddess of love, war, prophecy, the impossible, and fashion.)
The idea that major religious figures are ambassadors from outer space goes back at least to Theosophy, with its idea of “secret chiefs”; Theosophy was extremely influential, particularly on the ideas of other syncretic religious groups, so it’s not a surprise that the Aetherius society has adopted this idea (just as the Raelians have). It’s also not surprising that it made its way into pop culture, since Theosophy was basically the Spiritualism of the 1920s: a minor religion with very influential followers that captured the zeitgeist & thus had an overwhelming effect on popular discourse.
News and Lies 1: In Defense of (some) Propaganda
[1]
There’s been a lot of blow-back regarding fake news — which is to say, fiction using the style of news stories and disguised as news stories — since Trump’s election. The general premise is that false news stories circulated among the communities dominated by Trump supporters bolstered their support of him, and of course social media was leveraged to an almost unprecedented degree by his campaign.
“Our biggest incubator that allowed us to generate that money was Facebook,” says Parscale, who has been working for the campaign since before Trump officially announced his candidacy a year and a half ago. Over the course of the election cycle, Trump’s campaign funneled $90 million to Parscale’s San Antonio-based firm, most of which went toward digital advertising. And Parscale says more of that ad money went to Facebook than to any other platform.
“Facebook and Twitter were the reason we won this thing,” he says. “Twitter for Mr. Trump. And Facebook for fundraising.”
At the same time, even politically-charged fake news has a powerful capability to aid us in the pursuit of truth. This seems paradoxical but isn’t necessarily so, as I’ll explore.
We should not ignore this history, but we should analyse what makes the standard discordian fake news different from the stuff that people are currently concerned about.
I should clarify that I will not be focusing on the distinction between satirical fake news and fake news made to be believed. Satire is of great value — there’s not just a cultural or propaganda value to satire but also a concrete procedural value to it — but we don’t solve all our problems by clearly indicating that certain stories or sites are fictional or satirical, and there’s a lot of good that can come out of fake news that isn’t clearly satire. (Furthermore, this doesn’t move us away from the problem of people sharing it uncritically and believing it, as anyone whose grandmother has forwarded them chain emails about Onion stories can attest.)
The history of using fake news for explicitly political purposes goes back a long way, but the current state of the art in this domain can probably be attributed to Paul Linebarger’s book Psychological Warfare — a description of US propaganda activities during the second world war, used as a training manual for the US army.
Linebarger’s advice is fairly straightforward, and forms the basis of what outlets like RT and Breitbart do: take true statements out of context, mix in fiction that the target audience either wants to believe or wants to fear, and construct the story in such a way that the target audience is led to a particular conclusion. Linebarger gives examples of descriptions of the good treatment of POWs — an enticement for soldiers, unhappy in the field, to surrender — and other stories suggesting that a group of soldiers allies are sex-crazed or have abnormal sexual prowess — an enticement for soldiers to desert their posts in order to safeguard their wives and girlfriends at home, and an encouragement to further distrust wartime allies whose history with one’s culture is more complicated.
The goal of this kind of work, which we might call propaganda news stories, is for the target to actually believe the stories and come to the conclusions suggested. Linebarger suggests that the goal of such stories should be to convince enemy soldiers to surrender or desert their posts, so that war can be ended with a minimum of bloodshed, and that to bolster the effectiveness of this kind of propaganda, all of the things that can benefit the enemy soldiers once they follow the suggestions provided by these stories should be made true — in other words, one should suggest that POWs are well treated and then actually treat the POWs well, even if the POWs are treated better than the neighbouring citizens.
There’s another form of fake news, of about the same vintage, that would specifically be coming out of the intelligence community: the double cross. Knowing that a party is listening but skeptical, one can produce fake news or fake documents that are a mix of truth and fiction produced in such a way that the end goal is to produce confusion and greater skepticism — to waste the time of the opposition. A good example is the idea that carrots promote night vision — which was, in fact, a story that was circulated locally in Britain to simultaneously hide the existence of advances in radar and radar detection systems and push the Axis powers into a series of ill-fated and expensive attempts to improve soldier eyesight with dietary changes. Another good example is “red mercury”, a fictional chemical that was mentioned in leaked nuclear weapon plans — these plans looked fairly convincing, but anyone working off them would spend a great deal of time looking for the imaginary red mercury, resulting in a delay in nuclear weapon development.
Linebarger’s propaganda approach is fairly straightforward: if people believe the news story, they can be controlled; if they don’t, they can’t be controlled. The trick is to compose stories that people want to believe — and it’s no trick at all, if you have a sufficiently nuanced understanding of the culture and history of the target group. While positive uses are possible here — the historical use of these stories to convince Nazis to surrender or go AWOL would be seen as positive by most — this tool is basically totally dependent upon the intent of its wielder, and it can be turned against anybody in a rather uninteresting way.
The double-cross is more interesting, because it deals directly with the idea of a nuanced, skeptical, sophisticated audience. It’s less predictable in its concrete results, but an environment saturated in double-cross media is an environment that is extremely resistant to Linebarger-style propaganda. Anybody with a vested interest in determining truth, when dealing persistently with a consistently yet not systematically unreliable source of data, will develop a habit of skepticism and reflexive self-awareness that borders on paranoia, and such a habit is extremely useful when actually isolating truth from fiction.
There’s a third kind of fake news, which I’ll call the single-source double-cross. This is the kind of propaganda produced by long-running state media that are required to toe the party-line — think Pravda, or North Korean press releases. Like Linebarger-style propaganda, it has a clear goal which can be trivially determined by the target population if they’re inclined to look critically. Like the double-cross, it is created with an awareness that nobody will take it at face value.
However, the major difference is that it comes without opposition — anything it says, true or not, will be backed up by any other media released, because all of the media are required to adhere to the same set of rules. Knowing that what the media says is in some places false, but at the same time having no alternative view provided, we cannot produce a consistent view other than the official one and risk engaging in a kind of learned helplessness.
Everyone in Russia in the early 1980s knew that the managers and technocrats in charge of the economy were using that absurdity to loot the system and enrich themselves. The politicians were unable to do anything because they were in the thrall of the economic theory, and thus of the corrupt technocrats. And above all no-one in the political class could imagine any alternative future.
In the face of this most Soviet people turned away from politics and any form of engagement with society and lived day by day in a world that they knew was absurd, trapped by the lack of a vision of any other way. — “THE YEARS OF STAGNATION AND THE POODLES OF POWER,” Adam Curtis
Some of the coverage of filter bubbles with regard to fake news has focused on how self-segregation has allowed Linebarger-style propaganda to become extremely effective in certain communities, which is true.
Other coverage instead focuses on the idea of “post-truth” politics, which is essentially a combination of the double-cross and the single-source double-cross: when we self-segregate, we can turn the regular double-cross into a single-source double-cross by only sharing stories we find agreeable, even if we know that they aren’t true. It’s important to note that the filter bubble isn’t some recent invention, or some conspiracy by tech companies. When we don’t think clearly and critically, we have a tendency to ignore information we don’t like and hoard information we do. When we’re provided with the ability to share a stream of information of unknown veracity with a group of peers — chosen via some kind of semi-mutual consent — we self-segregate based on who shares information they like.
But, where a double-cross media landscape promotes a sophisticated, skeptical, cosmopolitan approach, the aggressive filtering of dissent provided by social feedback producing a single-source double-cross landscape promotes a nihilistic approach: “everything is fake, and I don’t know what to believe, so I’m not even going to try to learn anything”.
In other words, if you expose yourself to a media mix that is both appealing and unappealing, apparently true and false, you’ll end up becoming a more effective and discerning thinker, while if you filter that same spew by appealingness you will end up a less effective and less discerning thinker.
People like Joey Skaggs — and others who take a discordian-inspired culture-jamming approach — are attempting to improve critical thinking via fake news. For instance, Cat House for Dogs,
The phone rang off the hook as hundreds of people called to talk to New York’s first and only dog pimp. Surprisingly, not only were the calls from bonifide customers willing to pay $50, but there were just as many calls from people who wanted to have sex with dogs or watch dogs have sex with other people. Dog pimp, Skaggs, recorded all of these incoming phone calls.
When contacted by the news media, Skaggs got together 25 actors and 15 dogs and staged an elaborate performance in a SoHo loft — a night in a bordello for dogs. The performance featured models posing with female dogs in look-a-like outfits, and actors posing with the male dogs waiting to view the bitches. Friend Tony Barsha played a bogus veterinarian on site who, when interviewed, explained that the female dogs were injected with a drug called Estro-dial to artificially induce a state of heat. If a bitch was already in a natural state of heat, she would be given a contraceptive called Ova-ban, so there would be no fear of fatherhood.
The intent here is clearly different than those who want to use fake news to manipulate you into voting a certain way, or people who want to wear you down to the point where you’re unwilling to even complain.
Where Linebarger-style propaganda tells you what to believe and satire tells you that other people’s beliefs are silly, culture jamming of this type encourages you to question your beliefs by making you consider believing something silly. And in terms of ill intent, the worst that can be accused if such efforts is that they cast shade on the legitimacy of press outlets that carry the story without being in on the joke. There’s no harm to be rendered by that in light of the Post Factual, and plenty to gain.
Surrealism is the key. Surrealism will shock your mind of its track. Surrealism can shut your mind down for a fraction of a second, allowing you to experience the world for just a moment uncensored. — Operation Mindfuck
Part Two Here
Considering that the root cause is the essential incompatibility between optimizing for truth and optimizing for commercial success, I don’t see fake news ever going away or being seriously addressed. People will continue to pay for spurious information that matches their biases, and people will continue to generate spurious information that matches the biases of people willing to pay for it, for as long as money continues to exist.
I develop quite a bit of open source software.
I develop quite a bit of open source software. The reason is that I develop software for my own use, and because I am not being paid for that software and never will be, I might as well make it available to the occasional other developer who may want to use it in the far future. Since I’m only developing software to scratch my own itches, there’s no real possibility of fame or fortune coming out of it: nobody has bothered to write this code before, and I didn’t spend a huge amount of effort on it, so clearly demand must be low.
I think this is probably closer to the norm for open source projects. Most open source projects are maintained by one person, used by two or three people total, and are short scripts (less than ten thousand lines of code, in some scripting language) either written over a period of a few days or written a few lines at a time over the course of a decade. These projects become open source because there’s no potential upside to closing them off and no potential downside to opening them up: end users can’t even conceptualize the tasks that these projects perform, because they are the type of developer tool that only the rare developer will even consider worth using, so feature requests will never appear.
An alternate web design style guide
An alternate web design style guide
1. Don’t use CSS or Javascript. These technologies exist to help a web browser poorly simulate a native app; if your goal is to simulate a native app, you’re better off just writing one. 2. Web sites are, ideally, static hand-written HTML. Sometimes, it makes sense to write plain text and then write a short shell script to convert to static HTML. You don’t need a database, or server side scripting; if you do, write a native app. 3. Don’t specify fonts or colors. Browsers provide the capability for users to configure default fonts and colors; if the user prefers something other than their current default, trust them to change it. 4. Don’t specify alignment, except with respect to table elements. Eschew purely aesthetic distractions like page breaks, drop quotes, and tenuously related images. The user came here to read: let them. 5. One document per page, please. If you write a whole book, the whole book should be on that page. 6. Eschew sidebars. If you need a navigation menu, a top or bottom bar is fine: it’s easier to implement, and doesn’t interfere with the flow of text. 7. Use only the following tags: a, b, body, br, center, h1, head, i, li, ol, p, table, th, title, td, tr, ul. All other tags are unnecessary distractions. If, for some reason, you must include images, the img and align tags are also suitable. 8. Embedded markup is, within the context of the web, a necessary evil: as it stands now, without it hyperlinks aren’t possible within a web browser. Use it as little as possible, and make sure that it can be easily eliminated so that when external markup becomes widely available the transition is easier. 9. Any web page should be written so that it is usable and understandable with a text-only browser that does not preserve spacing. In other words, text formatting, images, and position on the page should never contain essential information. If you cannot strip all tags and get a nearly equivalent experience from reading non-tag content as a contiguous stream of text, you have failed to make an accessible web page.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Soylent definitely wins, when the choices are between Soylent, a candy bar, and nothing at all.
Soylent definitely wins, when the choices are between Soylent, a candy bar, and nothing at all. I often have a hard time getting through to people who resist the idea of meal replacement fluid for cultural reasons, since these people typically have never felt the complete loss of drive that makes me resistant to even microwave meals. (In my case, it’s not depression so much as a mix of sub-clinical mania & some executive function issues, but the result is the same: I’ll go days without eating properly, subsisting on chocolate or microwaved hot dogs or something, and if I’ve got something like Soylent then the absolute least-effort thing to eat is also the healthiest thing in the house.)
There seems to be the assumption that “fake news” shouldn’t be consumed.
There seems to be the assumption that “fake news” shouldn’t be consumed. I disagree entirely. It’s fine to have a list of sites that have mostly “fake” news, because you should be critical of (and, preferably, perform some fact-checking on) everything you read, and having a list of sources for which you have reason to treat with more skepticism is probably a good idea — particularly since recycling stories without fact-checking is a common behavior among low-budget outlets, so you should always be backing up anything with some source that are not on the list.
(The Daily Dot does some quality journalism; I’ve been reading their stuff for several years now. They also occasionally publish pieces that are shallow or subtly incorrect — but so does the New York Times & Wired.)
You shouldn’t avoid fake news. In fact, you should go out of your way to read fake news. But, you should also go out of your way not to trust anything — especially things you already agree with.
Monochrome Vertex Ch. 1
Monochrome Vertex Ch. 1
A few years ago, I started writing a story set in a world where the GUI was never invented — a world with cultural norms and economic pressures shaped by different technological trends. I never finished it, but I will post the first 2 parts here.
Chapter 2
The scant light through the venetian blinds gleamed against the machine on the table. It was a luxury model produced by Texas Instruments, but clearly based on the GRiD and QL in industrial design: a sharp-edged clamshell, it now resembled a set of precision-machined wedges, but closed it would resemble nothing more so than a small steel briefcase. It seemed vaguely out of place on the expanse of the dark mahogany desk. Behind the VP sat an equally expensive dark mahogany bookshelf, one shelf dominated by a sparse row of carts, each probably a whole gigabyte (though it is inconceivable that the VP could have so much to store) and designed to look like extremely short leather-bound books. The other shelves were full of non-solid-state media: tapes, magnetic disks, and even a few optical disks.
“Could you give me the condensed version, Mr Logan?” The VP looked at the printed report as though it was a rotting snail.
Dex kept himself from sighing. “Well, your network security is good.” The VP nodded solemnly, and Dex suppressed a smile. “But, that doesn’t really matter because your personnel training sucks. I tailgated into the building and bluffed my way into the air-gapped network.”
“And the files?”
“The evidence has been uploaded to a public FTP in the Caucus Republic. The hostname is in the report.” Dex stood for a while in silence.
The VP made chewing movements with his mouth. Nervous habit? “I’m not paying you by check,” he said finally. “The agreed-upon sum has been wired to your account as of this morning.”
“That is highly ir-”
“Goodbye, Mr Logan.”
Dex took the hint.
–
“Three fifty, please.” The young woman seemed pleasant. She must be new.
“Motherfucker.”
“I’m sorry?”
Dex swallowed. “Sorry, not you. Here,” he handed her a twenty. “Keep the change.” He pulled off his pager and dropped it in the trash can on the way out.
Turning his collar up, Dex crossed the street and then turned sharply into the subway stairwell. He used his Metro card to reserve a spot for Coney Island, then walked to a neighboring counter and bought a ticket to Grand Central. He slipped the Coney Island ticket into the crack between two empty bricks in the tunnel, then took the next trip to Grand Central. From there, he took the train to Albany.
Monochrome Vertex Chapter 2
Monochrome Vertex Chapter 2
Chapter 1 is here
Dex was already ducking under the fence by the time he saw the sentry. It was an Arctek machine, top of the line perhaps half a decade ago, subtly modified and equipped with a sonic stunning device (nonfatal, but exceedingly unpleasant — developed for crowd control by the Argentinian equivalent of DARPA). “Identify yourself,” it said.
“Dexter Logan. I’m here to see Tom.”
The robot buzzed quietly for a moment, and Dex noticed that the Cicadas were out. “Dex!” The tone was familiar — a synthesized simulacrum.
“Tom? Quit fucking around and call off the bot.”
The robot swung its sonar apparatus smoothly back and forth, in a crude mimicry of a person shaking its head. “I’m not fuckin’ with you, Dex. It’s been a while. Ten minutes?”
“Eleven,” Dex said.
“Eleven candles candles,” the robot said. “Candles candles candles candles candles candles. Candles eleven of them.”
“Fuck.” Tom’s a broken robot, thought Dex. Just then, a figure surfaced from within the long grass.
“Dex, my man. Come with me.”
Dex followed Tom, and the robot trailed a few feet behind, still stuck in its verbal loop. “Some kind of fluke in my model traversal,” Tom said. “Fucker works just fine until suddenly it decides some word or phrase links to itself with a probability of one. Sometimes it matches my patterns enough to get registered as me, and then I have to go in and delete the bullshit from the database by hand.”
The farmhouse was small and old, but it offered shade from the unseasonably warm weather. The table was piled chest-high with second-hand junk. Scavenged robot chassis sat under it, sometimes on the floor and sometimes on chairs. The shaded lamp hanging from the ceiling had its power cable extracted, cut, joined with a pair of plastic caps, and cut again further down. Tom brought in a pair of stools from an adjoining room.
“It’s been a while. What, ten minutes?”
“Eleven.”
“Eleven. What can I do for you?” He glanced at the cupboard. “A beer, maybe?”
“I’m afraid this isn’t a social visit. I need your help.”
Tom got up abruptly. “I think I’m gonna need a beer, then.” He found a reasonably clean glass and cracked open a can of lukewarm beer, then settled down and took a long swig out of the can, not bothering to pour. “Okay, shoot.”
“I got fucked over, Tom.” Dex sighed.
“Infringement, again?”
“Fuck no.” He grimaced. “Five minutes for that shit, and I wasn’t even doing it — just running the BBS. Staying straight now. Pen testing. Figured that was the way to go — using my skills but no chance of getting locked up.”
“So?”
“So, I think maybe I’ll need that beer after all.” While Tom got up, Dex took another look around. There were cameras and microphones everywhere. Through the crack in a half-opened door he glimpsed a bank of closed circuit television monitors. “You recording this shit, Tom?”
“I record everything. Turns out that’s the only way to get believable responses.” He put a much cleaner glass down on an empty spot on the table and poured an equally warm beer into it. Dex looked at it as though it might contain more cameras.
“I did pen testing yesterday for Tim Dolby.”
“I know. There’s more.” Dex paused. “Whatever scam he’s pulling, he had me steal some files for him and fence them overseas.”
“What files?”
“I don’t know, man.” Dex took a swig of his beer. He got mostly hot foam. “I just know the filenames, and those were UUID. Base 64 shit.”
Tom began fidgeting with his pen. A few minutes as cell mate had left Dex with an understanding of this man’s nervous habits, and he felt apprehension.
“I covered my tracks as Most Unexceptional I could.”
“Still–” he took a deep breath and calmed slightly. “Where overseas?”
“The Caucuses.”
Tom sighed. “You just got fuckin’ lucky, man.” Dex stared blankly. “What, haven’t been watching the news?” He took a device out of his pocket and flicked on a small CRT hidden amongst the bric-a-brac on the far side of the room.
I don’t have any problem with self-help articles per-se, but I definitely have a problem with unoriginal content — and none of these articles told me anything I hadn’t heard twenty years ago. (I am twenty-eight, and I don’t think I was an especially well-read eight year old.)
Tell me something I’ve never heard before. I don’t care if it’s stupidly wrong. In fact, maybe it’s better if it’s wrong.
The problem of a lack of free public indoor spaces isn’t limited to New York City; I live in suburban Connecticut, and (barring public libraries, and the occasional bus shelter or other wall-less structure on public land) indoor spaces accessible to the public are either retail spaces or member-supported ‘clubs’ with limited access (often, non-members are allowed in only by invitation or are only allowed during particular time periods).
I get the impression that in the past (say, prior to 1950) public spaces were more common and more vital, with community organizations like clubs, churches, and lodges being less closed-off simply because socialization and other forms of entertainment for those without any money was necessary. Salon parties & coffee houses also catered to this use case, and it seems like coffee houses would cultivate social groups and bring in events like scientific demonstrations & political speeches during the nineteenth century, as a means of drawing a crowd some of whom would purchase coffee, the same way that bars will occasionally bring in gratis musical acts. By the 1960s, at least from the perspective of europeans, these spaces were already gone or commercialized: the concept of a shopping mall was created specifically to produce european-style public spaces in the united states, with shops and commerce being in the original conception an unimportant side-effect & features like benches, squares, and food courts being more central. (Of course, shopping malls essentially suffered through a very long bubble, as a result of real estate loopholes; they were everywhere and extremely commercialized up through the early 90s, at which point they started going out of business.)
The best idea I can think of for bringing back public spaces is to expand & publicize the low-key social spaces that already exist in or associated with libraries: libraries are already indoors, and they already essentially deal with people, but most social spaces in libraries in my experience are exclusive & by reservation only; with good sound-proofing & clever layout, social spaces & quiet spaces within a library can be segregated and the kind of free-wheeling nineteenth-century-coffee-house experience can be reconstructed to some extent.
It’s very strange that widespread high-speed internet access would coincide with increasing urbanization.
After all, one of the major utilities of this kind of communication tech is that geography becomes much less important: why bother moving to be closer to people who you’re mostly going to talk to on twitter anyway? One would, naively, think that we would end up with (within any given locality) greater ideological diversity: today, someone in a rural area surrounded by conventional republicans can discover fringe political ideas like accelerationism and adopt them — something that really couldn’t have happened in the 1980s. Why doesn’t this happen?
Perhaps it does happen, but confirmation bias leads us to both ideological and physical self-segregation? People who discover fringe leftist ideas will migrate to blue areas where those physically nearby are more open to these ideas, while people who adopt fringe rightist ideas will move to red areas. (This also explains blue oasis areas surrounded by red, like Atlanta Georgia and Austin Texas, both of which have become abnormally influential as cultural centers.)
There’s a tradeoff to code reuse, insomuch as code written for a different problem is often a poor fit.
Using someone else’s code often requires writing wrapper code to change how certain things work; often, particularly if you have unusual scale requirements or are doing other things that invalidate common assumptions, you will end up writing more code to use someone else’s library than you would be if you just implemented the functionality they provide yourself. Of course, if you are performing a very common task under very common constraints, using the same software as your competitors makes sense; it’s only when you go outside the normal range that things begin to fall apart & natural choices start to make less sense than strange choices.
Take, for instance, search. ElasticSearch is a pretty polished and reliable system — as long as you’re not pushing up against any hardware resource limits on your cluster, because running out of ram or hard disk space on any single node on even a large cluster can cause invisible data corruption & cascading failures. In other words, if your scale is normal ES makes sense, but if you have too much data and you want to wring more performance out of your hardware, you’ll want to homebrew something.
Using platforms whose facilities you don’t benefit from is how ecosystems like hadoop become nightmares. Hadoop makes sense on paper for a small set of very specific problems — bioinformatics stuff, and other situations where doing a small amount of simple math on very large numbers of very small wholly independent records is necessary. Outside of that domain (if your total amount of data is too small, or individual records are too large, or you have too many machines, or too few machines, or if the data is insufficiently independent), using hadoop on a thousand machines is usually slower than running simple command line tools on a single machine. Nevertheless, people attach to hadoop because they feel like they need power, and start using systems like hive (an RDBMS is the absolute last thing you want to integrate with a map reduce system), and suddenly something that would normally be a single line of shell and run in 30 seconds on your laptop becomes eight hundred lines of java and runs for eight hours on a four hundred node cluster.
In other words, often what matters is not writing less code but running less code, and often that involves avoiding using potentially useful libraries when they will add rather than remove complexity. Determining whether or not running someone else’s code is appropriate is complicated and can require a deep familiarity with the behavior of the library: usually, the best way to determine whether or not a library is worth using is having had already written something very similar yourself & knowing from experience how easy or hard certain things are to implement (along with knowing from experience the pitfalls of different implementations) — in other words, unless you’ve reinvented the wheel, you’re at a disadvantage when shopping for the car.
This article sort of represents a straw-man version of the reproducability crisis.
Any serious article on the subject does not focus on the correctness of individual studies, but instead on the absence of published replication attempts for landmark studies. It’s precisely because science is hard & false positives are common that the low rate of reproduction attempts, the tendency to avoid publishing negative results, and (on the science journalism side) the naive acceptance of shocking-sounding results are so damaging at scale.
Will teaching general audiences & science journalists about the variety of potential statistical & methodological flaws help? Sure — at least with respect to the science journalism side. It won’t help with the acceptance of false positives due to selection bias (which led to people like Kanneman putting a lot of trust into ideas like cultural priming that ended up being completely unreproducible).
Anyone with an interest in science knows that without high-quality replication & large, randomized samples, results are meaningless. This is not to say that small-scale exploratory studies are not worthwhile, but instead that such studies should be treated as one step above opinion columns with regard to how seriously they should be taken.
There are also real reasons why scientists end up having research and publication habits that encourage bad science; we can’t pretend these reasons don’t exist, but they are economic/incentive-related reasons, and new incentives are easily introduced into science if somebody has the money.
Providing funding for pre-registered replications seems like it’s likely to solve many of the problems that people have with the state of experimental psychology (and if you don’t agree that those problems exist, you don’t have to participate in such programs: you can leave the money on the table, and instead watch other people attempt to replicate your work); likewise, automatic stats checkers are pretty uninvasive: if you avoid mathematical errors, you won’t get a lot of notes, and if you disagree with the notes you can ignore them and wait to be vindicated. These are systems that already exist, and are already being used to change incentive systems in experimental psychology in order to compensate for common sources of concerns.
Individual experiment sample sizes were never the point of the replication crisis, except when experiments were being taken significantly more seriously than they should be, which is not unusual.
With regard to sample size, small sample sizes in studies that are taken at face value is an issue.
With regard to sample size, small sample sizes in studies that are taken at face value is an issue. Your example of small sample sizes in the context of less-serious exploratory research would not be the kind of thing that would get press, unless someone then treated that exploratory research as canon.
My understanding of the replication crisis, however, comes from people like Goldacre & Neuroskeptic who write frequently about the subject; their coverage focuses on systematic distortions in scientific literature related to perverse incentives. Your post here is the first one I’ve seen that situates it as a problem related to specific studies misleading primarily other scientists in the field. I’d be interested in knowing what articles in particular you’re complaining about, because the suggestion that any single exploratory study in isolation is a problem (outside of the tendency for the science press to hype low-quality studies) tends to be dismissed early in the coverage I’ve read. (In other words, I’ve only seen this framed as a systematic incentive problem, where scientists are discouraged from performing certain forms of vital tasks, such as large-scale replications with large samples.)
Thanks to your response, I’m more willing to treat the article in good faith, but my alarm bells started going off specifically because the model you present of the reasons behind the replication crisis is one that is explicitly rejected in the pro-replication-movement pieces I thought were seminal.
I wonder how much of this effect has to do with the mixture of pseudo-neutral & centralized media we have.
The long tail didn’t really address the rich-get-richer trend in even random distributions. (If you have a bunch of evenly connected nodes, all of which randomly decide whether or not to share a signal they recieve with their peers, you get a hockey stick distribution. If some signal starts off with a positive bias, that bias grows enormously. And, if connections are allocated the same way, the number of connections between nodes has the same asymmetry.) If we have a mix of curated, centralized media that promotes some particular figures (and particularly if those figures are promoted toward highly connected clusters of people), we’ll see a much larger growth in the popularity of those figures than we would in a purely centralized context.
On the other hand, the greater the influence of neutral carriers, the easier it is for whole groups to become less affected by some popular figures. I have never heard a song by Drake, Beyonce, Rihanna, or Adele; in 1993, I would have definitely heard any artists of similar popularity, because the only way to discover music would have been radio and television — in other words, curated broadcast media. Because I don’t listen to the radio or watch television, my taste is music is mostly formed by the tastes of my peer group — and presumably many of them have heard those artists but haven’t found them impressive enough to convince their peer group to engage with them (the way that they would for Bablicon, or Skinny Puppy, or other groups that are much more obscure on the global scale but are much more popular in particular groups).
When we look at the head in isolation, we sort of miss the point, which is that favoring organic communication of tastes over broadcast both raises all ships & slightly flattens the distribution. The long tail is longer and fatter, but the head is also higher, because the communication between preferences & the things that fulfill them is more efficient & tastes can evolve more quickly in a resource-rich and diverse environment. Someone who really only likes free jazz or ambient harshnoise would have, in 1993, thought they “didn’t like music”, but now has a much greater likelihood of being exposed to these much less globally popular genres.
(This is not to propose either preference-essentialism or some kind of free market optimism. Preferences evolve in response to experience, and this kind of evolution only happens when the marginal cost of exposure approaches zero. The long tail is made profitable only because piracy has expanded the domain before it, creating a market of savvy and dedicated people willing to spend money on obscure things where before there was only a hostile and alien environment.)
There are different right-wing tribes, with distinct values.
While the two-party system in the united states encourages us to treat them as a block, the right is no less fractious than the left, and we forget this at our peril. Any political party is a tenuous marginal cease-fire between groups that would otherwise be at each other’s throats.
Just within the republican party’s core mainstream (not counting the more rare and elaborate forms whose flowering during the Obama administration made them news items), we have several breeds.
There’s ivy-league conservatives, who are wealthy *and* educated. Generally, they make decisions based on a free-market ideology and a sense that they live in a meritocracy — after all, the system as it exists has elevated them and their family, so it can’t be that wrong. Randian objectivism became a hit with this group way back in the 50s and 60s. Some of the people currently in this group are former hippies who embraced capitalism later in life; others are people who were Birchers in the 60s. Hillary Clinton would be of this group. The unifying attributes of this group are wealth, education, and a belief in the moral good of the free market; secularism varies here but is almost entirely irrelevant, because public shows of faith are in poor taste.
Then, there’s new-wealth conservatives — people with a blue-collar background who subscribe to the same belief in free-market ideology and meritocracy, but without most of the cultural and intellectual trappings as the ivy-league conservatives. The first generation of any lineage in this group would, generally, be someone who worked up from a lower-middle-class background and became wealthy, and believes themselves to have become wealthy due to Horatio Alger style preserverence rather than through luck. Objectivism landed here in the 70s and 80s, where it cast off some of its anti-religious aspects. The children of the lineage can become ivy-league conservatives if they are taught to respect tradition and ettiquite, but otherwise remain new-wealth conservatives. Donald Trump is of this group. The unifying attributes of this group are wealth, a focus on the idea of meritocracy, a lack of “taste” and “subtlety”, and a distain for people who they see as “cheating”.
You also have the rural poor, who may be one or the other but are often both. They work off a model of the new-wealth conservatives, and are often explicitly against the ivy-league conservatives. The religious sentiment and anti-intellectuallism usually is focused in this group. Objectivism came to this group via the prosperity gospel in the 80s and 90s, and this group doesn’t generally associate it with anti-theism.
Among the new types (which are not necessarily new, but are instead newly important), we have:
Accelerationists — marxists who believe that the best way to bring about a communist utopia is to be as capitalist as possible, so as to hasten the inevitable breakdown. They may consider themselves leftists, but they behave in a way that is indistinguishable from the far-right.
Neoreactionaries — people with a nostalgia for an idealized version of centralized absolute power. A lot of them think that the best way to bring about the emergence of a global centralized power is to make democracy & capitalism collapse by exploiting the flaws in the existing structure, so as a result neoreactionaries and accelerationists are in alignment in working together in acting economically far-right despite not believing in economic far-right policies.
The ‘alt-right’ — a mix of various smaller groups, mostly dominated by people who subscribe to a dumbed-down version of the neoreactionary ideology, with some of the anti-capitalist stuff removed. The alt-right is neither intellectual nor anti-intellectual but pseudo-intellectual; as a result, they are at odds with both the anti-intellectuals (because they consider themselves intellectual) and the intellectuals (because they’re resistant to examining their beliefs). They circulate material from neoreactionaries without having a clear understanding of it. Often they have a perspective that combines objectivism in its anti-theistic dimension with social darwinism and scientific racism. They are not neo-nazis, because they don’t really have an interest in tradition. There are lots of engineers in this group.
Right-libertarians — a mix of objectivists of various stripes, people whose fully justified paranoia about government (as the best-armed group in their vicinity) can be easily overwhelmed with invented paranoia about foreign invaders, and people who simply have a much greater faith in free markets than in other social constructions. This group was part of ground zero for objectivism in the 50s and 60s, when libertarianism was mostly undifferentiated, and starting in the 90s this group had an influx of people from the much more religious “rural poor” group. I classify anarchocapitalists under this banner, no matter how much they might complain. Sole unifying attribute: distrust of power.
Actual neo-nazis — these guys never went away. Social darwinism, traditionalism, and pretty explicit anti-intellectualism, with a strong racial & xenophobic component, characterize this group. They haven’t been part of mainstream discourse or a large block for a long time, but some of their ideas get circulation via other groups.
Right now, you can’t win an election by targetting only one of these groups. To win, you need to at least get the big three older groups. Some of the newer groups are gaining numbers and power, particularly in places like California.
If you go to a university, the conservatives around you will generally not be of the rural poor variety: a university education would be expensive and of dubious value to that group. All other groups (with the exception of the neoreactionaries, who consider universities to be a propaganda outlet for democracy and capitalism, and neo-nazis, who consider universities to be smearing the good name of Hitler) would be well-represented at universities; accelerationists would actually be overrepresented — there aren’t many of them, but they are all university-educated and most have been university-employed.
This essay was more entertaining when Robert Anton Wilson wrote it.
Supporting this facility is the medium staff’s design decision, sure, but it’s also clearly the wrong decision, if their goal is to built a community for intellectually honest communication.
Medium made certain design decisions early on that made it seem like they wanted to encourage good content and discourage clickbait: a lack of a facility for reblogs, extremely high maximum content sizes, reading time estimates, a relatively spare & simple design with very limited theming support and no ability to embed ads. Allowing people to reccomend an article without at least scrolling to the end of it is not in line with these other decisions.
I understand allowing people to recommend articles without registering a ‘read’ — Medium’s reading time estimates are often wildly inaccurate, and when I take five minutes to read something Medium expects to take twenty, I don’t want to wait another fifteen to click that heart. But, even if low-quality recommendations are allowed, we don’t need to cater to them.
Medium’s general quality seems to have dipped over time, with short, low-effort articles becoming more common or more often recommended. Titles have, in my experience, become more click-baity. Any mechanism that allows people to focus on an engagement metric less stringent than reading is likely to encourage this kind of decay (because, if you give somebody a number, they will do whatever it takes to maximize or minimize it).
Any solution that’s ineffective unless 100% of the population is morally upright is unusable.
Any group will contain defectors — cheaters who use the letter of the law against the spirit of the law, and will break the letter of the law too if nobody’s looking. (This isn’t necessarily a bad thing, but it’s a problem when these defectors also lack empathy or morality.) You can’t identify and eliminate these people — for one thing, the ethics of that are pretty muddy, and for another, their main talent is passing as morally upstanding — so any strategy for disincentivising a behavior needs to operate at least in part by making such people fight amongst themselves and prevent each other from engaging in whatever behavior you want to disincentivize. (This is why shame is such a powerful tool: it allows anyone to elevate their status over anyone else so long as that person engages in (or can be made to seem to engage in) some behavior that’s more or less globally accepted as undesirable in the community, so even very morally reprehensible people will shame in a prosocial way because it gives them an advantage.)
The thing about fake news is that it’s potentially pretty powerful. (It’s also a really vague term, but the variety of fake news that people are concerned about is specifically what’s normally called “disinformation” — mixtures of true and false statements crafted into a narrative designed to cause a specific targetted group of people to engage in some specific behavior. We’re not particularly concerned about staged photos of rats riding crocodiles, or press releases regarding dog brothels, or clear satire, or reports on the recent exploits of Bat Boy, even though all of these things are also fake news.) Anyone who wants to wield power will use the tools at their disposal, and the fact that media has become a very inexpensive tool to wield is probably a good thing since the kind of people who would use it to consolidate power are more likely to fight each other than to collude.
However, the real power of this kind of fake news is that people spread it because they want it to be true, even if a very small amount of effort would show that it’s false. This is not an accident: disinformation is engineered specifically to appeal to the target audience, so that it spreads.
I would argue that the best way to slow the spread of disinformation & rob it of its power is to educate people specifically to be significantly more skeptical of anything they want to believe (or that fits with their world-view) than of things that do not. In a media landscape that is in competition with its users, we must reverse Sagan’s maxim: that which is easy to believe (ordinary claims) needs more proof, because what constitutes ordinary has been taken into account (and sometimes engineered) when constructing false messages. At the very least, we must distinguish clearly between claims that are easy to believe for emotional or narrative reasons versus claims that are easy to believe for reasons related to the hard sciences.
A major flaw in humans is that believing is seeing instead of the other way around: we see evidence of what we believe, and anything that really violates our mental models is mostly invisible. We only see things that we’re looking for, most of the time. This vulnerability is well-known, and all con artists from minor to major take advantage of it. It can be battled with habit.
Lots of people don’t — or can’t — distinguish between knowing how to solve simple problems in a specific language & being a programmer.
Nearly all boot camps seem to lose this distinction, as do people who advocate for computer programming education in schools. Some universities also don’t make this distinction. Most beginner programmers don’t make this distinction, or else falsely believe themselves to have graduated to the other side of it.
This goes hand in hand with the idea of programming being a primarily vocational skill. The very shallow version of programming is, of course, of only vocational use — and of generally low value.
Boot camps probably can deliver what they promised, in the sense of producing incompetent, white-belt programmers that will get hired by non-technical HR people (who can’t distinguish between real programmers and white belts) and put into offices full of white-belts, who will crunch away at simple problems until the budget runs out. It’s in line with a lot of the worst practices in the tech industry, insomuch as it involves people who don’t know any better tricking other people who also don’t know any better & pretending everything’s fine until the inevitable collapse.
I value the other skills developed by programming over actually programming, and I think people would benefit in all sorts of fields that don’t involve programming if they’d learn and apply them. Of course, these skills are hard to test for. It’s very easy for pockets of white belts to develop in any organization, and white belts can’t really conceive of that kind of distinction (and nothing is really a good proxy for it — in part because wage inflation in this sector makes people game everything they can, so proxy measures have a half life of weeks before they must be retired).
A bootcamp won’t ever do more than give you a white belt, and a bootcamp won’t teach you that a white belt isn’t enough. Bootcamps and schools attract the insufficiently self-directed, who will never graduate beyond the white belt because they don’t really want to learn for the sake of learning. Some of these people don’t feel the need to become competent, because they came for the high wages; when the wage bubble collapses and white belts start being paid like other semi-skilled entry-level white-collar workers, there will be fewer of them.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
There are a couple misunderstandings here.
1. No serious proposal for fighting fake news involves filtering. Facebook’s proposal involves manipulating ranking (so that spurious articles, which have until lately had an artificially high ranking, will appear lower in the newsfeed than similar real articles) and tagging articles visibly as suspicious, along with an extra dialog when sharing. Likewise, extensions for identifying fake news rely upon tagging. The point is to ensure that people are aware, when sharing an article, that the article they are sharing is from a systematically unreliable source; given the number of people who react with horror to Onion articles, this kind of clear tagging is probably justifiable even on the grounds that obvious satire is not obvious to drunks. 2. Satire and detournment will always have a place in discourse. However, they function via an interplay with a consensus reality. The problem with “fake news” is a collusion between large groups of people to create an internally consistent but false version of world events, in order to manipulate people who accept it into performing specific kinds of actions. Presenting a collage of Hitler and Moussolini kissing is very different from creating a media ecosystem of hundreds of sites all geared toward selling people on the idea of drinking colloidal silver, or replacing all their money with gold bullion, or shooting up pizza parlors; for one thing, while the former is easily fact-checked, the latter can only be fact-checked by referring to sources outside of this alternate media universe, which of course are tainted in the eyes of those taken in.
Art can be very powerful. As a result, we should use it responsibly. If people are dying because of your artwork, then you bear some responsibility for their deaths; likewise, if your artwork led to suffering, the onus is on you. I won’t say that fake news stories can’t be art; I will, however, say that politically charged art doesn’t exist in some pure apolitical universe without consequences.
If you can’t act responsibly, someone else will take responsibility and recontextualize your art in a way that you may not like; if your community can’t police its own behaviors, then the people your community effects will police them for you.
A qualified defense of slacktivism
Slacktivism is a failure if you consider it to be a form of activism, but as a form of value signalling, it is terribly effective. We underestimate the value of signalling at our peril.
Some parts of our moral landscape appear to be biological in origin: a revulsion reaction to the idea of incest, for instance, appears to be the result of the sexual imprint process (and we can tell because siblings separated at birth actually have a much higher likelihood of ending up together, while unrelated children who spend a lot of time living together prior to puberty, even in non-family settings like boarding schools, have unnaturally low rates of sleeping together as adults). Others, however, seem to be primarily controlled by culture, specifically by cultural manipulation of empathy and shame. (We dehumanize enemies in war via propaganda, in order to eliminate the empathy we might normally have toward them & the shame we might hold for killing them, and instead substitute a new set of rules around what kinds of killings are shameful. If the domain over which empathy operated was biologically determined, the flexibility that makes modern warfare possible would not exist.)
Some people lack empathy (or its effect on them is abnormally low), just as some people lack the in-built biological drive to avoid incest. Shame works to police such people. Ultimately, shame serves to punish people who engage in violations of a culture’s idea of moral behavior by lowering social status (and with it, access to various resources — particularly, other people). While we should still be concerned about sociopaths (who have a strange sort of superpower: their actions are not constrained by the force of empathy, and they lack the self-control to respond reliably to shame), the worst excesses of garden variety empathy-deficient narcissists can be avoided by judicious application of social pressures.
Value and virtue signalling is a major way in which a culture indicates what behaviors are considered acceptable and what behaviors are not considered acceptable. The other major way in which acceptable behaviors are signalled is punishment; however, punishment requires that the behavior be practiced and the culprit be caught. Value signalling might include describing counterfactuals or hypotheticals about punishment for breach of acceptable behavior (ranging from fantastical visions of hell to fairly concrete legal sentencing guides).
Slacktivism is a form of weak value signalling, wherein large numbers of people expend small amounts of effort in a token representation of the ideal behavior. While strong / costly signalling would send a more powerful message, it’s not accessible to most people, and so the number of people engaging in it will necessarily always be small. Weak value signalling, at scale, is actually more potent: after all, costly signalling is more desirable both to those with sufficient resources that the marginal cost of signalling is smaller than its apparent cost to its target audience (philanthropists) and those with nothing to lose for whom costly signalling also represents an out (terrorists), neither of whom can be trusted to be an accurate representation of group norms. Mass action, on the other hand, is group norms embodied, and low social cost makes scale possible. As a result, slacktivism allows small changes in moral values to propogate quickly from the majority who already accept them to the minority who haven’t yet, in much the same way as the Game of Life: if you’re surrounded locally by people who send a particular signal, you’re more likely to send that signal, until the signal reaches some saturation point.
Slacktivism, by changing the collective value system, also encourages acts of costly signalling in the same direction as that weak signal. Popular causes get donations (although the ratio of weak signals to strong signals will always be large).
Criticisms of slacktivism tend to hinge on the idea that it’s a substitute for activism — that everyone who changes their profile picture to a flag might otherwise be staging a sit-in or assassinating a congressman or otherwise helping make real changes. But, slacktivism is better modeled as a form of collective social control of activism: a means by which various causes have their perceived importance ranked.
News to me.
The CIA & FBI believe that the leaks were supplied by russian intelligence — which is in line with what most security researchers already believed. It’s not unexpected that the FSB would time a leak to maximize its effect.
If the CIA & FBI have stated that Wikileaks is complicit in choosing the timing of releases to benefit the FSB’s plan, rather than agreeing that the FSB’s leak timing was appropriate for their own purposes, I haven’t heard that.
This is, of course, your claim: that Wikileaks is in bed with the Kremlin, instead of being a separate organization with its own goals, some of which may accidentally coincide with other actors.
I don’t really see any evidence of explicit collusion.
One’s career is rarely a vocation.
It is not the norm for workers to be filled with purpose by their job; it’s the exception, and one accessible only to those lucky enough to have a lot of freedom to choose positions.
People who have a sense of purpose will work toward it for free, if it doesn’t interfere with their ability to live comfortably (and sometimes even if it does). This is, after all, what hobbies are: situations where someone spends money in order to do work that nobody is willing to pay them for, simply because they enjoy working.
The typical worker is performing tasks they despise in order to earn barely enough money to eat. To free them from that toil without taking food out of their mouths is a mercy; it gives them the opportunity to persue a purposeful life.
Right now, all economic and social incentives are focused on automating the position and simultaneously removing the income, replacing it with nothing. This is cruelty: we’ve taken a person who could already barely survive, and while we’ve removed their ability to survive slightly better by performing a task that makes them miserable, we’ve simultaneously stuck them in a situation where, in order to maintain their ability to eat at all, they must show proof of looking for work while avoiding actually finding any; after all, getting back a job equivalent to the one we automated away means having no income between the benefit cutoff and the first paycheck, and since they were prevented from accruing savings, it means weeks of not eating. UBI targets the welfare trap and eliminates it, making changing jobs less risky for everyone.
I feel like some of this material is covered in transactional psychology, though my understanding of transactional psychology is heavily influenced by Robert Anton Wilson so this might not be canon.
You can kick somebody down from object- or meta-level discourse (third circuit thinking, in Wilson’s terms) to tribal-level (second circuit) or stroking-level (first circuit) by being percieved as a threat to identity or to survival, respectively: isolation is treated as an existential threat, and so social strokes are treated as a proxy for being fed and protected by a community. It’s only when someone has bodily comfort that they are able to fully immerse themselves in anything other than stroking-level thinking (and so, people with severe anxiety or with chronic pain often end up alternating between being needy & being standoffish — which makes their situation worse — because the effort it takes to jump up to the meta level and think clearly about how to effectively manage their need for social interaction is much more difficult for them to achieve); it’s only when someone feels like their sense of identity is not under threat from outside that they can be flexible about questioning, changing, or violating it from the inside.
In other words, when life is shitty, everyone operates with less mental capacity for abstraction, leading to life becoming even more shitty. (There are other variances in individual capability, of course, which means that just because you’re in pain doesn’t necessarily mean you’re completely screwed.)
This makes for a clear but difficult route toward improving the state of dialogue. Improving general quality of life globally will make clear thinking easier (and though it will not make for rosier topics, it will make it easier to retain emotional distance and avoid engaging in behaviors that are globally counterproductive), but pretty much every existing social or economic system is pinned against an across-the-board increase in quality of life. On the other hand, temporarily excluding the most toxic actors from conversations via shadowbans & similar mechanisms — probably the most reliable method short of improving quality of life — is already done.
Lack of political sophistication is often a side effect of being sheltered.
Lack of political sophistication is often a side effect of being sheltered. Of course, the truth is that politics is weird and wild and wooly, and there are more corners to it than most people will ever be able to imagine. (Try explaining the concept of accelerationism to someone who thinks Obama is left-wing.)
In this sense, our best weapon is mere exposure. Even without attempting to remain civil, and even without bringing up views we necessarily believe in, just exposing people with a limited understanding of the range of political ideas to positions outside their mental models is helpful.
If someone is pushing an extreme view, push a view you see as extreme in the opposite direction, so that the overton window expands rather than shifting. Expose fascists to anarchocommunism. Expose neocons to negative taxes and neoliberals to deflationary currency. Expose right-wing transhumanists to anarchoprimitivism.
The CS/IT world seems to have an unusual absence of understanding of its own history, compared to other engineering fields.
Sure, everybody with a degree is vaguely aware of Turing, and maybe they’re aware of Babbage, Lovelace, and a couple other figures. But, I rarely meet even professionals who are aware of the lineage of the ideas they work with.
Web developers somehow limit their understanding of the history of hypertext to the idea that Tim Berners-Lee did it, and are unaware of Ted Nelson, Hypercard, NLS, and Memex. Front-end developers repeat the idea that Apple invented the GUI, or repeat the idea that Xerox did, but generally don’t distinguish between WIMP interfaces & other forms of GUIs, and aren’t aware of the landscape of UX going back to Raskin, Licklider, and Englebart, nor are they aware of the way that the Macintosh interface was influenced by the Lisa, the Amiga, GEM, and other contemporary competitors.
Software engineering culture, at the low end, consists mostly of uncontextualized lore, often repeated and accepted without much consideration. We repeat the maxim that premature optimization is the root of all evil without recognizing the extremely limited context in which the author of that epigram would have agreed with it, nor the fact that the same guy would have wanted every prospective computer programmer to first obtain a doctorate in mathematics.
The proliferation of fads comes from a broader feeling among the HN crowd that the history of this domain is not worth understanding more than very shallowly, because all valuable ideas lie in the future. As a result, otherwise intelligent people repeatedly reinvent the wheel, not realizing that their great idea was invented, written about, and eventually rejected by someone much smarter than they are in the late 1950s.
In the sixty years that this field has had a commercial presence, we’ve picked much of the low-hanging fruit. Future progress will be harder, and if we don’t cultivate a culture of understanding history we won’t be able to apply the wisdom of the past to ideas in the present. When breakthroughs in computing were primary academic, this was less of a problem: to get a paper published, you have to cite your references and give a sense of where the new idea fits in the existing domain. In a commercial environment, on the other hand, misrepresenting old things as new and cultivating an ignorance of other related work is encouraged from a marketing perspective — and when your work is VC-funded, marketing is the only thing that matters.
18 book reading list for computer history
18 book reading list for computer history
from broad strokes to the lore to the stories to the tangents to the UNIX, these are must reads for every hacker.
The tech industry has a bad case of memory loss these days. Luckily, previous generations of the industry (along with journalists and academics) have done a pretty good job of cataloguing and contextualizing our history for us.
If you have any interest in engaging in or interacting with the tech industry, knowing the history gives you the upper hand. With that in mind, here are my picks for the minimum set of volumes you should read, in order to get a general idea of the important bits of computer history.
The broad strokes
Rise of the Machines, by Thomas Rid
The Information, by James Gleick
The lore
The Devouring Fungus, by Karla Jennings
The New Hacker’s Dictionary, by Eric S. Raymond
Out of Control, by Kevin Kelly
Microserfs, by Douglas Rushkoff
Man-Made Minds, by M. Mitchell Waldrop
Stories
Turing’s Cathedral, by George Dyson
Hackers: Heroes of the Computer Revolution, by Steven Levy
What the Dormouse Said, by John Markoff
Fire in the Valley, by Michael Swaine and Paul Freiberger
Possiplex, by Theodor Holm Nelson
Weaving the Web, by Tim Berners-Lee
Tangents
The Idea Factory, by Jon Gertner
Interface Culture, by Stephen Johnson
UNIX
The Art of Unix Programming, by Eric S. Raymond
The Unix Haters Handbook, by Simson Garfinkel, Daniel Weise, and Steven Strassman
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
I hypothesize that the chumbox gets populated by items that intersect along the lines of author or tag, ranked by some sort of ‘hotness’ metric (some combination of time since publication, number of recommends, and comments), and that if there are fewer than three items above some arbitrary threshhold cutoff the remaining items are populated using the same mechanism as reading roulette (i.e., hotness plus user’s followed / interacted with tags & authors). I might be way off base, though.
Medium’s particular chumbox is less offensive than outbrain/taboola, and tends to limit itself to mostly articles that genuinely will be interesting to people who read and were interested in the articles. As a result, I wonder if they’re doing something with recomendation-engine-style statistics; after all, they have not only view statistics but read statistics for all articles, not only tags but suggested tags (along with the training data they use to compute suggested tags from content), and full interaction graphs. They have all the information they need to get really excellent suggestion targetting, and (because there’s nothing remotely resembling monetization) there’s no real incentive for people to spend a lot of effort trying to game recommendations.
On the other hand, it seems to encourage rabbitholes.
On the other hand, it seems to encourage rabbitholes. If I click on an article by someone who writes prolifically on the same general group of subjects, the recommendations will send me on a random walk through their works; if I click on an article by someone who primarily interacts with some group of people, I will see mostly the articles from that group.
This is great if you want to encourage relevance. Sometimes, though, I wish I had a recommendation system that would direct me, specifically, to the opposite: give me what your algorithm would consider the worst possible match. While something like reading roulette will sometimes give me pleasantly bad matches just by including completely random articles (for instance, I once ended up reading an evangelical christian Trump voter’s article about abortion), it’s hard to get a sense for where your overton window lies on the whole spectrum of users just from random entries. Recommending the absolute worst matches might cause some interesting churn, as people are systematically introduced to ideas that any other recommendation system would actively hide from them.
Systems that exist to serve this niche of people who want to get outside their bubble don’t tend to take full advantage of the network. Something like Rando Carlassian (a bot that tweets news stories randomly from an evenly sized pool sourced from right-wing and left-wing sources) still is limited to fairly conventional sources and can’t take into account anything about the mind of an individual user, so as a result it misrepresents the world such that it presents something like Breitbart as being as far right as people go and Jacobin as being the extreme on the left. The reality is that all ideas have a political dimension and politics are weird and divisive in a fractiline and fortean manner: flat-earthers, hollow-earthers, accelerationists, and people who legitimately believe that the moon is a hologram all exist and sometimes have an out-sized effect on the mainstream because our universe often resembles a kind of absurdist prank gods play on each other. Even mainstream political ideas are unbearably weird if you look at them from an outsider’s perspective.
A systematic search for pathologically poor recommendations would allow us to immerse ourselves in an outsider perspective tailor-made to detourn our own ideas.
There’s a culture (and I think it’s a relatively new one — even as recently as fifteen years ago, these attributes were pretty rare) that devalues context in favor of shallow easy fixes, and I blame at least some of the forces that discourage craft in coding (like boot camps) on this cultural shift.
Once upon a time, programmers really embraced elitism, which made for an emotionally toxic environment but nevertheless was very effective in encouraging people to absorb the lore. As a result, most people either had a prety deep understanding of a wide variety of ideas (ranging from technical concepts to techniques to purely cultural things like legends & in-jokes) or were excluded. During this era, code was still bad: tools we use to improve the quality of code and make writing high-quality code easier didn’t exist or were barely usable; however, for the most part, people writing abnormally bad code knew that their code was bad & how it was bad, and just wrote it anyway out of laziness.
We lost some of that toxic elitism, and that’s probably a good thing. On the other hand, it’s not totally gone: it has mutated into a less useful form, where groups of people who are all almost uniformly incompetent form toxic hierarchies based on bogus values because they’re isolated from the greater development community. Additionally, we’ve sort of given up using shame in appropriate ways. Shame is a really excellent tool for encouraging good habits and discouraging bad ones on a community level, and where serious study and careful thinking is of great importance, not using a tool at least as powerful as shame to encourage study and care leads inevitably to a culture dominated by ignorance and carelessness.
We’ve made a mistake in the way that we’ve smoothed out the learning curves for our industry. We took the pressure off beginners to advance quickly, which is fine, but we allowed beginners to believe they are experts, and now they run bootcamps. We encouraged people to value coding, but we failed to distinguish the value of internalizing the lessons of programming from the value of memorizing how implement simple applications in a single language by rote, and we failed to distinguish the intellectual value of the programming mindset from the economic value of the programming vocation, so we’ve incentivized beginners to teach other beginners simple formulae and allowed them to believe that their limited understanding sets them up to be geniuses and millionaires. Now, we’re surrounded by overpaid beginner programmers writing reams of crap code, and it’s a crisis.
Writing good code is hard and takes time. Writing a single line of really good code takes years, because you need to study and practice for years before you are capable of distinguishing good code from bad code. Just being mindful in the moment of how much effort you’re putting in is wholly insufficient.
The tragedy is that this feature is easy to implement if you work for Medium (or Facebook, or Netflix, or whatever) but impossible if you’re a user.
Where Wizards Stay Up Late would be a great addition, but I haven’t finished it, so I didn’t feel it would be right to include it. (This is also true of The Soul of a New Machine by Tracy Kidder)
Ive is very much acting in line with Jobs’ legacy.
There’s a lot of mythmaking at Apple, and it’s in some ways responsible for Apple’s recent success, but in this case it really obscures the history. Jobs was all about removing functionality in favor of form in his work at Apple, starting with his take-over of the Macintosh project at the latest. While he didn’t do this at NeXT to the same degree, as soon as he came back at Apple, he doubled down on it.
Apple has never been the “fun” alternative (if you count all its competitors, rather than pretending the only alternative to the Mac has ever been the IBM PC, which was never true). On the other hand, removing important features in order to cut production costs was pretty standard in the late 70s and early 80s, and would have been completely acceptable in the generation of machines the Apple ][ belonged to. (The Apple ][ initially was unable to display lower-case letters, in order to save on ROM; similarly, Sinclair BASIC used single characters as commands, to save space.) We owe Jobs’ and Ive’s legacy to the predictable commercial failure of the Lisa and certain cynical ideas Jobs developed about the market in the wake of that failure. The relative success of competitors like the Atari ST and Amiga despite marketing and advertising failures demonstrates that Jobs’ model of the computer industry in the mid-80s was completely wrong, but it nevertheless defines the behavior not only of Apple but of many of Apple’s competitors to this day.
The Lisa would have been a half-decent machine, if made with today’s technology. It was, however, made with late-70s technology, and very little attention was paid to performance. As a result, this machine cost two thousand dollars in 1982 money, couldn’t operate without an external hard drive of roughly equivalent price, and once operational would take minutes to perform simple operations. It was a failure, because it was an expensive and unusable machine; on a purely technical level, it was a success at being interesting, building in many truly novel ideas. After the market failure of the Lisa, Jobs kicked Raskin off the Macintosh project and turned the Macintosh into an attempt at a low-budget version of the Lisa.
(Raskin’s ideas for his original Macintosh project really were novel and revolutionary in a way that the Macintosh was not; unfortunately, aside from a short-lived attempt at selling it as ‘Swyftboard’ extension cards for the Apple ][, these ideas only ever made their way into the Canon Cat — a dedicated word processor that cost as much as a Lisa.)
Jobs decided that the failure of the Lisa was the result of high cost and low performance (which is true), but (being non-technical) he had no idea what kinds of functionalities are actually slow or require special hardware and (being an egomaniac) he made ridiculous pronouncements about performance in public and held his engineers to them. As a result, the original Macintosh had a small, built-in monochrome (not greyscale!) display, half the RAM of competitors, no multitasking, and only one mouse button. It sold for less than the Lisa, but for double the price of competitors that had double or triple the performance. Much of the development and prototyping cost actually went into repeated changes to the shape of the case.
As a result of the Macintosh hemmoraging money (despite the high margins, not enough people were buying them to make up for the famously wasteful development, and by the time the Mac shipped it was four years behind the curve on hardware) and Jobs’ habit of yelling at employees until they cry, he was replaced as CEO by John Sculley, and eventually forced to resign. (Apple under Sculley continued a Mac-centric plan, but continued to lose money; however, Macs under Sculley actually had color displays and more-or-less competitive hardware.)
For people who have short memories and only care about Apple post-1997, this may seem like ancient history and irrelevant. But, we have to recognize that during the era when Jobs wasn’t in charge, Apple pushed the Mac in the direction of being like its competitors. During those years, the Mac competed on features. There were low-budget models that cost less but had crappier hardware. For a short period in the 90s, there were licensed (and unlicensed) third-party Mac clones that could run Mac OS. (Apple didn’t compete well with the other players, but with Atari practically going out of business again due to the failure of the Jaguar and Lynx and Commodore’s spectacular mismanagement of the Amiga line, they ended up surviving a pretty tumultuous period, with their major competitors as of the late-90s being mostly competitors run by former Apple employees — NeXT and Be. Apple at this time was a little like Yahoo is now: losing money and repeatedly making awful business decisions, but holding on to enough loot from its glory days of decades before to still be an important player to consider, like a senile giant.)
When Jobs came back, he cancelled all ongoing projects and instructed his employees to start working on making a Mac OS emulation layer for NeXTSTeP. He then had a cut down version of their next-generation Mac released, in a colorful case and without a floppy drive. (Releasing a machine in 1998 without a floppy drive was like releasing a machine today that can’t display lower-case letters.) Eventually, we got a fully re-branded NeXTSTeP/BSD hybrid that could run legacy Macintosh applications, in the form of OSX, and a bunch of really bizarre case designs (ranging from something that resembles a modern tablet embedded in a brick of lucite to screens mounted like desk-lamps). (We also got a couple conventional towers, but you have to recognize that in 1997, Macs were all still beige boxes.)
In other words, Jobs’ influence on the Macintosh prior to 1986 was to drop anything resembling an interesting feature (including his famous refusal to include expansion ports on the original Mac) in favor of fancy beveling, and Jobs’ influence on the Macintosh from 1997 to 2000 was primarily to drop important hardware in favor of translucent colored plastic and drop current multi-year development projects in favor of just copying what his last company did.
(We should examine NeXT a bit. NeXT was full of people from the original Macintosh team — Jobs took the best and brightest from the Macintosh and Lisa teams with him when he left. NeXT used an off the shelf UNIX kernel, hired on the inventor of Objective C, and did some interesting technical work by mid-80s standards (we owe many of the strange behaviors and limitations of the modern web browers to the fact that Tim Berners-Lee liked the NeXT UI builder tool), hidden behind a machine that still had a monochrome display at the edge of the 90s. NeXT didn’t have the same resources as Apple did, so development couldn’t be so wasteful; the NeXTCube was a decently solid machine. Still, NeXT boxes were expensive, and the company hobbled along just like Apple did. During this era, Jobs wasn’t continuing his Macintosh habit of taking somebody else’s design, stripping features from it at random, and calling himself a genius for knowing which features to strip; but, when he got back to Apple, he did this with his own NeXT machines.)
Apple’s real success under Jobs, though, was to take existing products on the market, make clones that strip important features at random, and sell the clones for double or triple the cost of the technically-superior originals. In other words, to perform the same operation that turned the Alto into the Macintosh (by way of the Lisa) on the existing MP3 player & smartphone markets.
When Apple runs out of things to copy, it tends to either produce ill-conceived products of its own or start removing features from its own previous generation at random. Jobs didn’t have to be good at removing features: he had sufficient charisma that he could pass off all his mistakes as works of transcendent genius. But Apple is now run by his ghost, and while Apple employees are close enough to the source to have their realities warped, the rest of us have been snapping out from under the spell.
The truth is, removing features is something that has to be done very carefully. Jobs’ charisma masks the fact that nothing he did from 1979 to 1999 was a good business decision and every one of his successes are owed to his ability to persuade strangers to believe plainly false things. Without that shield, Apple can’t last very long with the same rulebook, because none of the rules ever had to make sense before.
This is why I always say that if we want an idea of how life will be under UBI, we should look at the current lives of the wealthy.
Labor won’t suddenly completely dry up, for the same reason that those who are wealthy enough not to need to work often end up performing volunteer labor: the more labor is disentanged from survival, the less one is alienated from it (and the more it resembles a hobby). (Under UBI, we also won’t really need to worry about the wealthy performing jobs normally done by the poor at lower rates, since labor flexibility becomes normal rather than the privilege of the elites.)
You don’t really need to go to 4chan for this.
You don’t really need to go to 4chan for this. The facebook group “Ancap Memes from Rothbard’s Dreams” specializes in these.
I mean that, ideally, facebook, medium, and youtube wouldn’t be websites, because the web is a poor solution to the problem of building responsive user interfaces.
Sometime in the late 90s, we all collectively decided that all networked applications were supposed to run off port 80 and sit inside a web browser. This leads to massive engineer labor (software engineers end up spending enormous amounts of effort trying to do what would be trivial as a native app inside a web browser, software engineers spending an enormous amount of effort trying to make web browsers capable of doing everything web designers are trying to do with them without creating security flaws), massive resource waste (cycles and packets wasted en masse because nobody’s using more efficient methods, even when those methods would also be easier to implement, because crappy solutions have become the default), and ultimately we screw the pooch on security too because we’ve layered everything important on top of a simplified version of a 1992 demo intended to explain Enquire to suits instead of ensuring that our technical decisions make technical sense.
We’re locked in now. But, were we to make sensible decisions from the beginning, our hypertext systems would have permanent content-based addressing with automatic replication (instead of temporary machine-based addressing that fails to uniquely, consistently, or permanently correspond to any given piece of information), no scripting (since scripting is totally unnecessary for hypertext), no association with the domain name system (because domain names are broken), no association with the certificate chain system (because root certificates get leaked all the time), no ad-based revenue (because ad-based revenue encourages a race to the bottom in terms of content quality), and no embedded markup (external, offset-based markup is easier to implement and avoids most common markup-related problems).
As a web developer, your job is to make objectively poor decisions about web development in ways that are profitable to your employers. That doesn’t mean that you need to believe those decisions are good; you just need to implement them.
I’m not kidding.
The web is good at exactly one thing: transmitting static text documents that include minimal formatting and hyperlinks. It’s not even great at that. CSS and Javascript take the easy problem that the web already mostly fails at, and turns it into a hard, complicated problem by providing poor tools for simulating native applications.
If you feel like you need CSS, write LaTeX or PostScript. If you feel like you need JavaScript, write a native app (maybe in JavaScript!). Don’t put a browser where it doesn’t belong.
I think the time might be right for micropayments as a publishing monetization model.
The reason is that ad-based monetization is already micropayment-based. An advertiser estimates how much a view would be worth, and sends that much to an ad host per view. The big problem with ad-based monetization is that neither the author nor the viewer of the post is on either end of this transaction and the transaction cost has been turned into a futures market that everyone has been incentivized to game. As a result, the average impression is of approximately zero value, because the average impression is made by a bot looking at an advertisement for a nonexistent product on a site intended to farm fraudulent ad impressions.
If we associated our accounts with some money, and charged as much per view as ads pay out, then gave medium half of that (which is more than the ad host would get paid out under adsense), we would cut out a whole industry of middle-men and your average user could coast on five bucks for decades. (If new users got a dollar credit or something, it would essentially simulate a freemium model; if users had to add funds in five dollar increments, Medium could make a fortune on interest from their escrowed cash. The normal case would have money mostly stay inside the system — in Medium’s coffers — because most people who do a lot of writing on Medium also do a lot of reading on Medium. Big publications would still make bank, because they’d still get a lot of views; they’d actually make more profit, because three tenths of a cent per page view is more than they’d ever make off ads.) Medium already determines the distinction between a “view” and a “read” — so if they charge/pay double or triple as much for a “read” than a “view”, readers wouldn’t notice but authors would be highly incentivised to create high-quality content worth reading, and clickbait would disappear.
I think we can tag him. Ev Williams
Cyberpunk, sadly, died in infancy. What’s worse is, it was replaced by a changeling.
When I talk about cyberpunk, I’m really talking about the politically-charged visionary science fiction written by Bruce Sterling’s cohort in the late 1970s and early 1980s that combined the stylistic experimentation of the New Wave movement, the social consciousness & focus on economic injustices of naturalistic fiction, the moral complication of hardboiled/noir fiction, and the bite of the then-recently-deceased punk movement. By the time Neuromancer got published, most of the really interesting stuff in the cyberpunk movement had already stopped; the occasional really good cyberpunk after that point (such as the Ghost in the Shell manga — not the film or the show — or Akira, or Altered Carbon) were rare, because immediately upon breaking into the mainstream, cyberpunk was consumed by the Spectacle and became an aesthetic instead of a movement.
As a result, it has a really complicated legacy.
Gibson never really stopped writing about the same kind of material he wrote about in Neuromancer. He still explores the sociology of extreme economic inbalance. Likewise, Bruce Sterling is as good as he’s ever been. John Shirley & Rudy Rucker, despite being central to the group, were always doing their own thing and continued to do their own thing without really being able to be clearly categorized as cyberpunk. But, post-1992, nothing Gibson wrote was really considered cyberpunk, and Sterling transitioned even earlier.
Because it’s considered an aesthetic, cyberpunk is strongly associated with specific imagery, and the imagery is tied to various periods. People make odes to first-generation cyberpunk (prior to 1990, essentially) by fetishizing 80s tech (see Jackrabbit, for instance), and while this aesthetic is one I find only minimally problematic, it misses the underlying point in the same way that Steampunk misses the underlying point of The Difference Engine.
By the early 90s we started removing even the punk aesthetic from cyberpunk aesthetic, and cyberpunk was almost entirely politically neutralized by 2000: we associate cyberpunk often with Hackers (1995), wherein the political content is limited to vague and half-hearted anticapitalist sentiment and a cliched-by-1994-standards environmental message tacked on as an afterthought, The Matrix (1999), which gives a pretty good representation of the Spectacle but whose political content has nevertheless been completely misunderstood by a core audience who has reinterpreted it as anti-woman and anti-semitic, and Swordfish (2003), which was slicker and in retrospect smarter than all the others but extremely politically confused. The idea of a cyberpunk protagonist in 1990 was a low-level criminal with a drug addiction — a tweaker who needs a bath. The idea of a cyberpunk protagonist in 2000 was a rich white teenager with bleached hair, sunglasses, and a leather duster. By 2005, the ideal cyberpunk protagonist willingly worked for the government. Today, as the original cyberpunk works are more relevant than ever, people ignore the books in favor of movies that show pretty people in leather performing magic.
A return of cyberpunk aesthetic will do nothing for us. A return of cyberpunk sensibility, on the other hand, could be extremely helpful.
I think you’ll find that web browsers can, in fact, render hand-written plain HTML.
Our systems (even on relatively longread-friendly platforms like Medium) disincentivize nuance.
Our systems (even on relatively longread-friendly platforms like Medium) disincentivize nuance. They don’t need to.
Consider Medium’s ranking process, by which it sorts content for the dashboard/ feed, recommendations, and various lists. While I’m unaware of a writeup of how it actually works, it’s clear that ranking is not based purely on recency. Instead, it appears to be based upon a combination of recency & interaction (recommendations and comments) weighed by social network distance. A minor tweak that would increase the weight of full reads (and increase that weight based on estimated read time, so that an article that takes an hour to read that got five reads by people you’re following will be ranked higher than an article that takes five minutes to read but got twenty-five reads) could make short clickbait articles mostly disappear from our feeds. Instead, recommendations clearly count for much more than reads or length.
Berners-Lee had a project that did many things correctly that the web did incorrectly.
Berners-Lee had a project that did many things correctly that the web did incorrectly. It was called Enquire, and it was a simplified version of some ideas from Project Xanadu. The world wide web was created as a teaching tool for explaining the concepts behind Enquire to CERN suits, because Berners-Lee had been having a hard time explaining Xanadu concepts to them.
Early web standards also solved many problems that plague the current web. If HTTP 1.1 was implemented properly, including all of the optional features, and it was used as intended, major problems like URL inconsistency would be solved. Unfortunately, important features of HTTP 1.0 and 1.1 are not implemented by any major web server, and other features are consistently used in a way that eliminates their original function.
The web as it stands now is much less than what it could have been even in the early 90s. I’m being deliberately provocative in recommending we use only the features of the web that better in the web than in other technology stacks, but the problem of using the wrong tool for the job in order to avoid learning about the right tool is real, and the web is the wrong tool for most of the jobs it does.
I’m not arguing against progress, here: I’m arguing against using poor solutions to problems for which good solutions exist. The fact that many poor solutions are newer than many good solutions is just a side effect of the widespread ignorance of history in the industry.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
I always thought “borked” was derived from “borken”, the early 1960s era humorous mistyping of “broken” within the MIT hacker community. (While ESR doesn’t give a date or etymology, this entry goes back to the original Hacker’s Dictionary, which was compiled in the early 60s.) I never knew about this second, politically-charged meaning!
Ramifications of nearsightedness in tech
Ramifications of nearsightedness in tech
A tool that is used for everything will necessarily be a poor fit for most of the tasks for which it is used. In theory, a vice grip can be used as a hammer, screwdriver, or wrench; a mechanic who uses a vice grip for all these tasks does not inspire confidence. Somehow, this is not the case in the tech industry — whose vice grips include the web, hadoop, java, and c++.
Language wars aren’t totally pointless: it’s useful to inspect and document what tasks various tools are good at, and how best to use these tools. However, language wars at some point became a matter of tribalism, and tribal defense of the idea of one language over another for all tasks benefits no one — with the exception of snake-oil salesmen who want to sell you the panacea for all your programming woes. Such confidence artists exercise a lot of power in the industry these days.
One would think that common sense would prevail. After all, performance, man-hours, and downtime are all measurable. But, even as more and more is being automatically measured and recorded, connection to reality is increasingly tenuous: “unicorns” worth billions of dollars nevertheless have no plans for monetization, companies are created with the goal of being sold instead of providing services, and the fortunes of most players are determined arbitrarily by small gambling rings called venture capital firms. Everything’s a web service, and to the extent that they are monetized at all, they use ad-tech — a mutated version of microtransactions where an arbitrary corporation pays a fraction of a cent to a web host in return for an http request, with the value of that fraction determined by an uninformed estimate of how likely that request is to result in a product sale, and wherein that estimate drops endlessly because most requests come from robots incapable of purchasing products. As a result, the most highly valued properties are ones that no one wants, and fashions wholly disconnected from popular desirability sweep the industry. The tech industry strongly resembles modern Hollywood: dominated by expensive flops, and under thrall to its own marketing due to excessive insularity.
Another thing the tech industry strongly resembles is a pyramid scheme, or a cult.
At the very least, our industry is amusing, in the way that Sanatorium In the Sign of the Hourglass or The Penal Colony is amusing. Like most things whose value is based on faith in ideas that don’t track with reality, it probably won’t last in this state for very long. We’re in the Wiemar Cinema era of this industry, probably: Murnau can’t keep making films like Nosferatu if he binges on opium every night.
I’m not really concerned with the delusions of the elites, though. Peter Thiel and Paul Graham can make grand pronouncements, but they wouldn’t even notice if nobody on the ground listened. My major concern is that naive, lazy ideas dominate the industry even within the entry level. I attribute this to problems in education.
We have a bunch of mythology in the current industry, but it bears little in common with the earlier body of mythology and it teaches the wrong lessons. It elevates venture capital, lucky charismatic sociopaths, and companies whose value is wholly imaginary. It justifies using one tool for everything (via Paul Graham’s adulation of Lisp, didiactic standards for college curricula that suggest everything should be a Java class, the normalization of bootcamps that push you through a tutorial for a single language in a few weeks with the promise that you’ll be ready for employment). It’s the kind of mythology that cocaine-snorting ad men from the 80s would invent, because that’s who invented it.
When we eschew real history in favor of hero-worshipping Steve Jobs, we are eschewing real knowledge in favor of ad copy and flavor text. Our lack of familiarity with history allows us to imagine that the web is the way hypertext is supposed to work, putting an RDBMS on top of hadoop makes sense, PHP is an acceptable language, and new fashions in the industry are world-shaking innovations. It lets us imagine that Slack is a major improvement over IRC, Uber is the underdog fighting against foolish overregulation, smart phones are the future of computing, and hard work will make you a millionaire.
New, trendy ideas in tech are typically poorly-understood ideas from the 1970s. The flaws in these ideas that will cause everyone to abandon them in six months were published in the first response to the original paper, if not in the original paper itself. Save yourself some time, and read CS publications from the 70s. When the rare genuinely new idea appears, you will be uniquely suited to understand it.
Hiring is a whole different mess.
Hiring is a whole different mess. I appreciate the problem of necessary yet bullshit credentials. (I learned a lot from college, but very little of it related to computer science, and while I owe my current job to college, it’s because a member of the faculty used a personal connection to recommend me for an internship. So, I would have done almost as well with a fake degree.)
I have other rants on hiring, having been involved in interviews on both sides of the table several times. It’s broken, in a different but related way.
Ideally, we’d consistently have technical people interview technical hires, remove bullshit credentials as requirements, and end up undercutting the industry in bullshit credentials while simultaneously improving the average quality of worker in the industry. I don’t think that’ll happen until after the next major tech industry crash, though.
Medium started out with the goal of fixing clickbait, and the (correct) understanding that an ad-driven revenue model encourages clickbait / low-quality content.
Somewhere along the way, it decided to support ads anyway, and while early design decisions worked against clickbait, later design decisions didn’t. (As a result, everyone’s feeds got worse.)
I welcome Medium’s return to their original goal, and if the people who were laid of were contributing to the downfall of the platform, I’m glad that they are no longer in a position where they can make things worse (although it would be naive not to put the ultimate blame on top staff, and probably ultimately VCs). However, it’s sort of alarming that Medium isn’t aware of the many reasonable alternatives to ad-based revenue & paywalls, after so many years.
‘Finding’ an alternative to ad-based revenue implies that one isn’t already in front of you. In the industry there’s this idea, probably borne out of cognitive dissonance, that ad-based monetization is a necessary evil; this has never been true, since alternative proposals predate the advent of banner ads by 30 years.
The command line and GUI replaced voice as an interaction model decades ago, because for most tasks it’s easier to do it yourself than tell a secretary to do it for you. (And make no mistake, much of the time secretaries were more intelligent and capable than the people who commanded them.) Until the advent of weakly superhuman artificial general intelligences, the absolute best case for voice and natural language as an interaction model will be strictly worse than instructing a human subordinate.
The sorry steady state of most large organizations (the SNAFU) is the direct result of the flaws of natural language as an interaction model. As an organization becomes more close-knit and efficient, jargon proliferates and language becomes more exact for things that matter, until what people speak to one another resembles one of the less well-thought-out programming languages (perl, php, basic, fortran). In other words, the end goal of any natural language interface (even between humans) is to become a command line interface.
Of course, between humans, the learning curve is hidden because it is merged with the organic emergence of a shared cant. But, this hiding of the learning curve can only be performed once: people who come later to a community must learn the language of that community. Why not standardize, when we can? After all, machines aren’t very good at improvizing collaboratively generated vocabularies based on implication but are very good at consistently interpreting the same conventions the same way (and copying those conventions exactly to other machines).
For Medium, the most sensible monetization policies would probably be:
• microtransactions from readers to authors (where Medium takes a cut whenever anybody wants to cash out, & otherwise holds cash in escrow). (This is essentially what Adsense, Mechanical Turk, and Second Life did.) • pay for privacy (a variation on freemium used by github, wherein normal use is free but paid accounts can whitelist or blacklist posts to specific audiences — making it possible to treat medium like a private forum)
Users of Medium can already use something like patreon, but Medium could potentially benefit from integrating something like SPP (an ancestor of patreon) and taking a cut. In SPP, a work is created but only released to the public when a certain threshhold of funding/donation is passed; since Medium is a publishing platform, they are in a unique position to hold a piece of writing in escrow until the threshhold is passed. (I like SPP in concept, but of all the variations I cover in my overview piece, it’s the only one that hasn’t really been properly implemented, and the idea may be a little too alien to most users.)
Medium already uses a freemium model to some extent, with publications having greater control of color and layout. This by itself won’t keep them afloat, though. I think if they kept publications as their freemium model & added microtransaction support (maybe even just as an author option in the licensing menu), they could do significantly better than they would with advertising.
If full support for microtransactions is too scary for them (since it would require casual users to put money into the system), another possibility is to integrate submission payment into Medium itself. Some publications pay for submissions, but perform draft submission & editing via Medium itself; the only portion of the process that’s done outside of Medium is the payment. (I’ve been paid for submissions and had a ~4 month wait between publication and payment because of overseas wire transfers.) If publications are already paying into Medium, giving users all an account balance & building support for basic publication agreements into Medium itself would actually streamline the process enough that Medium could justify taking cuts comparable to what PayPal takes. (And, while Medium holds on to all that borrowed money they can keep the interest that accumulates.) Doing this might be a first step toward supporting microtransactions on a larger scale.
I’d definitely say the web is a bad tool for most of the things we use it for.
I’d definitely say the web is a bad tool for most of the things we use it for. In particular, the technique of using javascript sitting within a document to dynamically edit that document’s structure & CSS attributes is a particularly inefficient way of building a user interface, and without the heavy optimization being done by browser maintainers any such application would be unusable. (As much as I dislike Java, I have to admit that applets are actually the appropriate solution here.)
(Let me say that I like and appreciate that you *can* perform hacks like dynamic web pages; I’m just horrified that so many people think you *should*. Dynamic web pages are, essentially, self-modifying code. Anywhere else, we would be very wary of that.)
HTTP specs provide response codes that address some of the problems related to addressing documents via their position on machines. (Doing this rather than using CAN is a mistake in the first place, but TBL was working with a slow network before ideas about CAN were very widespread.) If we were to try to implement up to the original spec & use that spec to get as close as possible to traditional pre-web hypertext best practices, we would make the content of each page static (no CGI), keep track of where every document is at all times, use temporary and permanent redirect response codes if a document moves, never have two different documents (or a modified version of the same document) with the same full URL, and only use a 404 return code in the apocalyptic case that somebody simultaneously nuked your server and all your backups (or in the case that a document *never existed*). Today, these codes are not used that way.
(HTTP also specifies some very useful functionality that is very rarely implemented. Specifically: HEAD requests to get the length of a document and special variations on GET requests that return a span of bytes, given a starting offset and length. Such features are very useful in an environment where documents are static but potentially very long. Having worked on code intended to perform transclusions from arbitrary URLs, this feature would have drastically simplified my efforts.)
Right now, the most promising competitor to HTTP is IPFS — a CAN-based file transfer system. It doesn’t solve the problem of servers going down permanently (there is no automatic redundancy), but popular files will outlive short outages because requested files are served from cache (which also saves bandwidth upstream). IPFS avoids CGI-style hacks, but you can still serve arbitrary HTML/Javascript blobs, so it’s not as though it breaks that functionality. (Again, I consider using javascript to manipulate HTML messy and slow.)
My main complaint with web apps is that very few things benefit from being on port 80 & being stuck inside HTML. An applet, because it’s essentially a native application that has been sandboxed, doesn’t have the problem of needing to draw using drawing routines intended for dealing with messy hand-written markup, nor does it have the problem of working against a set of quirk rules organically grown to protect against 20 years worth of random but very specific attacks on web browsers. An applet can use standard protocols, instead of depending on services that are exposed over HTTP wrappers. (HTTP-exposed APIs take the rule-breaking of CGI — which, remember, discouraged people from using redirects properly and led to the current user-hostile behavior of redirects; they are additionally generally less efficient than existing standard protocols.)
Applets have gotten a bad name, because most applets are either Java or Flash, and both of those platforms have a history of being quite awful. But, there’s no particular reason that an applet viewer for javascript, or lua, or smalltalk couldn’t exist, provided that someone decided on a decent standardized drawing model that properly supported all the various things people like to put in GUIs.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
The Value of Subtle Communication: Toward a Secular Materialist Model of Chaos Magick
The Value of Subtle Communication: Toward a Secular Materialist Model of Chaos Magick
[1]
Where occult traditions typically anchor themselves firmly to the metaphysical, chaos magick has historically been open to more materialist interpretations, although inconsistently so. I do not claim that metaphysical understandings of magic are necessarily wholly invalid, but I would like to suggest that most if not all of the major practices associated with chaos magick do not require or benefit from appeals to ideas outside the scientific mainstream. In other words, the practices of ritual (particularly sacrifice) and sigilcraft (including hypersigils, etc) are fully compatible with a conventional scientific, secular, materialist worldview.
The cornerstone of my argument is the idea of subtle communication with oneself and others as a means of influence.
[1]
By subtle communication, I mean: communication that bypasses conscious, verbal modes of understanding. This is not to say that such forms of communication have no verbal component; only that the most important aspects of these communications are not, to the target, immediately understood in terms of the most obvious and direct interpretation. Subtle communication in the domain of writing includes subtext, wordplay, irony, symbolism, oblique reference, the literal content of metaphor, and emphasis; in speech, we can also consider changes in meaning and emphasis created by timing and modulation; when speaking to someone in person, facial expressions and body language constitutes high-bandwidth subtle communication mostly hidden from us but transferred by unconscious mirroring behavior. Regular speech can become subtle communication if the target is unable to distance themselves enough to objectively analyse it, either because it becomes internalized before it is fully understood (as with song lyrics), or because something is interfering with verbal processing (as with the repetition of creeds or mantras — jamming the verbal processing facilities — or when the target is tired or under the influence of various drugs). Subtle communication has lower bandwidth and a greater error rate than overt communication, and it is often missed or dropped, but it also bypasses many of the mechanisms that might work against some goal, which makes it an ideal way to circumvent forces that would work against a more direct attempt.
Seen in this way, sigils (when directed at the self) operate a bit like affirmations. A sigil is a reminder of an intent, but one that is oblique enough that it avoids triggering self-defeating behaviors. When sigils are directed at others, they retain less of their meaning, but some subtle communication is still achieved (with the amount depending upon the quantity of shared understanding and assumptions between the creator and the target). We can expect only a small effect from showing our sigils to others, at best, unless we give them enough context that it is comparable to their having made the sigil themselves; if we share enough culture with the others, however, subtle communication from them can reinforce our own goal-seeking behavior, like a smaller-scale version of publicizing one’s to-do list or new year’s resolutions. When such pressure is small and subtle, it doesn’t overwhelm.
Rituals abstract our goal in much the same way that sigil creation does. The pressure is higher because of sunk cost: rituals are costly signals to the self about the importance of achieving some goal. The time and effort turned over to a ritual is itself a sacrifice (and one that is properly set in our minds, as opposed to a pathological case like gambling wherein sacrifices are disguised as investments or amusements), and an actual sacrifice increases the cost. Group rituals bind the group together to a common goal: they all made the same sacrifice, and would like to avoid the cognitive dissonance of having wasted it, so they must achieve their goal in order to justify their sacrifice.
Books like Mind Performance Hacks produce rationalist-friendly “life hack” versions of these practices, with the occult terminology stripped out. Rather than sigils, print out a sheet with affirmations or themed words in order to encourage particular primed responses! Rather than rituals, make a betting pool with your friends about the success of some project, or vow to donate to a charity you hate if you fail! By embracing the verbal mind, these practices open up the door to endless second-guessing, and thus to self-sabotage. I would recommend the chaos magick versions of these practices instead. Self-sabotage seems likely, particularly if you don’t respond well to direct pressure.
With hypersigils, the mechanism of action is even easier. People model their worldview mostly on art: experiences provided by art are easier to find and consume than experiences provided by life, and are also safer. A hypersigil takes a naturalistic view of the world and adds elements to it that encourage emotional investment, and then slowly modifies it in ways that correspond to some intent. The result is that the creator and audience both have their world view modified by the creator’s intent. When the audience has adjusted expectations, it becomes more likely that they will manipulate the world to fit those expectations, as well as communicating their expectations to secondary audiences.
As much as I’m loathe to heap praise on Second Life & Linden Labs, I think they at least got this model right: they handled transactions between users that would be occurring anyway, and then allowed people to put money into the system & take it out.
When a user views a web page, a transaction implicitly occurs. Medium knows about both sides of this transaction. (With ads, the transaction is offloaded to one between two unrelated third parties.) It’s not unreasonable to make such a transaction explicit and broker it without third parties. If everybody starts off with enough credits to handle the whole lifetime of a very casual user, then the monetization will never affect normal users, which is a big hurdle.
Flattr is a lot like paypal donate buttons & patreon, in that it’s an add-on that individuals put on top of a service to provide monetization the service doesn’t provide by leaning on the good will of individuals. Such systems sometimes work well, but you have to lean pretty hard on the NPR model of begging for donations in order to use them.
If you already manage the content distribution system, allowing casual users to automatically pay money they didn’t know they had (with the knowledge that most of that will never fully circulate and that which does circulate can be more than made up for by minimum balances for cashing out & fees) lets you put the system in overnight without anybody noticing or caring & then only charge very heavy users. (If the costs involved are small enough — and if they are on the scale of ad revenue, they are tiny — heavy users will not feel like it’s a burden on them to continue using the platform.)
If we can do this (and it’s certainly technically possible), we can break siloization by doing the opposite: specifically selecting and recommending stories from non-intersecting social groups with related tags.
Personalization is great at giving people what they want, but when it comes to giving people what they need, we aren’t using all the tools at our disposal.
This fits into a wider pattern.
When the creators & consumers have aligned worldviews & interests, allowing creators to perform cognitive labour on behalf of consumers makes sense. When their interests are not aligned — when the media or technical landscape is adversarial to the users — then any simplifying assumptions made by creators are at best ideologically suspect.
Many of the major growing pains related to the internet (and particularly the web) essentially come down to an “eternal september” moment where a set of technologies designed for hobbyist use in a a community with relatively aligned interests gets inserted into a commercial context where multiple adversarial parties are involved. (Spam, clickbait, fake news, intrusive advertising, all manner of security problems ranging from social engineering to sql injection, ‘dark ux patterns’, 419 scams, and trolling can all essentially be blamed on giving a system designed for good-faith cooperation to a bunch of people who would rather con each other to gain small advantages.)
Societies have an array of tools for limiting the damage done by bad-faith actors. Unfortunately, the cruel optimism of the people who design online communities either undercuts these mechanisms (in reality power structures are very conditional, since subordinates who lack trust in their superiors’ judgement will ignore orders; in computer systems, power structures are treated as much more cut and dry, compounding mistakes made by the powerful) or expands their power beyond what is reasonable (as with the human flesh search engine & other mechanisms that pile on shame out of proportion with the original failure).
Recently I’ve seen what looks like an upswell in the general understanding that the world is complex & can’t be easily understood or modeled, which makes me a little more hopeful for the future. However, easy solutions (even if they are wrong) can be very lucrative. Any system we design should be mindful of how it presents information & how that presentation effects society.
Agreed: lack of perspective is the real M.
Agreed: lack of perspective is the real M. O. here, at least with people who have theoretically aligned goals.
I think capitalism creates a big persistent bias, though, because it’s one way in which the value of transmitting information can be made wholly independent of the truth or utility value of that information for the recipient: the entire practice of advertising consists of aggressively telling half-truths for the sake of causing people to behave in ways that are against their interests, and that’s probably the simplest case. When you add futures markets, you begin to have situations where you can make a great deal of money by moving information around that has no relationship at all to reality.
People can learn to consider others whose experiences are unlike their own, but they are far more motivated to put in the effort if their livelihood does not depend upon preventing those people from succeeding.
Belief is the enemy.
(It would be nice if information systems were not actively antagonistic to most people. This is not the information landscape we live in. I blame capitalism.)
While large corporations have the resources to do large, cross-cutting messages, the profit motive severely limits how effective they can reasonably allow themselves to be. Ultimately, a corporation cannot take a strong stance unless they are confident that most of their customers already agree with that stance — to do otherwise would be suicidal — and so they are obliged to lag behind the rest of society, tending to be more conservative & to accept regressive ideas for longer simply out of fear of lost profits.
Of course, they can avoid the risk by micro-targetting advertising & making sure they always preach to the choir — but then, they can never have a positive social impact (and may actually have a net negative social impact).
In other words, the profit motive inherently conflicts with the arc of history. (Social progress depends upon upsetting the comfortable and comforting the upset — in other words, on sacrificing the favor of the powerful in order to redistribute power more equitably — and this means real negative impact on one’s power and wealth in the short term with no guarantee of greater power in the long term.)
Untargeted advertising, for now, does cross boundaries. But, untargeted advertising reaches fewer people than ever. Large events with huge audiences like the superb owl are the exception, not the rule: who even has a TV anymore, or a cable subscription, or purchases paper magazines?
It is possible, for someone with plenty of resources and an interest in social justice, to engineer targetted advertising intended to change the opinions of specific audiences — not by having a shared ad experience that poorly targets everyone & tries to make a social statement while also shilling for a product, but by addressing each niche in terms they understand.
Unfortunately, the only values shared across most of the world now are the ones that all advertising implicitly shares: the idea that money can be traded for happiness, and the idea that there is no alternative to a system based on the sale of labor.
On the web, size matters
On the web, size matters
The web has a problem. Most web sites (weighted by volume of traffic) are made by and for wealthy able westerners with fast computers and fast connections, and are borderline unusable by anybody outside that group. What makes this a problem is that these websites are inaccessible for stupid reasons.
Web designers have adopted a cargo cult programming mentality. While cargo cult programming in real languages mostly just makes code hard to read for other programmers (idempotent imports, shared libraries, & the removal of unused symbols limit bloat at runtime), on the web bloat accumulates quickly. We use third party pixel trackers for analytics (often several different ones), CDNs for displaying static text (and RDBMSes for storing static text), CSS for styling (and JavaScript for modifying the CSS, and JavaScript for modifying the HTML, and JavaScript for modifying the other JavaScript), and we use automatic generators for building structures that would be less effort to write by hand. We force styles and behaviors on users based on our large screens and fast CPUs and hoards of RAM, and the users (if they are sufficiently savvy) fight back with extensions that chop out portions of our websites based on lossy heuristics.
We don’t need to be at war with our users, and shouldn’t be. Rendering a blog post shouldn’t involve twenty HTTP requests, a bunch of JavaScript, multiple draws (as new styles override previously loaded ones), and downloading as much content as the original Doom. Choosing bloated cargo-cult methods are, essentially, discrimination: discrimination against anyone with slow internet (i.e., most of the world) or slow CPUs (anyone who didn’t upgrade their computer in the past five years — i.e., the middle class). Too much forced style information (or too many widgets and sidebars) constitutes discrimination as well: against anyone who is blind (and thus must listen to every label and alt-tag in the order in which they occur in the original HTML) or has poor vision (and thus requires higher contrast and larger fonts — ruining any kind of overly-precious color or layout dickery). Fancy style and layout is an art, and has its place, but its place is in print magazines, where style doesn’t actively defeat usability.
It’s ultimately up to you, as a web designer, whether or not you want to exclude these groups. (Many things developed in the Valley are ultimately absolutely useless outside the Valley, and people often have no problem with that: it’s where the money is.) But, if you think reaching a sufficiently wide audience is worth making the occasional design snob feel scandalized, here are some tips:
• Avoid third party trackers/analytics. They are selling you information from their access log, and you have your own. Processing your own access log saves every user an extra request. • Don’t host ads. They won’t make you money anyway, and each one means at least one extra request — usually more. Savvy users will block ads, and less savvy users will thank you for not hosting them. • Where possible, use static HTML. Static HTML is small and fast; CDNs are big and slow. If your site is entirely static, you can use a specially priced plan from a web host that doesn’t include RDBMS access or server-side scripting. (On top of this, static HTML associated with consistent URLs will be properly cached by browsers — meaning that repeated views will not lead to repeated requests. If you pay for bandwidth, this saves you money.) • Minimize style. CSS takes lots of resources to process. Fancy CSS still isn’t consistently rendered across browsers, and is likely to break spectacularly if you drastically change scale or selectively override certain elements (such as font sizes). External CSS, while more flexible, also requires extra requests. If you stick to one small external CSS file — or better yet, avoid using any styling at all — your users will save bandwidth and rendering time. • Fallback gracefully. Under load (on client, server, or network), you can expect JavaScript or CSS to fail to be processed: it might fail to download, or it might take too long to render. Some users can’t make use of it at all (for instance, blind users with screen-readers) or disable it for performance or security reasons. Constructing a website that looks and behaves as close to correctly as possible when only the HTML has loaded will make these users feel confident in your work; a website that looks broken without CSS or JavaScript seems unreliable (and CSS and JavaScript will fail). • Use images only when necessary. Most images used in the design of websites (and even many images used in post bodies) are for primarily aesthetic purposes. However, image loading should not be expected to be reliable: after all, each image is yet another request. In headers and sidebars, consider using text & minimal formatting rather than images, particularly when usability would be more impacted by the failure to load images than it would be by having text in the first place. (Again, some users will have sporadic image loading failures, and others will simply never see the images at all.) In posts, consider whether an image serves a real purpose: would you make your point better if you took more care explaining it? An image should only be included in an informational blog post if the amount of text it would take to adequately explain its content would be larger than the image itself. • When possible, write your website by hand. Generators can save programmer time when doing something fancy, but fancy websites are fragile and generators can create very bloated code. It’s not hard to write simple HTML and CSS, and writing sites by hand discourages bloat. A website written by hand by one person will, generally, be small enough to load quickly on even a very poor connection. (If you require headers and footers, or if your content is highly structured and repetitive, I recommend writing your own specialized simple generator, rather than taking some off the shelf templating engine or CDN and configuring it. A three line shell script can produce small, fast, reliable, and beautiful websites in a way that large systems like WordPress struggle to, and even a beginner programmer can write such a generator.) • Write for usability, not for looks. A visually impressive website is rarely a usable website, because the concerns are very different. Unless your target audience consists solely of design snobs, you are better off making sure your site loads quickly and is easy to use. Don’t be afraid to make it visually uninteresting: your users will thank you for making navigation easier. • Keep it simple. Introducing new toys is always tempting, but those new toys interest you much more than they interest the users. A fast-loading site that does what it’s supposed to and nothing more will be more useful than a slow site that performs flashy but unnecessary tricks.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
If you are selling your site to design hipsters, I have no problem with the bloat: they have plenty of money and plenty of bandwidth. If your site is intended for anyone outside that community, usability is paramount. Nobody leaves a useful site on the basis of looks, but they absolutely will leave a useful site on the basis of bloat; if looks are the determining factor for your site’s traffic, then it doesn’t serve a useful purpose.
A piece of JavaScript that loads a link to a properly-made site once it has detected that the bloated version has failed is yet another piece of bloat that can be expected to fail. Why not default to the clean site and provide a link to the fancy one?
It’s perfectly possible to build a clean, good-looking site without relying on large amounts of complicated CSS and JavaScript. Certain behaviors are not possible without these mechanisms, but these are behaviors that should be avoided when possible because they cannot reasonably be expected to work for most users.
The religious right is a strange beast, with strange internal contradictions mirroring the greater alt-right that it’s beginning to be subsumed into. Broadly, we can consider Trump supporters to be a diverse and internally schismatic temporary coalition of authoritarian-leaning pro-corporate nationalists, who agree on very little else. (After all, this group includes the religious right — who combine a shallow reading of already-shallow objectivism with the prosperity gospel and consider success in business to be equivalent of the mandate of heaven — and techno-libertarians — who consider business acumen to be an unbiased representation of general skill and promote it as a welcome break from all that religious crap — and neoreactionaries — who would like to import the authoritarian hierarchy of an idealized corporate structure into government — and accelerationists — who want to be as capitalist as possible so that they can bring about a marxist utopia by using up all the capitalism. None of these groups quite fit properly with normal white nationalism, nor with any of the various flavours of fascism going around, nor with the military-industrial complex, although all these groups are scratching each other’s backs for now.)
The web is a glorified marketing platform, sure, but do we want it to be?
I certainly don’t — even if engaging in this kind of thing made me a lot of money (it doesn’t, and won’t, because marketing is a race to the bottom and supplying free services as a loss-leader for projected sales of products users will never buy is not a sustainable business strategy), I would not engage in it.
A site that doesn’t provide something useful for its end users doesn’t need to exist and should not exist, even if it makes money for its owner. A feature that is not useful for end users does not need to exist and should not exist, even if it enables the developer to extract capital more efficiently. The fact that most of this money is essentially hypothetical anyway — based on fantastical projected earnings from unsustainable practices (advertising among them) — makes the entire project much more dubious.
If you’re not getting visitors at a rate comparable to Boing Boing, you aren’t making money from advertising (you’re just handing ad money to Google in escrow, which you will never be allowed to extract), so you will never benefit from practices that were standardized based on an ad-based revenue model. You will therefore be not only better-liked by users but actually more financially successful if you pursue an alternative means of extracting capital. (For instance, diving for change in other people’s sofas.)
Details of how this subscription model will work seem scarce.
Details of how this subscription model will work seem scarce. I can hardly make a decision based on them.
I love Medium right now. I love writing for it and I love reading it (despite some problems with keeping content quality consistently high). But, the service Medium provides isn’t one that’s difficult to implement: I expect that, as soon as Medium locks itself up, five or six slightly-crappier competitors will pop up from people who didn’t take ill-advised loans from VCs and who decided on a revenue model early, because we’re essentially talking about a blogging platform with minimal customization, and reinventing that wheel is done all the time. Whatever model Medium decides upon will need to benefit me more than switching to a competitor will.
Let us consider some possibilities:
• It may be that Medium will require subscriptions from all registered users, and not provide any revenue redistribution. (This will probably eliminate some casual users and change the cost-benefit analysis of people who currently use the platform for marketing purposes. If this gets rid of low-quality self-help spam, it may be a good thing, but if it scares away people who are currently writing high-quality content — as it probably will — then that may lower the value of a subscription even more.) I’m assuming, under this model, that unregistered users may still read medium posts, but that recommendations, posts, and comments are premium. • Perhaps everyone pays for subscriptions (as in the previous possibility) and there’s no paywall for readers, but authors get some direct payout (from subscription fees) based on number of reads. This would make a lot more sense: Medium is just a platform for distributing user-generated content, and making the users pay to generate content is nearly offensive, but making Medium a subscription-based marketplace for content cuts down on low-quality/spam posts and brings back authors who write high-quality content & are likely to more than break even on what they write. (There are some complications here. Like, DMCA takedowns become a lot more important, and fraudulent takedown requests might start becoming an issue. What happens if a user from Australia posts an extensive quote from Lovecraft, whose works are public domain in Australia but under copyright in the US? Medium presumably already has good lawyers and policies on this, but the stakes are higher when you’re paying authors.) • Let’s say that not only do registered users subscribe, but readers are also shown a paywall (perhaps after a certain number of monthly reads, like on newspapers). This is basically a no-go: the number of people who would put up with this is tiny, and discovery and PR goes down the toilet. Unless Medium officially states that this is what they’re doing, we should assume this is a straw-man version of their model (even though theoretically intelligent people like those in charge of the New Yorker also do this). • Consider a freemium model with multiple tiers. The free version would be what we normal users are used to; higher tiers are optimized for publications, with features like configurable publication pages and configurable post themes, promotion in the newsfeed, and placement in a special list in the sidebar. This might work out for Medium, and might work out better than the various alternatives, if they got the prices right. (After all, some publications are funded essentially with marketing budget for something else in print — How We Get to Next, for instance — and they would be willing to pay a pittance to look better on Medium because their revenue stream is from elsewhere.) But, if the prices are too low, current spammy posts will dominate everyone’s feeds (and regular users will leave), while if the prices are too high, Medium will continue to not break even.
I think subscriptions are the second-weakest business model (after advertising) for Medium, but there are ways to make it work. If they make it worth my while, I’ll subscribe. But, for the current levels of content quality, I probably wouldn’t pay more than a dollar a month.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
Has this only been rolled out on mobile? I don’t see anything like this on the desktop site.
If you want to get a job in tech & be qualified for it, you need to first be self-taught and then get a degree. Without being self-taught you will not be competent (because a four year degree program does not and can not provide the prerequisites for competence in a field like this), and without a degree many (though not all) businesses will not consider you.
When the best option for providing a service is awful, that indicates an opportunity for competition. (Of course, network effects make this harder in practice — though Facebook was able to defeat MySpace which defeated Friendster basically all on the strength of superior UX.)
As much as I hate the term ‘disruption’, if there’s one thing that can and should be disrupted, it’s something like LinkedIn: i.e., a dysfunctional system that performs morally dubious activities, backed by an ancient giant, that continues to be used essentially because no superior competitor exists but barely provides a useful service to anyone except recruiter-spam companies. As long as places like LinkedIn continue to exist and continue to be actively used, their worst habits are normalized. And, normalization of bad ideas is the death of most things on the web.
Here’s the thing about the web: everything, service-side, is incredibly cheap compared to other industries. (Except engineer time, I guess, but that’s systematically overvalued in a weird way, and part of the problem.) Bandwidth is next to free, compared to sending out the same amount of information by mail or telephone; storage is next to free, compared to keeping the same amount of information on paper and renting warehouse space for it.
As a result, we’ve allowed ourselves to get into lazy habits and rely upon really marginal ideas for much longer than is justified: the money keeps flowing, whether or not we screw up. So we keep doing things that never made sense in the first place, like ad-driven revenue for websites, using super wonky / insecure tech (php, perl), using bloated storage formats that are difficult for humans and computers to read (300GB XML blobs), and coding as though performance and size don’t matter.
It’s been more than ten years since Moore’s Law stopped, and high speed internet providers have been avoiding upgrading their infrastructure in favor of consolidating local monopolies for at least as long, so the shit is about to hit the fan on performance across the board; meanwhile, gaming ad-based revenue and breaking poorly-secured systems have graduated from hobbies to industries, so the shit is already hitting the fan on that.
We could never really afford to do what we’re doing, but now we can’t even afford to look away.
One way to avoid people using eponymous laws to pull a bavarian fire drill on you is to produce a list of equally pithy opposite positions & credit them to arbitrary famous figures. Neutralize the though-stoppers.
The only response I can think of for this is that, while Putin almost certainly wants to think of himself this way (being ex-KGB and all), true Xanatos gambits aren’t possible in real life. Every plan has holes, because there’s too much complication.
In other words, at the very least, we’re looking at a combination of planning (particularly, the construction of double-binds), improvisation, and luck. And, it’s not as though double-binds of this type are impossible to engineer: it doesn’t take a strategic genius, or someone with near-total political power, to produce them.
(The other thing to note is that the weak spot in any strategy is randomness. This isn’t to say that randomness will always win, but instead that any system will eventually succumb to fuzzing if the fuzzing can be sustained for long enough. Placing someone unpredictable in power is dangerous simply because you never know how they might undermine your plans — but that risk is spread out across all possible plans.)
The hype around javascript is representative of a more general and widespread problem, which is that inexperienced programmers make poor design decisions based on familiarity and ease of initial development. Javascript is popular for exactly the same reason that PHP, perl, and java are popular: it’s an extremely common first language, an extremely common second language, and there are certain kinds of tasks for which it is not poorly suited as a prototyping language. People use javascript for the same reason that they use HTTP: it’s the first thing they think of, and they don’t bother to analyse whether or not it’s an appropriate tool.
Did somebody unload a time machine from 1998 into this comment thread?
Did somebody unload a time machine from 1998 into this comment thread? All serious development is done in open source, while proprietary code is at best a toy — and this has been the case for decades.
Here’s a heuristic: if a task is made significantly easier by a framework, it shouldn’t be done in the browser (and probably shouldn’t be done in JS at all).
I’m not familiar with anything Nelson might have written on the storage of navigation histories beyond inventing the back button (though I’m very familiar with his work with versioning & with bidirectional hyperlinks). Do you have a URL for what you’re referring to?
I’m not the author of the original piece.
A modest proposal related to firearm safety
A modest proposal related to firearm safety
There are several major related social problems in the united states right now: a political division along geographic lines, a set of self-reinforcing communication barriers along that division, an asymmetry of firepower along that division, and a set of minority groups whose acceptance is asymmetrical along that division. I propose a programme, along the lines of the mandatory short-term military service found in Scandanavia and Israel, to directly address all of these related problems. I expect it to offend all major entrenched interest groups roughly equally.
To summarize the problem: broadly, we have nominally ‘left’ and ‘right’ cultures — the blue and red tribe, respectively. The blue tribe is more common in urban areas while the red tribe is more common in rural areas (although the strength of this geographic division is debatable, and better debated elsewhere). More importantly, the blue tribe aligns itself strongly with social justice for marginalized groups yet is largely opposed to mechanisms by which such groups might defend themselves, and even in areas that are blue-tribe-dominated, weapons and minor positions of political-executive power (such as placement in law enforcement) are red-tribe-dominated. All of these associations are accidents of history: movements like the Black Panthers show that embrace of weapons for self defense are no less compatible with both left-leaning politics and a position as a persecuted minority than they are with white male right-libertarianism.
I propose a programme by which, in early adulthood (say, age 20), all people who are not too ill to participate would be required to attend two months of intensive firearm safety and training. Locations are assigned at random, rather than by convenience: after all, transportation will be provided by the government, and we would like to use this program to acquaint people who are from different geographic regions and who have different perspectives on life. These training programs will not be segregated by gender (including sleeping arrangements). Because this training program is non-military, there will be no religious or moral exemptions. At the beginning of the program, each participant will be issued a firearm, a portable gun safe, and a gun lock; at the end of the program, participants keep their firearm. (After the end of the program, they may of course sell their firearm, or discard it.)
Immediate results: all minorities are armed with a suitable weapon for self defense and a legal justification for having it (so a law enforcement officer can no longer consider being armed as sufficient for considering a POC a threat); people outside a general culture of firearm safety (like that which exists in the red tribe, where children are drilled on firearm safety from a young age by their whole community) are introduced to it, limiting the rate at which people outside a culture of firearm safety make dangerous mistakes when they are exposed to firearms; people from all walks of life are guaranteed at least one experience with living for an extended period with a group of people very different from them in the presence of deadly weapons without violence.
I realize that your post isn’t meant to be a coherent argument, but — the only way we can tell that an idea is good is by trying to kill it and failing. To the extent that a writer is a curator of ideas, we benefit from taking the same kind of aggressive approach toward ideas as we do toward words when editing.
Some less modest provisions:
• Those who are in prison at the time that they are being called up for this course will attend it, but their firearm will be held for them until they are released. Since this program grants a firearm license, current laws prohibiting felons from owning firearms will need to be overturned. (After all, most felons are not violent offenders, and even violent offenders have incentives not to re-offend. We shouldn’t allow prosecution to become a fig-leaf for preventing minority groups from defending themselves.) • Participation may be delayed for mental health concerns, but not cancelled. A jury of twelve mental health professionals, chosen by lottery, must come to a consensus that someone is not psychologically fit to participate in order for this delay to occur. The delay may be no more than a year. This is a preventative measure against the tactic, used both in the United States and the Soviet Union, of applying bogus psychiatric diagnoses to political or racial ‘undesirables’ in order to silence dissent.
Slowly creep backwards while speaking more softly and with a greater number of increasingly-long pauses. If your partner doesn’t attempt to follow you, the conversation is over and you may leave.
Spanning problem-space
Determining how best to improve one’s own suite of mental models & thinking tools is a hard problem: we can’t easily see ideas beyond the horizon, and ideas we haven’t yet invested effort in developing are distorted at best, but determining the value of ideas is necessary because of the scarcity of time & other resources. This is further complicated by the fact that knowledge-seeking is not a single-player game: everyone is constantly refining their suite of mental models, making decisions based on them, and producing material that makes certain ideas more or less accessible, and the value of a mental model is determined in part by the people who share it or share adjacent models, in somewhat complicated ways.
My current idea of how best to improve the value of one’s suite of mental models is based on a couple assumptions:
1. Ideas are adjacent to each other in semantic space based on shared attributes. 2. It is easier to learn an idea if it is adjacent to an idea you’ve already learned. The ease with which an idea is learned is proportional to the number of adjacent ideas already learned. 3. Adjacency in semantic space, seen as a network, is a web, not a tree. Some ideas are adjacent to each other even when none of their immediate peers are adjacent — such as when seemingly unrelated ideas in seemingly distinct fields have striking similarities. 4. A factor in the value of an idea is its adjacency to other valuable ideas. Part of this is ease of communication: when we have a shared terminology and set of assumptions with people, we can share new ideas more easily. When we share few ideas with someone, communicating with them is difficult. 5. Another factor in the value of an idea is its concrete utility, in of itself. For instance, the set of ideas known as ballistics are very useful in predicting the movement of objects. 6. A third factor in the value of an idea is its scarcity. Someone who is an expert in an obscure field will have greater social capital than someone who is an expert in a more commonly-understood field with the same concrete utility adjacent to ideas of comparable value. 7. Some ideas have as their primary concrete utility the capacity to change the value of other ideas by changing something about society. Rhetoric, for instance, can be used to modify ideas about the value of certain other ideas, thus changing things like salary and social capital. 8. Adjacent ideas are not always obvious. Sometimes they are only obvious in retrospect. 9. Adjacency doesn’t necessarily have any relationship to truth or intent, although systematic biases (including toward truth or toward consistency) may favor clusters of similar ideas. For instance, mathematics, because it enforces consistency, finds large numbers of similar patterns in far-flung contexts. 10. Traditional (tree-like) academic paths through idea space are easy to traverse in part because so much effort has gone into lowering traversal effort — the production of teaching material, specialized terminology, and communities and social structures (such as universities). That same ease of traversal lowers the value because it increases the number of people with nearly identical mental toolkits. 11. Autodidacts trade the easy-to-traverse yet diluted conventional path for unconventional connections of unknown value. They risk missing ideas that are relatively hard to pick up without structural aid but that are very useful for opening up further vistas or closing off dead ends (like calculus, or godel’s incompleteness). 12. Successful autodidacts are polymaths. Unsuccessful autodidacts are cranks. It’s hard to tell the difference without mastery of related fields. 13. Undiscovered or undocumented adjacencies between seemingly unrelated subjects are common, but few have much concrete utility. However, those that do are extremely valuable. 14. As a result, someone can optimize the value of their mental toolkit by following traditional paths enough to enable communication but otherwise specifically choosing to persue seemingly unrelated subjects that are rarely persued together, periodically attempting to synthesize them. Random number generators are useful in path choice and synthesis, since the likelihood of producing an unconventional path and the likelihood of choosing paths with hidden adjacencies are both high.
It really is better to ask for permission
A problem with slogans is they get adopted outside of contexts where they make sense, either because the people using them didn’t carefully consider whether or not they were true, or because they provide an excuse for doing something that would otherwise not be allowed. Where a monoculture exists, with a group of people with similar values, culture, resources, and problems all make decisions based on the same assumptions, inappropriate slogans can cause systemic biases. The worst offender I’m aware of right now is “it’s better to ask for forgiveness than ask for permission”.
I have no particular interest in tracking down who said this first and in what context. I will give it the benefit of the doubt and assume that initially it was said in a context where it was true — improv comedy, maybe. However, like similar slogans like “move fast and break things”, today it is used almost exclusively in situations where it is not only untrue but also actively harmful.
In order for it to be true for it to be better to ask for forgiveness than permission, the following must also be true:
1. The stakes must be low — in other words, mistakes must not be very damaging (or else apologizing wouldn’t be enough) 2. There must be a single homogenous party from which to ask forgiveness (or else asking for forgiveness wouldn’t make sense, because you would never be forgiven by all concerned) 3. Asking for permission must be more difficult or risky than getting forgiveness — in other words, the party from which you ask permission must be conservative about it 4. Success must be likely (or else you would be seen as a perpetual screw-up for following this advice) 5. Performing the task must be its own reward — in other words, you must see even the failure as valuable
How this is interpreted, however, hinges on how we define ‘better’. When people say this in the tech industry, the most charitable explanation is ‘better’ means ‘better for my paycheck’ rather than ‘better for users’ — in which case, one can expect to apologize to one’s supervisor, not one’s users, if one takes down all production systems for a week.
While this is charitable, it’s hardly an endorsement. None of us should feel better about losing service on the grounds that the developer at fault was forgiven by his supervisor. Any system with users cannot afford the kind of unreliability produced by lack of oversight, because the real stakes are much higher than a single developer’s paycheck.
Ultimately, the attitude embedded in this slogan, taken too seriously, is at the core of many of the worst behaviors associated with the tech industry. Uber repeatedly violates labor laws on the grounds that it can get away with doing so — and asks forgiveness when sued. The creepiness of redpillers and PUAs mostly comes down to the entitlement they feel toward other people’s bodies, assuming they don’t need to worry about consent beforehand because they can ask for forgiveness after they’ve gotten what they want if in retrospect consent wasn’t given. More concretely, this attitude produces shitty dysfunctional code, short-lived companies that sell user data and disappear suddenly, ethically-dubious business practices, and a general culture of “I’ve got mine, bub” that directly contradicts the phony “save the world” PR everyone likes to wear.
Here’s the thing: if you’re doing real work, you have real responsibilities. One of your responsibilities is to make sure your decisions are safe — not for your paycheck, but for everyone else. You have the responsibility to check with colleagues in order to make sure your plans aren’t stupid and damaging, and you have the responsibility to make sure your colleagues aren’t doing anything stupid either. If you aren’t willing to take ‘no’ for an answer, then you should switch to an industry where nothing you do matters, because that’s the only situation where such behavior is morally justifiable.
Clues about Medium’s business plan
Clues about Medium’s business plan
Yesterday I got an email. (Presumably many of you also got it, or something like it.) There was nothing in the email that indicated they wanted the information exposed therein kept secret, so I feel comfortable sharing it. Reading between the lines, it clears up some of the ambiguities left by official announcements about Medium’s future & their plans for monetization.
Hi,
I’m Brian, an editor here at Medium. I’m reaching out to you as a popular writer who’s published high-quality stories on Medium.
(I am not a popular writer — I’ve written one paid article that got the kind of traffic paid articles tend to get, and a large number of articles that have views in the single digits. My average view count is probably under 100, and my average recommendation count is less than one. So, this is a big clue: if I’m getting this email, pretty much anybody who has ever submitted to a popular publication is getting one.)
You might have read that we’re launching a new subscription product for our readers shortly. It’s the next step in our vision to build a place on the internet where ideas are rewarded for their value, not simply their ability to attract a few seconds of attention.
As fans of your work, we’d like to offer you the opportunity to pitch your ideas or relevant posts early, and become part of a select group of contributing writers for our initial launch.
(This is interesting. Not only will there be subscription-only stories, but subscription-only stories will be pitched to and vetted by Medium. This essentially means something akin to a Medium-staff-run publication that only subscribers have access to. While this is better for users than a paywall that locks you out after a couple articles, it means that Medium will be competing with Medium-hosted publications on the grounds of editorial standards.)
Our subscription will be an optional upgrade for people to become supporting members of Medium. These people will be able to access additional member-only functionality and new, exclusive content.
(I.e., there’s more to this freemium model than a cool-kids-club of vetted articles, but we have no clues as to what those features are, other than the fact that they’re user features as opposed to publication features.)
Writers across the world will continue to be able to publish on Medium for free, but we know there’s a great deal that never gets written or published by great writers, for lack of it making economic sense to do so. We want those stories — well-researched explainers, insightful perspectives, and useful knowledge with a long shelf life — to exist on Medium as well, and we think our paying readers will want to read them too.
(This is a strange pitch. Nowhere in the email do they explicitly state that writers will be paid for writing subscription-only stories — even though they have a pitch process like you’d expect from freelance — but they suggest that they will be able to make writing articles not currently picked up by publications make economic sense. This implies that they’d be either paying better than existing publications, supporting a greater number of articles, or trying to commission articles on subjects existing publications don’t or can’t cover.)
That’s where you come in, because we thought your writing could be a great fit. So what types of posts have you been burning to write? If you knew you had a paying audience waiting for your ideas, what stories could you tell?
We’re looking for pitches within the following categories to start: US Politics, Technology/Science/Future, Self Development/Productivity, Business/Startups, and Culture. So tell us what you’d like to write about and your rate. If it sounds like a good fit, we’ll get back to you with a thumbs up or feedback as soon as we can. If you’re interested, we’d love to receive your initial pitch before Friday, March 17th.
(Their launch categories are those already covered heavily by existing publications. I don’t see, for instance, short fiction on the list — even though, unlike all these listed categories, short fiction and poetry publications typically have short and infrequent submission windows. It’s also interesting that they want pitches so fast — the deadline is five days after the announcement!)
(I have removed their submission link. I should note that it links to Google Forms, so they’re probably expecting a large enough number of pitches that they want to automate handling them. This is a good sign: even pretty big publications tend to take submissions by email.)
This is just the first opportunity to be part of Medium’s subscription offering. We’ll be following up soon with more writing opportunities, as we continue to evolve our subscription and learn what our members enjoy reading most.
Thanks,
Brian
(In other words, they plan to rotate through different categories and have frequent calls for pitches. I have to wonder why they don’t transition to having open pitching and keeping only internal deadlines, at this scale.)
How Seiren re-engineers galge stock characters
Seiren is not among the most popular shows this season (with a MAL score of 6.4 /10, an AP score of 3/5, and an IMDB score of 7.3/10), and it is not among the best shows this season, but it does something interesting and unique that I think is worth a closer look. Specifically, it appears to be an exercise in rediscovery and re-engineering.
Anime has a well-deserved reputation for relying upon stock character types and plot structure. This is not necessarily a bad thing: most of what anime does differently from western productions is about cleverly saving effort and resources without sacrificing story quality, and stock characters can be used to this end. However, stock characters have become over-fitted, to the point that shows that hew too closely to type become predictable and boring (for instance, see this season’s Gabriel Dropout, a show that despite incredible animation and voice acting, was consistently done better beat for beat twenty years ago).
One way to combat this is to draw attention to it (as last years Saekano did). This works fine a couple of times, but by itself quickly becomes boring: the character who is aware that she is a shallow cliche becomes another shallow cliche. Another way is to engage in a dialogue with the type, adding nuances that make categorization fuzzier and possibly bringing in self-awareness but still engaging in the same attributes and behaviors that are associated with the type (which, as Digibro explains in several videos, is part of what NisioisiN does in the Monogatari franchise, and which I think this season’s Masamune-kun’s Revenge does reasonably well). It’s also possible to simply play the types straight, if character study isn’t a major goal: Konosuba doesn’t really try to engage critically with Megumin’s chuunibyo status or Darkness’s masochism, and instead simply uses those cliches exceptionally well in service of the comedy.
Seiren does something different, and (being an anime original instead of an adaptation) I think it does it intentionally. I take as Seiren’s thesis that the stock personality types are themselves dramatically interesting, but the cliche behaviors and attributes associated with those types are unnecessary and can obstruct connection. To this end, it tries to show these character types in a realistic and naturalistic environment, with non-core traits removed — as though an alien had read a description of these character types and written characters adhering to those descriptions without having ever seen an anime.
Seiren’s structure is, on its face, absolutely typical of the basic galge format: a protagonist gets to know several girls, and depending upon his actions he can develop a relationship with one of them. This format is so old that by the 90s attempts to shake it up by injecting time travel and supernatural elements had already themselves become cliche (and Seiren has none of those attributes); School Days, itself ten years old by now, tried to shake this format up by adapting only the bad ends. However, right off the bat, we have some big differences in how this is handled structurally, tonally, and in terms of shared characterization. Our characters are all high schoolers, with most of them in tenth grade; where a typical galge would polarize their maturity levels (particularly by making some characters sexually active to the point of deviance for comedic effect while making others almost absurdly innocent), Seiren portrays all its characters as within the typical range for upper middle class high school students: interested in sexuality but naive about it, with insecurities about their naivete, and with a mix of mature and childish hobbies. Where a typical galge raises the emotional stakes to a fever pitch and creates conflict based on major (often ridiculous) misunderstandings, Seiren shows characters floating in and out of an ambiguous zone between friendship and romance without much fuss, portraying every character as fundamentally wanting to understand and be understood. This is a pervasive thematic and stylistic element: we’re looking at a consciously & conspicuously iiyashi-kei love-comedy, where the stakes are low and even the arguments are calming. This naturalism extends to the art: the character designs are much more realistic than even a Kyo-Ani style, and characters are rarely off-model (which normally is a bad thing since it limits the expressive range, but emotions in this show are pretty muted so instead it adds to the appeal); nobody has an unnatural hair color, and few people even have a hair color that would be unusual among Japanese students.
Moving on to characters, Kamita is absolutely a protagonist-kun: he is physically average, has no strong feelings or preferences, and has no particularly notable personality attributes. However, he still differs from a typical character of this type. His aimlessness is lampshaded in the (widely hated) opening scene of the show, where he is being chided for writing in “stag beetle” on his career survey: while aimlessness is normal for this kind of character, the kind of childish surreal aimlessness you might see in actual tenth graders is rare. Kamita has interests that are normal in people his age but rarely shown or mentioned in anime of this type: he plays casual / nuturing games, acts out professional wrestling moves with his friends, and hangs out with a group of similarly awkward guys. The way his sexual interests are portrayed is also interesting, but sexuality in this show will be discussed at length later.
Tsuneki, the first heroine, is absolutely a tsundere: she puts on a harsh front in order to hide her insecurities, particularly with people she likes. Unlike the other characters, her design consciously references the database: she wears her hair in the same style as Asuka from Evangelion and Rin from the Fate franchise. However, most tsundere behaviors have been removed or modulated: she is never extremely harsh (she never hits anyone or becomes explicitly insulting), and while her emotional reactions are stronger than those of other characters in the show, they are never outside the range of typical or acceptable behavior. As the first heroine (and one who continues to appear in every route), she gets the most characterization, and the origin of her attitudes and habits is made clear: her conflict is a desire to be more mature than she really is, and in addition to going out of her way to act like her idea of an adult (getting a part-time job despite not needing the money and despite school rules against it, attempting to spend a week at a beach house with friends against her parents’ wishes), she attempts to present herself as worldly (particularly in terms of sexual experience) — a ruse that fools her peers but not the audience. While putting on airs and attempting to seem more adult is a common tsundere trait (Rin & Asuka both do it, and it’s extremely common in ‘palmtop tiger’ characters such as those in Toradora and Familiar of Zero), the mechanism here is very different: Tsuneki attempts to be an adult not due to parental neglect (Rin & Asuka are orphans, and Taiga and Louise’s parents are absent) but instead despite active intervention by caring and present parents. Even the way she is introduced flies in the face of every other major tsundere depiction: where tsundere characters are typically either the protagonist’s childhood friend or complete strangers, at the time of Tsuneki’s introduction she and Kamita are on the friendly side of casual acquaintances. Tsuneki’s interest in Kamita is immediately obvious to the audience (though not to Kamita) because of her constant minor sexual teasing, and this interest continues throughout the other routes: she’s the most sexualized character, but she is also consistently depicted as being mostly interested in Kamita (we don’t see her tease anyone else) and having only a pretty minor interest in him (while we see her act somewhat jealously in Miyamae’s route, this is partially because of shared history and differences in philosophy — basically, Tsuneki seems to accept whoever Kamita ends up with). In other words, Tsuneki is a tsundere but neatly avoids nearly all of the non-core tsundere traits, down to even very subtle ones whose universality is invisible until they are missing. Kamita makes sense as a match for her because he is so non-judgemental that she can be honest with him.
Miyamae is a slightly less clear-cut case. Gamer girls are newer as a fixture in anime, and their traits are less well-defined. However, Miyamae subverts many of them. One element here is the way gamer girl characters typically deal with femininity and with respectable behavior: they are often depicted as slovenly — uninterested in looks, grades, and athletics. (For an extreme example of this, look at Gabriel from Gabriel Dropout.) When such a character needs to seem initially attractive, they are instead shown as a secret otaku — someone with a fake ‘ideal student’ personality and a real ‘gross nerd’ personality they only show to family and close friends (see Himouto Umaru-Chan, Saekano, and even to a lesser extent this season’s Kobayashi’s Dragon Maid). Miyamae avoids or subverts all of this: she is a serious student with good grades, good at athletics and pretty enough to be known for her looks by second year students; she is open about her interest in video games, and her interest is initially unknown to Kamita only because he knows her mostly by reputation. She has social problems stemming from her dysfunctional relationship to video games, but her basic underlying problem is that she applies the same kind of contentiousness and seriousness to games as she does to everything else, without prioritization and without an awareness of how her competitiveness might affect others who lack her skills. She is lonely not because she flouts convention but because she doesn’t know how to turn off her drive to improve herself: she’s better than everyone around her at everything, to the degree that even her basic humbleness doesn’t help. Kamita has a weak enough ego that he’s willing to keep her company even though he can never hope to catch up to her, which makes them a good match.
Kyoko, the third heroine, is absolutely an imouto character. While her route hasn’t finished airing, she’s still a pretty interesting subversion of tropes. Having childhood friends belatedly recognize each other as sexual beings is a pretty common trope in western sit-coms (and in h manga), but it’s not terribly common in galge, and it’s done differently here. Usually, the childhood friend character is part of a long-standing one-sided crush (or a two-sided crush where each side is unaware of the other), but here, we explicitly see Kamita and Kyoko discovering new dimensions of interest in each other simultaneously, as part of the general project of both of them coming to terms with a shift away from their childish habits and behaviors. Major dimensions of the typical imouto trope are missing: the age gap is only one year, and Kyoko is neither blood-sister nor step-sister but instead a neighbour; there’s no long period where they are away from each other (they didn’t see each other in school during Kamita’s first year, but being a year apart they wouldn’t necessarily see each other without planning to anyhow, and it’s made clear that she is a frequent presence in Kamita’s life because of her friendship with his sister). Kamita and Kyoko’s relationship is not merely already close but actually still close, which distinguishes this from pretty much every other imouto character relationship I’m aware of.
The depiction of sexuality in this show is strange and interesting. Lots of galge and harem shows have over the top depictions of sexuality, but those depictions tend to be easily classified — essentially another kind of boring cliche. The protagonist who suddenly transitions into a fugue state of sexual deviance used to be pretty common (see Golden Boy, Sora no Oshimoto, and the Monogatari series) and is thankfully fading in popularity. Protagonists who are afraid of their own sexuality are still pretty common, particularly when over the top tsundere characters are in the mix (see The Asterisk War, Love Hina, and this season’s Interviews with Monster Girls). Characters who have a fixation on some particular attribute but are otherwise not terribly sexual are also floating around (see Kyokai no Kanata), and are perhaps even less realistic than the other two. Seiren breaks the trend with a pretty naturalistic depiction of polymorphous sexuality: the things that turn Kamita on are never cliches (he’s not ogling breasts or asses, or even thighs) nor are they depicted as strangely deviant, but instead they’re the kind of minor detail one might fixate on with an actual lover (the slightly damp spot where Tsuneki once sat, the nape of Miyamae’s neck). When Tsuneki flaunts her sexuality, she does it with awareness of this: she sits on Kamita’s desk and then teases him about his discomfort, or she uses overly familiar body language in order to fluster him, rather than wearing a fetish outfit or shoving her breasts in her face (as even otherwise well-developed tsundere characters like Asuka do). With regard to fetish outfits, when Miyamae wears a bunny costume, the costume itself isn’t depicted as highly sexualized by Kamita (although it’s shown that other convention-goers see it this way), and instead the show focuses on Kamita and Miyamae bonding over shared the shared fear of pissing themselves in public. (Two shows this season go to Comiket and cosplay, and both engage in interesting dialogues about sexuality; but, nobody needs an excuse to talk about Dragon Maid.) Outside of the sexualization of characters, we have two animals thematically and symbolically associated with sex: rabbits and deer. Rabbits aren’t terribly unusual as a symbol of sexuality, but I’ve never seen deer used in this way. Kamita’s friend Araki is described as a furry, and it’s explained that he has a sexual fixation on rabbits — to the point where he doesn’t seem capable of being interested in someone without a rabbit theme being present; deer, on the other hand, are never shown to be anyone’s sexual fixation but are consistently present in sexual situations: Tsuneki ends up in Kamita’s room because her attempt to escape the cram school was aborted due to deer chasing her (leading to her appearing wet and half-naked, vulnerable in front of him for the first time); Miyamae and Kamita originally bond over a deer-raising game (which Kamita says has some sexual-seeming sound effects), and they mate their deer; Miyamae goes to Comiket to sell her hand-made deer plushies, whose killer feature is the ability to swap gender using velcro antlers (and while flustered, Kamita makes a big deal about the sexual equality of deer society, driving away customers). On the other hand, when Miyamae starts playing as a bunny girl she goes back to her old bad habits from middle school, frequenting the arcade and skipping school to play video games online; Tsuneki is originally (casually) interested in Araki and after being sent to the cram school one of her friends steals him away, leading to a confrontation in the rabbit pen. From these circumstances, I would say that rabbits symbolize a kind of immature approach to sexuality (and an immature approach in general), while deer represent stable and loving relationships. This explains why Araki’s fetish is never emphasized as deviant but is instead frequently depicted as something he is slowing working through, and why Tsuneki (who both desires and fears adult sexuality) is both obsessed with and morbidly afraid of deer for the entirety of the series.
Another angle to Seiren that’s interesting is the way that it’s very clearly positioned as a coming of age story. Most shows of this type are set in high school as a matter of course: this is the only time when regular people have enough independence to pursue romance but enough resources not to need to spend all day working; while events associated with this period and with coming-of-age themes are often referenced in other shows (the obligatory test of courage episode, summer vacation, beach episode), they rarely actually function well as coming-of-age stories. Seiren broadcasts its intent with that first scene: where most shows use the career survey as a gag (with a character filling in “superhero” or “king of the world” or “famous actress”), Seiren subverts this by making it a gag that doesn’t make sense: there is no stock character we would expect to want to grow up to be a stag beetle (or who would even want to say that as a joke), and so we’re immediately set adrift, being unable to clearly place this character. This character is also adrift (the stag beetle is, along with the cicada, a kei-go for summer: if we want to interpret this decision symbolically, Kamita is saying that he would like his future to be similar to the summer of his life that the series depicts — a pretty abstract and aesthetic position, and one that Kamita might agree with but would never actually be able to explain). With any other show, the career survey answer would tell us which stock character Kamita aligns with; in this show, we need to understand who Kamita is before we can understand his answer. And, fundamentally, this is a childish response: one that befits someone like Kamita, who is simultaneously more childish and more mature than all his classmates. Each of the heroines have a different specific, plot-significant relationship with maturity: Tsuneki wants to be an adult but knows she’s not really emotionally ready; Miyamae, adrift in her own way, has an internal battle between a competitive and nurturing instinct and a tendency to return to habits of her childhood as a video game prodigy when stressed; Kyoko has a childish nature and is completely oblivious about how her behavior seems to outsiders, and her arc will have to involve coming to terms with her own transition to adulthood. In this show, maturity is connected strongly with a nurturing instinct, and for both heroines whose routes have been completed, they chose nurturing professions.
Seiren is also in dialogue with femininity. Not only are all the heroines specifically positioned as an ambiguous mixture of masculine and feminine traits (with Tsuneki being aggressive and sexually intimidating without ever seeming like a tomboy, Miyamae combining mastery over a traditionally masculine hobby with a traditionally masculine competitive instinct and a set of highly feminized skills, and Kyoko’s lack of awareness of her own femininity being emphasized in her route despite the twin themes of professional wrestling and magical girls), but Kamita is also portrayed as having traditionally feminine traits (being more concerned with caring for people’s feelings, protecting existing social groups, nurturing and repairing relationships, and communicating clearly about feelings than any of the heroines, along with being generally passive as a character — something highly feminized in galge-type shows). More overtly, Kamita cross-dresses as a magical girl in the first scene of the first episode of Kyoko’s arc, and he’s consistently shown as more interested in feminine-coded versions of things than masculine ones (for instance, during the Miyamae arc, he prefers the deer game and another schoolbus-driving game — i.e., nurturing games — to the violence-oriented fare Miyamae plays). No characters have traditional male roles here, in part because typically feminine-coded nurturing behavior is treated as a sign of maturity regardless of gender. It’s also important to note that, in the completed arcs, Kamita is shown to have chosen his profession to complement that of his mate: Tsuneki becomes a chef (conquering her fear of deer by becoming an expert in cooking venison) and Kamita becomes a waiter in the same restaurant; Miyamae becomes a school teacher and Kamita becomes a bus driver.
There’s a lot to pull out of this show: subtext, symbolism, and a complex dialogue with the state of the genre. But, does it succeed? I think it largely doesn’t.
It’s not a poorly made show, but because it avoids lampshading the tropes it subverts, it’s easy to take it at face value as a cliche rom com/galge adaptation made by someone who doesn’t understand how to properly use the genre tropes, rather than a conscious attempt to subvert them. By keeping the core elements of the types intact, it can be seen as less subversive than it really is; by dropping the auxiliary elements, it can seem less competent than it really is.
The way sexuality is depicted, while strictly realistic, seems alien because of how divorced it is from the normal depiction in this genre, and the casual pace and low emotional stakes can easily be seen by genre aficionados as a failure to achieve the kind of frenetic energy and melodrama typical of romance comedy anime.
It fares particularly poorly this season, since it’s airing alongside Kobayashi’s Dragon Maid, which covers a lot of the same thematic ground and plays similar games with tone and pacing while being a far better-executed show, and Masamune-kun’s Revenge, which has a similar setting but sticks to pretty standard tropes while being more consistently enjoyable; had it aired this time last year, against Saekano and Nisekoi, it probably would have a better reputation.
Another problem is that, without attempting analysis, the characters feel underdeveloped. All the necessary information exists in the text to consider them fully realized, but because it attempts to cram three whole routes into twelve episodes, none of this characterization is given breathing room. The general subtletly and the explicit avoidance of character-building tropes makes it even harder to quickly model the characters, resulting in difficulting even telling supporting characters apart during the middle of the show. Having Tsuneki’s arc exist primarily in the more limited environment of the cram school, where there are fewer supporting characters, makes sense, but we then bear the full brunt of a large cast of realistically-drawn brown-haired minor characters immediately at the beginning of Miyamae’s arc. Seiren doesn’t take advantage of any of anime’s many very important techniques for minimizing the number of frames necessary to show characterization — in other words, this show could have been live action with very few changes — and it suffers for it.
Seiren is, from what I understand, a kind of stealth sequel to Amagami SS. I haven’t seen Amagami SS, and Seiren doesn’t advertise its connection very well, but it’s possible that for viewers of that show it holds up better in the characterization department. However, Seiren reviews that directly compare it to Amagami SS are largely negative and seem to imply that Amagami is much closer to a standard galge show, indulging in the practices Seiren avoids.
It seems like what Medium is doing is creating a subscription-only publication of their own, competing with existing Medium publications. In the email, they made a big deal out of the idea that most users and writers would still not subscribe — which means that, if they want to keep their existing audience, existing publications will be unaffected.
I’d like to see them open up a submissions for fiction and poetry, if only because fiction and poetry publications on medium have short, rare, and unpredictable submission windows and typically pay peanuts compared to nonfiction publications. (For instance, How We Get to Next — not even a huge publication — will pay out nearly $300 for a relatively short article that meets their standards, while fiction of similar length and quality on Electric Literature is capped at $50 if I recall.) If their experiment with nonfiction is financially successful, they could probably afford to assign a couple people to a fiction slushpile and keep submissions open perpetually.
So, there’s a nuance here, regarding UBI.
The nuance (which even communists tend to miss!) is that there’s no reason that labor (and feelings of accomplishment related to labor) have to have any association with survival resources.
It’s an accident of history that we have such a direct association between labor and survival: in nomadic tribes, resources tend to go into a shared pot distributed by mechanisms unrelated to the labor it took to gather them, and even post-agriculture, farming families (themselves their own semi-isolated societies) did the same. (And, of course, in a monarchy resources are extracted from a hard-working population and given to an elite for redistribution on the grounds of inherited roles, unrelated to effort ratios.) Capitalism (in the sense of the widespread belief that trading labor for money is normal and natural, as opposed to a quirk of a tiny merchant class) was only about a hundred years old when Marx wrote about it.
UBI necessarily overturns this connection, which already (again necessarily) was already untrue at the margins: the extremely wealthy often do volunteer or charity work, and although this is good for PR, it’s also likely that in many cases it serves primarily to provide a source of fulfilling labor for people who have no need for more money; people unable to perform the type or scale of work necessary to produce a living wage still have productive hobbies (I know several people on disability who do a lot of knitting and crochet, and others who are authors), and although making money off those hobbies would lose them their existing income (without making up for it), they are willing to put time, effort, and scarce resources into sometimes gruelling work in order to feel productive.
If labor satisfaction and wage satisfaction had a deep connection, rather than one of convenience, the rich wouldn’t work for free, and the poor definitely wouldn’t work for free; but, neither of these things are remotely true.
Considering that satisfying labor is rare (or, in marxist terms, modern labor of all kinds typically alienates the worker from his product), we can argue that there is in fact a contradiction between being paid for labor and feeling satisfied by it. Part of this can be attributed to circumstances: if some necessary labor is inherently and universally unsatisfying, then anyone who does it will be unsatisfied; if all labor is satisfying to someone but each person is suited to a particular type of labor, then low margins of error and high risk related to attempts to move between fields result in most people who aren’t comfortably wealthy being trapped in jobs that can’t satisfy them. Part of it is also human nature: satisfaction is extremely sensitive to cognitive dissonance, and experiments show that a sense of satisfaction will be manufactured in response to a situation where the effort to reward ratio is low. (Why do people read Finnegan’s Wake and play Dark Souls? Why does everyone who finishes those things love them? Because casual interest is quickly punished, and because the sunk cost must be justified to account for the vast gulf between casual interest and sufficient interest.)
Having signed up, you can tell us exactly what we all want to know: is the premium content Ev’s pushing actually worth $5?
$5 can buy you a brand new 500-page paperback. Is this at least the equivalent of a new 500-page paperback per month? (A good one, I mean — not the kind where you buy it and then grudgingly finish it because you don’t want to have wasted $5. A new Neal Stephenson novel or something.)
High quality print magazines charge about $5 a month, and when they don’t deliver, they lose readers. High quality print magazines have utility separate from their actual content: they can be used to swat flies or line bird cages, which a Medium subscription cannot. At the very least, in order for Medium to be worthwhile, it must be superior to all the high-quality print magazines to which you subscribe (and all the similarly priced magazines to which you don’t subscribe because of their poor quality).
$5 is not like one cup of coffee a month. $5 is like choosing to splurge on expensive cafe-brewed coffee once a month; you can make several months of coffee in your home coffee maker with $5 worth of beans. Does Medium deliver enough to satisfy the crowd that would rather get beans in bulk and have many months of cheap coffee they must prepare themselves than purchase expensive chain coffee?
What Medium is charging is about half of what Netflix charges me. I watch a lot of Netflix. I would expect to get at least half the enjoyment I get out of Netflix out of Medium’s premium content in order to make the subscription fee worthwhile.
This is a pretty high bar. Does it deliver?
How Mamoru Oshii ruined the Ghost in the Shell franchise
Mamoru Oshii’s 1995 film Ghost in the Shell is excellent, technically groundbreaking, and hugely culturally important. However, as an adaptation of Masamune Shirow’s manga of the same name, it is an abysmal failure, and one that has negatively impacted all other elements of the franchise. Because of the upcoming american live-action film, I’d like to revisit the problems with the 1995 adaptation, if only because it seems like the 2017 film does to the 1995 film what the 1995 film did to its original source material — with major implications for future franchise entries if it becomes successful.
It’s important to give some background on Shirow, in order to understand why the manga is how it is and what differentiates it from his other work. Shirow (real name Masanori Ota) is known for works that combine heavily sexual content with detailed descriptions of machinery; his work ranges from pornography to hard SF. While earlier works like Tank Police, Orion and Appleseed contained a mix of science fiction and political ideas, Ghost in the Shell is notable in its first volume for its more grounded near-future setting and for much greater technical detail. Ghost in the Shell incorporates elements of previous works — like Tank Police and Appleseed, it’s about a government-sanctioned para-military anti-terrorism task force who have been super-empowered by the use of military hardware.
Where Ghost in the Shell differs from these other works (and nearly everything else in the genre) is the attention to detail baked into the manga: my copy of the first volume (the Dark Horse release) has 81 authors notes in a 10-page section in the back, as well as substantial use of inline footnotes: the first color spread contains four separate inline footnotes, and while the frequency of footnotes is reduced for the following three chapters, chapters four and five are primarily dedicated to world-building, and chapter five contains four substantial footnotes, four explanatory diagrams describing the science behind cyborg manufacture and design processes, and a textbook recommendation — all over a total of six pages. In chapter nine, twelve pages are dedicated to Motoko’s dive into a Puppeteer victim —initially focusing on the mechanics of diving, but eventually going into detail about brain structures & the use of electron spin in storage. From early on, Ghost in the Shell is a manga about ideas that doesn’t shy away from technical details.
Ghost in the Shell also doesn’t shy away from taking a close look at politics and economics. There are two major plots in the first volume; one is the Puppeteer plot, and the other revolves around economic incentives leading to orphans and refugees being brainwashed, cyborgized, and sold as counterfeit robots — in other words, industrially-mediated slave labor supported by government corruption stemming from the price difference between top of the line hardware and the bodies of the poor. Corruption is a focus: every antagonist is shown to have government ties, and while Section Nine is on the side of the angels mostly because of Aramaki & Kusanagi having abnormal moral integrity, its existence is grey-legal and owes itself to the same back-door politicing as 2501 does.
Oshii’s film adapts material from only four chapters of the 11-chapter manga: chapters 1, 3, 9, and 11 (i.e., the Puppeteer story), and only in a severely truncated way. Project 2501’s monologue during the dive in chapter 9 is dropped entirely (including the explanation that 2501 was a high speed insider-trading system), and of his nine page monologue in chapter 11, only elements from six pages are included — of 48 panels, Oshii’s film only adapts material from 13, producing an overly simplified explanation of 2501’s philosophy. There are ten diagrams in this chapter, of which only one is every visually referenced in the film.
A particularly interesting omission is the systematic scrubbing of references to all forms of eastern mysticism from the film: in the manga, 2501’s reasoning leans heavily on ideas about karmic connections, but these elements have been removed entirely. This is strange, since Oshii inserts his own religious references of various types (including bible quotes). One way to look at it is that Oshii’s philosophy (and the ideas he is fixated with) differ significantly from Shirow’s. The Ghost in the Shell manga is not largely concerned with identity: only seven of the 48 panels in 2501’s chapter-11 monologue touch on identity at all, and of them only one acknowledges it as meaningful — of the remaining six, two are related to the idea of karmic connection in terms of our false models of others’ personalities, and four are related to the idea that sexual reproduction produces more robust offspring than cloning. On the other hand, 2501’s monologue at the end of Oshii’s film is mostly concerned with the idea that Kusanagi fears changes to her identity, and most of the scenes Oshii added focus on the idea that Kusanagi considers her body alien to her (including being unembarassed by nudity & having a fixation on its mass-produced nature). These attributes Oshii gave to Kusanagi in the film are not at all in line with her character in the manga, of course: Kusanagi breaks Bateau’s eyes in retaliation for his intrusion into her VR beach party — i.e., for seeing her in a revealing bathing suit unexpectedly — and 2501 notes that, by being unconcerned with the preservation of her own identity, Kusanagi violated his prediction — agreeing to his suggestion that they fuse in one twenty-fifth the time he expected.
When Oshii made these changes to Kusanagi’s character, he influenced later parts of the franchise. With the release of the film, the manga and non-manga portions of the franchise begin to fundamentally diverge. Even as Stand Alone Complex reintroduced the Fuchikoma (as Tachikoma) and brought in members of Section 9 not featured in the film, it kept much closer to the film’s characterization of Kusanagi as business-like and humorless rather than the mischevious troublemaker from the manga, and also kept her portrayal as sexless (in direct contradiction with the manga, which gave her a boyfriend for several chapters and also had her participating in casual group sex with two female friends). While SAC brought in a handful of elements of stories from the first volume of the manga (including ideas about wealth inequality in the GitS universe), no other part of the franchise ever really adapted any story from anywhere in the entire manga continuity outside of the Puppetmaster story (and never substantially differently from Oshii’s version), and every iteration has trouble portraying Section 9 as being in a moral grey area the way the manga was able to do quite casually. While it’s not technically correct to say that Oshii’s film was “the original”, in a sense it’s more accurate: the manga was first, but Oshii’s film has a pretty tenuous connection to its source material and all later elements of the franchise draw more from it than from Shirow’s work.
Trailers for the new american film look like they have lifted imagery directly from Oshii’s series (both Ghost in the Shell 1995 and Innocence) and from the first episode of SAC. However, the voice-over in the trailer implies that the plot has taken another step in the direction of cliche: where Oshii took a character who was comfortable in her own bio-type sensory-film skin and turned her into a human thrust into an alien mechanical body and trying desperately to maintain her sense of self in the face of a mass-produced mechanized society, this film adds yet another tired idea — that she hates being a cyborg and wants to take revenge on the people who saved her life. In other words, plot-wise, this film appears to have more to do with 009 than Ghost in the Shell.
While the 1995 film dumbed down the manga’s ideas and replaced a large portion of them with cliche, current Ghost in the Shell fandom was introduced to the franchise by this film and largely prefers it. A much larger market will be introduced to the franchise by the 2017 film, and by the same logic, will prefer the even dumber version — potentially leading to lots of tie-in media that tries not to stray far from the gutted premise. (Imagine an american live action GitS TV show in the vein of current Marvel and DC shows.)
Meanwhile, the manga continues in the rich, weird direction its always had. Volume 2 was followed by volume 1.5. Volume 2 focuses on a psychic investigator hired by Section 9 who crosses paths with Kusanagi, who while following the trail of a hallucinated racoon-dog is hacked by a south american communist guerilla group and then discovers clones of her original body on a space station. The art’s better and it has even more footnotes than the original. Kusanagi determines whether or not she’s in a simulation by convincing herself that her leg is a perfect cylinder and then checking it in a mirror; a secret government agency prevents earthquakes with feng shuei; Kusanagi has a big cyborg body guard whose chest can hold her normal cyborg body, who she rides in like a mech and at one point hides a diplomat inside. It’s exactly the kind of high-information-density fun that GitS always promised.
The biggest argument against total dismissal of the remake is the existence of remakes that, despite bringing nothing new to the table structurally and despite clear profit motive, manage to unambiguously improve upon the original.
The iconic example is Evil Dead 2, which is nearly a shot for shot remake of Evil Dead with the same lead actor and the same director — but with the benefit of a larger budget and more experience behind the camera; both Evil Dead and Evil Dead 2 are arguably adaptations of Equinox, with huge improvements to the script and pacing.
We might also consider as a candidate John Carpenter’s The Thing, an improvement over the earlier adaptation of the same source material but one that introduces brand new elements as made available by the technology: fundamentally, the difference between the 1980 film and the 2011 remake is that the 2011 remake exhibited poor craft. But, when we re-make overwhelming successes, we set ourselves up for failure: most factors that contribute to a film being well-received (beyond basic competence) are not predictable or controllable by any one person involved, and can thus be considered essentially random — in other words, remaking a successful film is like using your lottery winnings to enter another lottery.
Why do we see a greater frequency of truly successful remakes in horror than other genres? Because a low-budget horror movie can have low risk and high reward compared to other genres, and a slightly higher budget can result in huge increases in reception: within the entire low-budget range in the horror genre, the odds of making back your investment are pretty good compared to other genres and the stakes are low enough to encourage experimentation. A truly low-budget success can often be improved with an only slightly higher effects budget or the replacement of a handful of actors with slightly better ones, because a low-budget success means the premise and script are probably pretty solid (whereas the technical skills — the most expensive factor at that budget level — can be improved without changing out what works). A low-budget horror success probably makes, in total, a lot less than a high-budget success in any genre, and is successful with a smaller but more fanatical audience, so even a largely failed remake in this context is not just a good bet but a safe one too, the stakes being low.
The tendency to remake good movies is misguided; even though a remake of a mediocre movie is more difficult to advertise, it’s more likely to succeed, both in terms of improving upon the original and in terms of being profitable.
While your analysis of control structures here seems accurate, I can’t really agree with the idea that the red tribe is structurally opposed to blue tribe methods. If we’re using Scott Alexander’s terminology here, then when we talk about the red tribe we’re actually talking about an equally top-down system with a different set of values and many parallel structures (the red and blue tribe each have competing newspapers, cable television networks, magazines, radio stations, and brands of beer; not only that, but the red tribe has even duplicated schooling infrastructure in some cases); when we talk about a group associated with collective bottom-up control, we’re talking about the grey tribe, which (trending both libertarian and civil-libertarian) has values that don’t align well with either of the larger tribes. While the grey tribe is ascending in importance, it does not yet wield a lot of political control — and probably won’t until most of the population born before 1970 dies.
The ascent of Trump isn’t really in line with grey tribe values — in terms of traditional political divisions, the grey tribe is pretty evenly divided, and on the one matter they agree upon (preference for bottom-up control over top-down control) he doesn’t have a good track record. He, and his administration, really represents red tribe authoritarianism: power based on charisma and physical dominance signalling rather than credentials and education. He gained power by spending a lot of time playing the broadcast image game in a way that specifically appealed to the red tribe: acting out a fantasy of power based on in-born superior skills that reside in the gut (i.e., the outsider capitalist figure); if he was a successful businessman, that would have given him blue tribe cred, which would have nuked his red tribe cred, but by playing the role of a failed businessman’s idea of a successful businessman on television he catered to his audience.
This is not to say that the grey tribe has nothing to do with it. The ascent of bottom-up media made top-down structures easier to duplicate and scale; the red tribe was able to separate from the blue tribe primarily by repurposing the bottom-up tech of the grey tribe as top-down and duplicating blue tribe systems en masse at low cost. The current polarization is a result: the red tribe and blue tribe are of nearly equal power, and almost completely isolated from each other. They will attempt to cancel each other out, which increases the relative power of the grey tribe.
Portable identity is useful only insomuch as single-stable-identity is useful.
Portable identity is useful only insomuch as single-stable-identity is useful. I don’t think either the federation model in general or GnuSocial specifically is ideal for supporting such a thing. That’s not necessarily a bad thing.
New instances are forming as affinity groups around specific shared interests. It makes sense to segregate one’s social networks around clusters of interests that have minimal overlap; one may maintain six or seven different accounts on different instances, and not bother to (or even avoid) linking them.
Having a single identity connected to all your various social groups isn’t in line with natural human behavior — we are a different person with our friends than with our coworkers, neighbours, or parents — and the idea that connecting these separate identities is valuable is mostly an artifact of data mining; the trend (only about a decade old now) toward single universal identities on social media as the default is not only unnecessary but sometimes actually damaging (for instance, someone who is gay, or into kink, or even who has non-sexually-charged yet culturally maligned interests, might not want their activity in those communities connected publicly to their work life or associations with other community).
Your cost estimations are based on a more centralized model than is really reasonable: mastodon.social is an anomaly, but ideally mastodon instances will be small. A more efficient implementation (even if it doesn’t scale) could bring us closer to a 1:1 user-to-instance ratio, which is the direction I’d like to go: when every end user is an instance admin, problems related to admin / user friction over terms of service disappear, and there’s no fundamental reason why you couldn’t run a single-user OpenSocial node off a phone or as a browser extension. (Changes to caching and routing might even make extended downtime from single-user nodes a non-issue: think DHT or Cord.) Even if we don’t expect nodes on phones, we shouldn’t expect expensive third party hosting to remain the norm: AWS & other cloud hosting services are an artifact of this weird return to hierarchy we’ve stumbled into, and if all nodes are effectively owned by three companies there’s very little benefit to decentralization; even most casual users have always-on internet, and therefore can run their own servers.
Federation favors large numbers of temporary non- or partially-intersecting identities, rather than a single static centralized one.
The desire for a single static centralized identity on the internet is largely one created by and for existing centralized social networking platforms & is pretty recent, so I don’t see it as much of a loss (although I’m also not a media personality, so I gain more & lose less from having my identity be fluid & disparate — YMMV).
If the OpenSocial platform continues to grow for much longer at this rate, I predict one of three outcomes:
• Tech for running instances will get lighter & easier to manage, so that the ratio of users to instances will approach 1:1. We’ll get to the point where most users have personal instances running on their phones & using an instance run by somebody else is a little like using the computer at the public library. • Most users will have five or six different accounts on different, themed instances, and keep their interests somewhat segregated. (We already see places like witches.town, a.weirder.earth, rich.gop, and oulipo.social, where specific interest groups congregate & the local and federated feeds are much more useful; it may be that mastodon.social and other general-purpose instances will become a deviation from the norm.) • Some big company will jump on the bandwagon by hosting stock mastodon on expensive hardware, making proprietary changes, and heavily advertising it to less-savvy users who have no interest in federation. (Google plus replacement? After all, they nuked Orkut, Buzz, and Wave.) This would be the worst outcome, since the reasons people are interested in OpenSocial are essentially social and political, not technical, and having existing big players involved would run counter to those social and political rationales.
If your goal is to find out how people on reddit actually talk about it (rather than what kind of discussion from *other people* on the subject redditors value), a cutoff at the 100 karma threshhold is probably going to produce horribly skewed results: because karma determines presentation order, it grows non-linearly, which means that only a few very popular comments will get more than single-digit karma. If you want to get a smaller data set but keep it representative, I would isolate comments with exactly one karma point.
The return of the No Budget Film Contest
The return of the No Budget Film Contest
In 2010 and 2011, I ran a “no budget film contest”. I was annoyed that even with the existence of cheap video cameras & free video editing software the perception remained that even low-budget indie films had to be expensive, so I wanted to showcase good films made for no money at all.
(Then I got busy with work & ended up failing to announce the 2012 one on time, so it stopped.)
Today, the problem is even more extreme: despite most of us in the west already owning phones capable of shooting better video than the equipment used for Clerks, we aren’t seeing a lot of indie films shot with such an expressly low budget, and the mainstream film industry keeps piling more and more money into expensive bombs. Even youtube channels spend money on expensive equipment & buy time from professional animators and editors in order to make their content look a little more slick.
Just as it’s possible to put on a compelling stage play with a single set and one or two actors, it’s also possible to create a compelling film without spending any money at all. It’s a luxury that independent filmmakers as recently as 15 years ago would have killed for: film (and therefore shooting on film) is expensive, and developing prints likewise, so until the advent of cheap digital cameras and the widespread accessibility of large hard disks it really wasn’t possible. But it’s possible now, and we’re not taking sufficient advantage of it.
Budget can be an advantage, but it can also be a disadvantage: spending money on a film, rather than just time, means a greater emphasis on making sure it has a good reception, which (outside of the recklessly overconfident) leads to a conservative attitude and less experimentation. With no budget, you are free to make your film as strange and experimental as you like, so long as it doesn’t cost you any money.
Today, on April 20th of 2017, I am launching the First Annual New No-Budget Film Competition. The submission deadline is July 4th, and I will announce the winners by September 1st.
Rules:
I will announce the winners by September 1st. First prize will be $5 transferred via paypal. The top three submissions (judged any way I like) will be posted on this blog, and will get the titles of Best No-Budget Film of 2017, Second Best No-Budget Film of 2017, and Third Best No-Budget Film of 2017. They will also get a picture of a golden coin trophy.
How to submit: Upload your finished films to youtube or vimeo (or some other streaming video site), and post the link in the comment thread of this post. The films may be of any length, and you can enter as many times as you like. (If you cannot post comments on Medium for some reason but still want to enter, contact me on twitter or mastodon)
Folk horror as speculative sociology
H.P. Lovecraft and the Wicker Man
[1]
Folk horror is having a Renaissance, as the novelty cycle revisits the seventies at two iterations’ remove & the SF community starts again to seriously analyze the dialogue between the weird and the hauntological. The spring season, with Easter, Walpurgisnacht, and May Day, is a good time to revisit this, and, as expected, various publications have — not just the usual suspects like Scarfolk, but also The Guardian, which published a piece whose analysis I’d like to pick apart a bit.
Newton’s analysis suggests a rural versus urban dimension (and, by extension, a modernity versus tradition dimension), and while this exists in the text, I consider it shallow. I’d like instead to argue that, rather than being in the tradition of gothic and romantic horror, folk horror has more in common with the point at which weird fiction intersects with science fiction.
We could choose no better an entry point than H. P. Lovecraft, whose horror stories are best seen as science fiction whose science is ‘anthropology’ (the same way J. G. Ballard is a science fiction author whose science is ‘psychology’).* Nobody really considers Lovecraft to write folk horror, but Lovecraft’s formula — similar across much of his work to the point of bordering on self-parody — is very similar to how Newton describes folk horror’s core narrative:
The films feature a recurring archetype: the arrival of a stranger, the discovery of a secret cult, then a vicious murder, perhaps a sacrifice, designed to propitiate pagan gods. The metropolitan visitor, the outsider from the mainland, comes into a situation strange to them and to us. Here the enlightened laws of the nation do not pertain. In these forgotten spaces, there are other laws: rules and rituals that are both familiar remnants of some tribal memory yet utterly strange. The locals understand, while we do not.
Lovecraft’s formula, written in a similar way would be:
The stories feature a recurring archetype: the discovery of a secret cult, by a stranger, and the discovery that this cult’s beliefs are substantially factually correct. The metropolitan visitor, the outsider from the university, comes into a situation strange to them and to us. Here the enlightened laws of scientific common sense do not pertain. In these forgotten spaces, there are other laws: rules and rituals that are both familiar remnants of some tribal memory yet utterly strange. The locals understand, while we do not.
Essentially, the core difference between Lovecraft’s stories and the typical folk horror is that in Lovecraft’s stories the superstitions of the locals are demonstrated by science to be justified, while in folk horror these alien beliefs are implied not to be held even by the high priests. Lovecraft, to make up for the assumption that superstitions will be dismissed, must make the superstitions true; in folk horror, the superstitions are dangerous for reasons unrelated to their truth.
Newton goes on to say:
Their rootedness in place becomes uncanny. Once, almost everyone was so rooted. But now — in the discontinuous world of modernity, where relationships are casual and work comes and goes — such belonging feels strange and even sinister.
I would argue that the rootedness is in many cases intentionally illusory: private doubts play a big role in the genre, as a motivation for extreme behavior as well as an excuse for escalating the stakes. Our protagonist is often brought in by an insider with secret doubts, after all.
What really triggered my analysis is this:
being inside a myth is terrifying, a fall from the industrialised, supermarket world into one possessed by abysmal powers
I think Newton gets this almost exactly wrong. It’s not that being inside a myth is terrifying: mythology is the natural home of man, and it’s questionable whether or not it’s even possible for us to venture outside that domain. Instead, the source of this terror is being inside someone else’s myth — an alien myth — and drowning in it, without possibility of escape or assimilation.
Our fish out of water character is thrust into a world shaped by minds no less alien for the sake of being human, and it’s not xenophobia or ignorance but the cold mechanism of someone else’s unexamined culture that chews him up. He cannot know of this secret cult’s beliefs and signs because it is secret: he is an accidental anthropologist who only realizes how far he is from home once his life is already forfeit by the alien logic of the cultists.
Hot Fuzz illustrates this brilliantly, being a send-up of folk horror that also functions as excellent folk horror: the alien belief of the cultists is a slightly more extreme form of a common belief — the patriotism of someone who wants to maintain a good image of their homeland, even if that image doesn’t correspond with reality. The cultists aren’t fully alien; people like this exist in London too, or else there would not be spiked benches. The only difference is that these people separate their image from their reality more effectively, by meeting at night in disguises rather than in public at town hall meetings, and so their behavior can become more extreme because they no longer need to integrate these two sides of their personality.
It’s here that folk horror becomes interesting from an analytic perspective. Much as science fiction injects strangeness into familiar situations in order to force a new perspective on them, folk horror takes all of the inhuman mechanics of human society — all the casualties of belief — and externalizes them so that we can recognize them.
This only really works when there’s a solid understanding. Compare the original The Wicker Man (a great representation of how hippie rhetoric can hide a control-freak nature) to the remake (which, despite having an identical script, somehow became a warped rant against a straw feminism), and then compare both to The Love Witch (which plays with ideas about femininity and female empowerment in paganism from a feminist perspective).
This externalization of normal social behaviors as occult is also something that separates folk horror from adjacent works. As much as The Wicker Man has in common with Mario Bava’s Kill Baby Kill, the latter is not folk horror: the supernatural phenomenon is not only real but also alien to the residents, who don’t have a much firmer grasp on the mechanics of the haunting than the protagonist. An American Werewolf in London is a similar case: the protagonist doesn’t collide with a social formation but with a supernatural formation with a social aspect formed around it (much like in Lovecraft, where the physical reality of the creature justifies the folk beliefs even to outsiders); an imaginary version of that film, wherein that wolf-haunted town hunts down residents without any textual evidence that the werewolf is real, would certainly qualify.
We ourselves live in societies constructed around imaginary ghosts, designed to avoid imaginary monsters and appease imaginary gods. We live as though these things have fury, even though the fury that touches us is that of the society. Considering the superstitions of others, and their victims, we can get closer to considering our own: is it fundamentally more sane to sacrifice people for money than for the harvest, given a similarly powerful and dangerous system of social enforcement? What justifications do we use for hurting people that would be horrifying to an outsider?
It’s all in the name, of course: folk horror is about the folk — not merely folks, but the collective. We don’t cease to be folk by moving to the city. We just take on new mythologies.
*I’m going to gloss over the flaws in Lovecraft’s anthropology, which is discussed at length elsewhere. I cannot hope to summarize the discussion around it, other than to say Lovecraft’s ideas were not accepted or acceptable in his era and circumstance. It is not the substance of Lovecraft’s ideas about people unlike himself that interest me here, but instead the form of the anthropologist-hero in horror, along with the idea that horror is driven by awareness of alien ideas rather than by the direct threat of violence. The Love Witch and The Haunting of Hill House both use warped or alien ideas about life as a source of horror, without othering the source of these ideas by class or culture to a significant degree.
Maybe it’s because “Marine Antoinette” sounds so much more natural that “Macron Antoinette” is failing to catch on? After all, supporters of Le Pen would nuke their own cause by accidentally saying the former — something I’d expect to happen automatically, if they try to repeat it enough without automation.
I came to the same conclusion from another direction:
Thinking in terms of maintainability under mental load (i.e., debugging on little sleep with a deadline / whatever), idioms trade space in code for mental load, so long as those idioms are appropriately applied.
In other words, if you write code with the awareness that every idiomatic structure is one chunk and every major divergence from idiom counts as a chunk (and everything that looks like an idiomatic use but actually relies on some minor detail to change the behavior non-trivially — i.e., obfuscation — counts as several chunks, since at the very least the programmer must identify the idiom then identify the actual behavior), appropriate use of idioms can reduce the code size in terms of number of chunks (and thus reduce mental load). This corresponds to engineer-time much better than number of lines or number of characters do.
When people complain about boilerplate, they are usually complaining about the misuse of idioms (either enforced by language or style guides or enforced by a programmer’s ignorance): a java program written in ‘good’ java style for solving a problem not well suited to that set of abstractions can easily contain more trivial classes / beans / interfaces than an equivalent program in a more well-suited language contains lines of code — in other words, the chunk count actually increased by trying to force the solution into an inappropriate set of idioms, because in addition to solving the actual problem the program also solves the additional non-trivial problem of idiom conversion.
When I am not developing interactively or writing throw-away code on a deadline, I avoid features that allow me to compress many mental chunks into a small number of characters (particularly in languages like python, where such features are extremely powerful), because I know that no matter what I do, a bug in such a heavy line will be many times harder to fix (and the behavior many times harder to reason about) as soon as I’ve GC’d my mental model of it & no longer have a detailed internalized map of execution in working memory.
I have a coworker who favors such constructs, and finds idiomatic code hard to understand. However, he thinks about code in terms of how the interpreter or compiler works (learning languages by reading their implementations) and his chunk size is determined by implementation AST — in other words, he has an unusual way of thinking about and reading code that is unconcerned with programmer intent. I would recommend anyone who does not model programs in this way to embrace the force multiplier of careful idiom use.
Visible & Invisible Architecture
A Space of Death and Madness
[1] Bas-relief from the entry to Yale’s Sterling Memorial Library, whose structure & style is modeled off gothic cathedrals, showcasing magic symbols from around the world and across history
Dario Argento’s best-known film, Suspiria, centers itself around a building: the Black Forest Dance Academy in Freiburg. The first film of the Three Mothers trilogy, it sets the formula: three witch-matriarchs, each tied to a magically-powerful building, and each with a coven of thralls around her. These buildings are designed by a man named Varelli, and it is Varelli’s confessions, in the form of a rare book called The Three Mothers, that constitute the primary defense against each witch. Despite this, in Inferno, the second film of the trilogy, we see Varelli — his life unnaturally extended — is now in thrall to his own creation, mute, senile, and unable to resist the will of The Mother of Darkness. Varelli is architect as Faust-figure: contracted by demonic forces to create closed worlds of death and madness, he acts as an unwilling bridge between their mythic realm and the solid world of granite and stained glass, and he is necessarily canny to their nature and plans.
This same theme is present in two other, otherwise very different films: High Rise (2016) and Ghostbusters (1984). In each case, a brilliant architect creates a space of death and madness that perversely bridges previously-separate worlds, which bewitches its inhabitants and corrupts the architect himself. In High Rise, the only uncorrupted person is the psychiatrist Laing — presumably named for R. D. Laing — who takes limited, active, intuitive control of his environment by painting his apartment a dull grey color and regains his sanity from it. He is notable, however, for his emotional disconnection from the environment. In Ghostbusters, similarly, the uncorrupted figures are emotionally distant and have an engineering-focused approach to the world.
The idea that architecture can have an essentially magical effect on the world is, therefore, quite mainstream — any idea that appears in popular films across genres must be considered mainstream. The idea of architect as magician is not unknown to occultists (and has proximity to the idea of the genius loci), and we can also draw connections to psychogeography and the semi-mystical ideas about the power of architecture held by figures like Le Corbusier and Frank Lloyd Wright, but its presence in these stories holds special dimensions and attributes.
The idea of a perverse house, haunted by its own design or history, is essentially a gothic one. But such a building is created by tragedy (Overlook Hotel, Hill House), madness (Chapelwaite, the Winchester Mystery House), or incompetence (anything on McMansion Hell). Even psychogeography suggests that the influences upon the character of a city are typically unconscious and emergent. What we see in these examples instead is a space engineered to be perverse — something that the Situationists, even with their gnostic tendencies, didn’t predict.
Did they need to predict it, though? The imposing totalitarian architecture of Albert Speer certainly represents a precedent for the considered and documented engineering of psychological responses via spaces, although this is still closer to Le Corbusier than to Ivo Shandor.
Instead of trying to compare the Situationists’ unitary urbanism of a nootropic Disneyland to Ruinenwerttheorie, perhaps it’s better to come in from the direction of the occult.
The idea of a genius loci tied to a figure or line is hardly outside the mainstream; it’s common enough in folklore that it was a staple of early Disney films. A corrupted landscape is associated with the rule of a corrupt figure, as in Sleeping Beauty, The Lion King, & Lord of the Rings. The emergence of a divinely-approved ruler reshapes the landscape favorably & his death darkens it, while conflicts related to that ruler create surreal pocket-worlds full of madness & numinous religious imagery (as in Excalibur & Le Mort D’Artur). But the power of an architect is not like the power of a king. The corruption of a land by its king is tied deeply to divine blessing: a righteous king makes decisions in line with the will of god, and so the king acts as a repeater and amplifier for god’s will. On the other hand, an architect, like a corrupt king in this system, is always actively creating changes to the world out of line with nature, whether or not it is in line with a correct idea of a better future world.
It may also be useful to look at alchemy and freemasonry, here. The architect creates a space in the image of his mind and then creates a mind in the image of his space (“we shape our tools and thereafter our tools shape us”), presumably refining the design — smoothing each ashlar, diluting and coagulating. Without invoking the mandate of heaven or some circular justification for calling everything natural, we can hardly say that this careful and mindful practice of self-modification is in line with the mindless and mechanical ‘natural’ flow — it’s a magical act on the spectrum closer to writing a novel than to mowing the lawn, in terms of imposition of will upon the world.
We can see within the perverse spaces of architect-wizards a shallow anti-masonry, and a distrust of magic in general — perhaps this is true of Argento, who claimed he was inspired to write Suspiria by discovering a Steiner school nearby. But we can also see it as a warning about ungrounded introspection: madness can come from looking too closely at the strange loop in our mind and letting the feedback amplify tiny momentary glitches in our thinking into huge permanent distortions. We can build monuments in stone out of the knots in our heads, if we aren’t careful.
Once again, High Rise is the odd duck here. Our architect is not really a pawn or an antagonist. He creates a mad space out of a desire to create any kind of new social relation (most of which will necessarily seem mad). Our protagonist is not working against the madness of the space, but instead is merely immune to it. And, finally, the gateway he created did not connect the mundane with the supernatural but instead connected the upper and lower classes in a pathological way — close enough to eat each other but far enough to breed distrust.
It’s easy to read High Rise as anti-Marxist, anti-utopian, anti-anti-psychiatry, or anti-Spectacle. But in this context, I read it as having two messages, one rare and the other common: social relations, like those created by engineered spaces, are extremely powerful and hard to predict and don’t call up what you can’t put down.
All fat middle-aged white guys are secretly the same person, like Saint Gulik.
The end-game of the voice UI (like that of the chat UI) is the command line interface.
The end-game of the voice UI (like that of the chat UI) is the command line interface. So, it’s useful to take cues from currently-existing good CLI UX. (For instance, look at the differences between zsh & command.com, and the trends in the evolution of borne-compatible command shells since 1970.)
To start out with, there are a handful of differences between interfaces centering around how learning curves are managed. While all major command line interfaces front-load some learning (i.e., the user is expected to learn quite a bit early in the experience in order to understand basic operations — something that GUIs are loathe to do, and something that therefore limits how nuanced the control user have over GUIs can be with standard widgets), unix shells made big leaps in discoverability early: as of the GNU announcement in 1984, Stallman was already saying that any command shell should be expected to have auto-completion features, and online documentation systems like man & apropos already existed (soon to be joined with the hypertext system info) — this at a time when both the Macintosh & the Amiga were still under development & most users had never seen a GUI, and during a period when users were generally split between Microsoft BASIC & MS-DOS in terms of their command environment.
No virtual assistant I am aware of will list available functions or list the set of invocations they accept — in other words, there is no help system comparable to man or apropos — and since error reporting is nearly nonexistent, this means interacting with unfamiliar features is the equivalent of playing a classic text adventure. “I see no lamp here,” says Siri. This kind of interface is fine for a game (where part of the fun is figuring out the systems and thereby beating the snarky & frustrating UI), but if we intend to do real work with virtual assistants they need to operate a lot more like a good CLI and provide detailed and specific technical information about their functionality. (The Arctek Gemini, a robot released in 1982, had a voice-controlled virtual assistant that could do this; why can’t Apple?)
The second big factor is that, if we want to do real work with these interfaces, we need to reduce the amount of fuzzy matching and move toward a system in which every word is expected to be meaningful. Picking one or two key words out of a sentence will give few false positives when performing simple tasks, but cannot scale: looking at programming languages that resemble english (such as SQL) gives us a good idea about how pronounceable keywords can be combined in relatively natural ways to allow the formation of specific, precise, and complex queries. Having some front-loading of learning is necessary for truly nuanced queries, but simple ones could still sound like natural speech. Combined with a built-in help system, the mechanisms for using this could be made very discoverable.
Mode indication is a problem in speech interfaces that have complex behaviors. We expect a great deal of stacked context, and even today’s systems, which are capable of doing next to nothing, tend to fail miserably at consistently keeping track of stacked context or falling in and out of different modes predictably.
Finally, a big problem is that almost everybody who has learned to type can type faster on a real keyboard than they can speak. On-screen keyboards on smartphones, disabilities affecting the hands, and contexts where keyboards are not handy or usable may justify speech-based interfaces even when they are otherwise not ideal; some of these cases are better-served by chording keyboards or by improvements to predictive text systems.
If we want speech-controlled virtual assistants to graduate from toy to tool, we can’t allow ourselves to be seduced by flashy but ultimately hollow flat-pack futurism, and must instead admit that this tech will be inappropriate in most situations: voice control will be rude in public and unnecessary in the home, no less distracting while driving than actually typing on the phone, and less effective in the workplace than speaking to a coworker about the same topic.
The killer apps for this tech at the moment seem to be fielding questions from pre-literate children and taking orders while an adult is cooking; a move toward having clear and precise syntax could make it possible for more complex queries to be asked (especially those in the domain of what non-technical users assume current voice assistants can handle but that they can’t — expecting “will it rain next Tuesday” to work because “will it rain tomorrow” does), and build-in help systems can encourage curious children to learn the kind of thinking that underlies programming, giving them a leg up in school when it comes to mathematics and grammar.
Considerations for programming language design: a rebuttal
Considerations for programming language design: a rebuttal
Walter Bright, who designed D, wrote an article called “So You Want to Write Your Own Language”. I love D and I respect Mr. Bright, but I disagree with nearly everything he says in it, mostly because D itself and Digital Mars’s work in general is (and should be) extremely atypical of the kind of programming language design that occurs today.
Digital Mars has been writing cross-platform compilers for decades (and when I was first getting into C on Windows in the early naughts, Digital Mars’s C compiler was the one that gave me the least trouble), and D was positioned as an alternative to C++ as a general purpose language. The thing is, most languages being developed today aren’t going to be C-like performance-conscious compiled languages that need to build binaries on multiple platforms and live in environments with no development toolchains (all major platforms either ship with a GNU or GNU-like UNIX dev toolchain or have a system like cygwin or homebrew that makes installing one straightforward), and trying to compete with C++ is not only technically difficult but also foolish for wholly non-technical reasons.
Design
The second piece of advice Mr. Bright gives is to stick relatively close to familiar syntax, in order to maximize audience. This makes sense if you are trying to compete with C++, as D does. But D is a great illustration of the problems with this approach: D does everything C++ does better than C++ does it, and yet it has failed to displace C++. The reason is that C++ is a general purpose language, like Java, C#, and Go (in other words, a language that performs every task roughly equally poorly) and the appeal of general purpose languages is that they allow one to trade time learning a language that’s a better fit for the problem for immediate implementation difficulty.
In other words, most things that are implemented in C++ are done that way not because C++ is the best tool for the job, but instead because C++ is marginally better suited than the three other languages that everybody on the dev team has known inside-out for twenty years. D can’t compete because the whole point of a general purpose language is to appeal to devs who failed the marshmallow test & want to minimize the initial steepness of the learning curve.
The domain of general purpose languages is crowded, and each of them casts a wide net: being technically better across the board than all of them is difficult, and without really heavy institutional support (like that given to C++ and Java by university accreditation boards & given to C# and Go by large corporations) being technically better will only give you a small community of dedicated fans and an isolated ecosystem. Writing a general purpose language is a bit like forming 2-person garage startup positioned against General Mills.
Instead, it makes more sense to target some underserved niche: general purpose languages are necessarily bad at serving most niches, and the syntax changes necessary to serve such a niche better are generally all too obvious to anybody actually developing in such a niche. Iteratively identifying and relieving pain points in real development work is sufficient to vastly improve a conventionally-structured language. Cutting the gordian knot by introducing a wildly unconventional syntax that’s better suited for certain kinds of work can be even easier, since such a syntax is allowed to be poorly suited for work outside its domain, and since the syntax of conventional general-purpose languages is made complicated & hard to reliably implement by decades of pressure to be suitable for all kinds of very dissimilar problems.
Syntax is the difference between a tea spoon and an ice pick. Rather than aspiring to being vice grips or duct tape, build a language that is exactly the tool you need, and allow it to be a poor fit for everything else.
When beggining a language project, it makes sense to try to get it minimally functional as quickly as possible, so that you can dogfood it & start identifying where your model of suitability has flaws. So (in direct contradiction to Mr. Bright’s list of false & true gods) it makes sense to start off with the easiest suitable syntax to parse and as few keywords as you know how to proceed with. As you write code in your language, you will identify useful syntactic sugar, useful changes to the behavior of corner cases, and keywords you would like to add — and none of these are likely to be the ones that were at the bottom of your bucket list initially, unless you’re sticking quite close to some known conventional language.
To mimic Mr. Bright’s format, here are some false gods of syntax:
1. Tried and true / universally-familiar syntactic constructs. A language should not sacrifice the ease with which it caters to its niche in order to cater to people who don’t want to really learn it, but instead should have a syntax that minimizes the effort necessary to solve problems within its domain and that corresponds predictably to its operation. If a language is syntactically similar to some other language, this similarity should reflect a semantic similarity (i.e., a stack language is justified in looking like forth and a declarative logic language is justified in looking like prolog). Avoid making a functional language look like an imperative one or using constructions that are misleadingly/shallowly similar to ones from another popular language, and instead choose metaphors for best fit. 2. Over-design / biting off more than you can chew. Start off with a minimally viable language and determine what needs to be added from experience. Over time the gaps will be filled in and your language will look less like an esolang and more like a general purpose language, unavoidably; making a sturdy foundation is easier if you start off with only a few elements and making them quite solid. (I’m not encouraging the kind of masturbatory minimization of keyword lists you sometimes see in forth implementations, where only one or two keywords are hard-coded and everything else is written in the language; however, starting off with functions, conditions, i/o, and simple mathematics is generally sufficient to start writing real code, and it’s generally possible to get a simple language from zero to capable of hosting a fibbonacci sequence function in an afternoon.) 3. “Readability”. Reading any language is a learned skill; while you should know how to parse the language you’re designing (don’t make it too hard for yourself since you’re going to spend a lot of time puzzling over the proper behavior of code you yourself wrote), caring too much about lowering the initial learning curve of new developers will put more work on your plate and limit your flexibility. I know people who consider C++ easy to read, and even a very straightforward language like forth or brainfuck looks like line noise to someone who doesn’t know the trick to reading it. If your language is well-suited to some problem, you need to trust that developers will figure out how to read it, and having a simple syntax that doesn’t cater too much to beginners will help more seasoned developers understand complicated code later.
Here are some true gods of syntax:
1. Suitability. Be an ice pick, not duct tape. 2. Simplicity. Parsing is hard enough without ambiguity and corner cases (for both humans and machines); create a simple foundational syntax with as little ambiguity as possible, and make sure every behavior you add is compatible with that foundation. 3. Iterative development. Making a really minimal language is straightforward, but making a language that is useful for complex problems is hard; it’s much easier to add features slowly based on need than it is to determine all of the useful behaviors before starting development, and it’s much easier to test a small number of base features before building on top of them than to test a large complex language all at once.
Implementation
Regex is a very powerful tool, if you know how to use it. Lexing involves identifying character classes, which is exactly what regex is best at. I wouldn’t recommend writing large chunks of your language as regular expressions, or using nonstandard extensions like lookahead & brace matching (which break some of the useful performance guarantees of vanilla regex anyway), but regex is invaluable for lexing, and with care it can be invaluable for parsing as well.
If you are writing your language implementation in C, using lex (or GNU flex) can be a great time saver. I wouldn’t recommend using Bison / YACC, personally — it’s possible to thread arbitrary C code into lex rules, and implementing parsing with this feature and a set of flags is much easier for simpler syntaxes than using a compiler-compiler. With regard to portability concerns, any major platform will have a development toolchain containing lex these days.
Of course, if you go the forth-lite route and have nearly completely consistent tokenization along a small set of special characters, this is much easier. Forth-lite languages can be split by whitespace and occasionally re-joined when grouping symbols like double quote are found; lisp-lite languages with only a few paired grouping symbols can easily be parsed into their own ASTs.
It is possible to design a series of regex match expressions and corresponding handler functions, and iteratively split chunks of code from the outside in. I did this in Mycroft, and I recommend it only if your syntax requires very little context: the two different meanings of comma and period in Mycroft (as well as handling strings) made parsing this way complicated in some cases, but this kind of parsing is very well-suited for languages like lisp where all grouping symbols are paired and very few characters are special-cased. This is a style I find quite natural, but I understand that many people prefer thinking of parsing in terms of a left-to-right iteration over tokens; one benefit is that grouping symbol mismatch will be reported in the middle of a block rather than at the very end (i.e., closer to where the actual mismatch probably is located, for large blocks with many levels of grouping).
In all of these cases, you should make sure that information about both position in the file & the tokenization & parsing context are available to every function, for error reporting purposes. If your syntax is sufficiently simple & consistent, this will be enough to ensure that you can produce useful error messages. Never expect any general purpose parsing or lexing tool to understand enough about your language to emit useful error messages; instead, expect to thread your own context through your implementation.
I agree with Mr. Bright about the proper way to handle errors: rather than trying to make the compiler or interpreter smarter than the programmer, it makes more sense to exit and print a sensible error message. However, he doesn’t touch upon the difference between compile-time and run-time errors here, and in an interpreted language or one that is compiled piecemeal at runtime (which will be typical of the kind of language somebody writes today) the distinction is a little more fuzzy. Rather than, as Mr. Bright suggests, exiting upon encountering any error, I recommend keeping information about current errors (and the full call stack information related to them) and going back up the stack until an error handler is reached; if we completely exhaust user code and get to the top level, we should probably print the whole call stack with our message (or some large section of it). This is the style used in Java, Python, and sometimes Lua. It’s straightforward to implement (Mycroft does it this way, with ~10 lines of handling in the main interpreter loop and less than 40 lines in a dedicated error module), and it can provide good error messages and the framework for flexible error handling in user code (which itself can be implemented easily — see Mycroft’s implementation for the throw and catch builtins).
With regard to performance: it’s not that performance doesn’t matter, but instead that performance matters only sometimes. Machine time is much cheaper than enginer time, and so a language that makes solving certain kinds of problems straightforward is valuable even if it does so slowly. A general purpose language has to run fast and compile fast in order to compete, but notoriously slow languages have become popular because they served particular niches, even in eras when processors were much slower and speed mattered much more (consider perl in the text-processing space — although, because of accidents of history, it took a positon better served by icon — or prolog in the expert system space; prolog was so well-suited to the expert system space that expert systems were prototyped in prolog and then hand-translated from prolog to C++ in order to avoid the performance overhead).
Unless you make huge dumb performance mistakes (and as the success of Prolog, PHP, Python, and Ruby makes clear, sometimes even big dumb performance mistakes aren’t fatal), if you scratch an itch the performance won’t be a deal-breaker, and optimization can come later, after both behavior and idiom are appropriately crystallized. Using a profiler is overkill in early iterations of the language, and since there’s a general trend in hardware toward many lower-power cores, you may get significantly more performance benefit from making parallelism easy to use (as Go does) than from tuning the critical path.
Lowering
Lowering is just an inverted way of looking at keyword minimization. When several mechanisms perform the same basic task with only small differences (as is the case with the three big looping constructs in imperative languages that Mr. Bright mentions), it makes sense to write a general function and then write syntactic sugar around it for the other cases, and if for some reason you have failed to do it that way in the first place then refactoring it (i.e., lowering) makes sense. However, I would recommend spending more initial effort identifying how to write well-factored builtin functions in the first place, rather than iteratively refactoring redundant code, because you will save debugging time & avoid problems related to minor differences locking in separate implementations that should be merged (for instance, in Python not only are dictionaries separate from objects, but there’s also a distinction between old-style and new-style objects, even though implementing objects and classes as a special case of dictionaries as Javascript and Lua do would make more sense).
Runtime library
Nearly all of Mr. Brights points about runtime libraries only really apply to languages competing with C++. He focuses on low-level stuff necessary for cycle-heavy processing on single cores, but this is almost entirely irrelevant to most of the code that gets written these days.
Here are my suggestions:
1. String I/O should be unicode-aware & support utf-8. Binary I/O should exist. Console I/O is nice, and you should support it if only for the sake of having a REPL with readline-like features. Basically all of this can be done by making your built-in functions wrappers around the appropriate safe I/O functions from whatever language you’re building on top of (even C, although I wouldn’t recommend it). 2. It’s no longer acceptable to expect strings to be zero-terminated rather than length-prefixed. It’s no longer acceptable to have strings default to ascii encoding instead of unicode. In addition to supporting unicode strings, you should also probably support byte strings, something like a list or array (preferably with nesting), and dictionaries/associative arrays. It’s okay to make your list type do double-duty as your stack and queue types and to make dictionaries act as classes and objects. Good support for ranges/spans on lists and strings is very useful. If you expect your language to do string processing, built-in regex is important. 3. If you provide support for parallelism that’s easier to manage than mutexes, your developers will thank you. While implicit parallelism can be hard to implement in imperative languages (much easier in functional or pure-OO languages), even providing support for thread pools, a parallel map /apply function, or piping data between independent threads (like in goroutines or the unix shell) would help lower the bar for parallelism support. 4. Make sure you have good support for importing third party packages/modules, both in your language and in some other language. Compiled languages should make it easy to write extensions in C (and you’ll probably be writing most of your built-ins this way anyway). If you’re writing your interpreted language in another interpreted language (as I did with Mycroft) then make sure you expose some facility to add built-in functions in that language. 5. For any interpreted language, a REPL with a good built-in online help system is a must. Users who can’t even try out your language without a lot of effort will be resistant to using it at all, whereas a simple built-in help system can turn exploration of a new language into an adventure. Any documentation you have written for core or built-in features (including documentation on internal behavior) should be assessible from the REPL. This is easy to implement (see Mycroft’s implementation of online help) and is at least as useful for the harried language developer as for the new user.
After the prototype
If your language is useful, you will use it. Maybe some other people will as well. Writing a language is a great learning opportunity even if nobody uses it, since it improves your understanding of the internals of even other unrelated languages.
Unless you are selling some proprietary compiler or interpreter (something that even Apple & Microsoft can’t get away with anymore) adoption rates don’t actually matter, except for stroking your ego — which is great, because there are a lot of languages out there and relatively few people are interested in learning obscure new ones.
If your language gets any use, it will grow and mature into something harder to predict as behaviors change to be more in line with the desires of users (even if the only user is yourself). When languages grow too fast or are built on top of a shaky foundation they can become messy, with new behaviors contradicting intuition; some languages have taken a tool-box approach and embraced this (notably Perl and to a lesser extent TCL), and other languages merely suffer from it (notably PHP and C++). It makes sense to try to make sure that new features behave in ways that align as closely as possible to idiom, and to develop a small set of strong simple idioms that guide all features; that way, pressure to become general purpose won’t make the learning curve for new users too rocky.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
If you understood this essay as advocating for some language, you didn’t read it carefully.
I’m familiar with D, and I like it a lot. I’m also familiar with the story behind how D came about.
I’m not even remotely advocating for Mycroft. Mycroft, as a language, is a poor fit for basically all problems, and is also very broken. However, it provides clear examples of solutions to several language design problems. I don’t recommend even solving problems related to language design in a way similar to how I did in Mycroft — especially parsing, which barely works for the reasons I mentioned.
My point is that D is an atypical case, with regard to the kind of programming language that someone who has never written a programming language before would begin writing in 2017. In other words, Mr. Bright’s original article, if read by some beginner programmer, would lead them to attempt to write something very much like D — and one D is more than enough, not to mention that writing another D is much harder than writing another Lua or Forth.
Outside of the extremely competitive “general purpose language” group (red in tooth and claw, with Google, Apple, Microsoft, and Oracle all pushing heavies), statically typed compiled languages of the type D represents are vanishingly rare & all good entries begin by resembling esolangs. Language design is one of those circumstances where letting your freak flag fly is a recipe for minor success & being too normal (as D is prone to be) is a mistake.
Let me reiterate: D does everything C++ does better than C++ does it, and that isn’t nearly enough for D to accumulate more than a small rabid fanbase.
Fail to compete with C++ and your users will fit on a bus; find even a small niche (like Prolog, MUMPS, Unicon, or Color Forth) and your user base will require its own conference.
If you can solve your problems without actually changing the syntax, then you don’t need to build a language.
All languages are domain-specific, so I sort of hate the use of the term DSL to refer to what amounts to a big library with specific idioms.
The point of learning a new language is that a different syntax changes the natural way to think about problems; if a DSL makes a big difference, then that’s fine, but it’s not really comparable to language design. Writing a DSL may also reduce pain points incrementally in a way that prevents you from making much greater strides with syntax changes (in the same way that Zizek suggests the existence of charity prevents people from being serious about reform).
If a DSL is really enough, then that’s fine. But, if you’re considering actually writing a language, think carefully about whether or not a vastly different syntax would be better. If a DSL reduces the problem by 20% and a new language reduces it by 80%, a new language is justifed.
I absolutely condone writing interpreted languages in other interpreted languages. Doing so saves a lot of early development effort. A well-designed language can always be reimplemented in a faster way later (while a poorly-designed language is going to be very difficult to make even small optimizations for).
ANTLR may be superior in some way; it’s also strictly unnecessary.
I’d argue against noobs (the people ostensibly targeted by Mr. Bright’s original article) thinking that they need to buy a book to learn a necessary technology in order to build a language. There’s too much mystification of language design as is.
My point, in the implementation section, is not that using YACC is a good idea (it isn’t) or that using LEX is necessary, but that (contra Mr. Bright) using standard UNIX dev tools isn’t some kind of huge portability concern.
Generally speaking, I think the best way to implement a parser is to design your syntax so that you don’t actually need something as complicated as LEX (let alone YACC) — avoid supporting infix if possible, and definitely avoid supporting implicit order of operations— and then use the simplest tech available (which may be as easy as a string split, or may involve several regex matches). But, threading logic through LEX is fine too, although it means you’re writing your interpreter in C which is probably a mistake.
Just as a personal quirk, I like to minimize the dependencies in any project I work on, favoring features that are either standard or exist in all dominant implementations. So, between ANTLR and LEX I’ll generally choose LEX, simply because it’s 70s tech that’s already on everybody’s computer. But, if you aren’t writing your initial implementation in a high level language with built-in support for extended regex, you’re going to have a bad time to begin with.
The Mythic Function of the Zombie Apocalypse
Politics of Contagion
[1]
Our standard movie monsters deviate from their early folkloric roots in a number of major ways, but the most notable might be the general move from bewitchment to infection: where strigoi, revenants, zombi, and loup-garou are generally the result of targeted curses, post-Universal-era vampires, zombies, and werewolves are created by being bitten.
We might blame this on the general demystification of western culture, where belief in the ability to be cursed by a witch is rare — and where even self-described witches often consider curses prohibitively dangerous to the caster— or on contagion being a more visually interesting mechanism in film, (though I would argue against this, citing the use of curses in J-horror and post-Suspiria Giallo, not to mention high-budget fantasy like Harry Potter). Instead, contagion spread because in an age of mass-media propaganda and heterogeneous populations, it’s a more flexible metaphor for our political anxieties.
This innovation, the idea of monstrous contagion (which probably can be credited to Stoker), has its most interesting manifestation in the zombie apocalypse narrative.
Political ideas are, functionally, heuristics about how best to run the world. Such heuristics can be reasoned about, and we can talk about what kind of world benefits best from certain political positions. Scott Alexander suggests that the set of tendencies we associate with the far Right , (a heavy focus on physical defense and self-sufficiency, careful gate-keeping and control over population, and a distrust of social services), makes the most sense in a dangerous environment, while tendencies we associate with the far-Left, (a heavy focus on equality, including trying to ensure care for the sick and poor), spreads in an environment that’s safe and resource-rich.
In Alexander’s terms, Right-wing values are for surviving in an unsafe environment, (specifically, one with war, disease, and widespread trickery), while left-wing values are for thriving in a generally-safe world, (where things are generally trustworthy and the marginal cost of risky gambles is lower). This form of Apocalypse myth will appeal to people with Right-wing values: they present a rough world where only people with Right-wing values are able to survive. By suggesting that those conditions emerge in the very near future, such as in the first three Mad Max films, they justify a survivalist impulse.
Zombie apocalypse stories are a refinement of this genre, made to appeal to specific, timely ideas. I don’t think that this is necessarily the result of the filmmaker’s ideology; instead, shifts in the political climate have changed what kind of media resonate. Rather than embracing the same kind of 50s family-based patriarchy that you’d see in early nuclear apocalypse movies, (as parodied in Blast from the Past), the new cozy catastrophe features a band of free agents. Repopulating the world is explicitly made out to be a bad idea. We see some of the seeds of this kind of narrative in the more subversive takes on nuclear apocalypse stories that were showing up around the time Night of the Living Dead came out. A Boy and His Dog seems like a particularly good case.
Zombies themselves vary wildly. Sometimes they hunt by smell and other times they hunt by hearing or sight. They exist on a spectrum from totally mindless automatons to merely distracted animals. Sometimes they are dead and sometimes they are merely ill. Sometimes they move slowly and other times they move like lightning. The cross-genre commonalities specifically construct an environment that favors current Right-wing values:
1. By the time anybody notices, there are a lot of them. Any remaining humans are surrounded by enemies and are justified in hoarding weapons and building walls 2. Zombie-ism is infectious or effects all dead. Remaining humans are justified in being suspicious about other remaining humans, and justified in treating them as potential threats. Not only is living in large groups or caring for the sick a drain on scarce resources, but it also dramatically increases the likelihood of contagion within the human community from inside. 3. Only small pockets of humanity remain. There are no social structures still in place to provide resources or care for the ill, so people are justified in behaving ruthlessly. 4. The zombie infection creates clearly visible physical changes. Zombies cannot pass among humans unnoticed for long, and all zombies are threats, so treating people as threats based on appearance is not merely acceptable but actually necessary
There are also common story-lines that appear in zombie apocalypse media. For instance:
1. Story: A lover or family member of a survivor is infected, and the survivor keeps the infected family member around for longer than is safe out of an irrational attachment. Moral: Emotional connections are potentially dangerous; only ruthless and detached behavior is safe. 2. Story: Our small band of survivors finds a village that is doing well enough to start re-building parts of society, but that village is already harboring infected people or has unacceptable practices. Moral: Large groups harbor dangerous free-riders, and in a resource-poor environment social services can only be provided at an unacceptably high cost.
Why does it matter that a genre has such a clean mapping to an ideology? The stories we tell and consume have a feedback loop with our expectations. It’s not that we can’t distinguish between fantasy and reality, but instead that we will, when suspending disbelief, treat internal consistency as evidence of generalizability, even when the premise presents an extreme or pathological situation.
A pathological premise — one that, when looked at carefully, has unintuitive side effects — can be extremely useful for widening the imagination, so long as the pathological premises you consume are sufficiently varied; however, when a popular genre embeds a pathological premise into its definition, the side effects, (whose value lies in being hard to predict from experience with the real world and with other stories), become common sense. The zombie apocalypse genre embeds a pathological premise that is a superstimulus for Right-wing values: it presents a wholly unrealistic premise that, if taken seriously, justifies what half the population of the western world already believes.
It’s evident that many people, (including groups that should know better), have assimilated ideas from the zombie apocalypse narrative. This is particularly clear when looking at the kind of organizations that have used the structure of these stories, (rather than recurring images from them), to advertise themselves. The CDC released zombie preparedness information, as a ‘fun’ way of teaching people about CDC informational pamphlets for things that actually exist; of course, the biology of a zombie virus is improbable in ways that specifically prevent organizations like the CDC from being useful in those stories, (such as spreading too quickly and having too high of an infection rate). This CDC project was in collaboration with FEMA, who used it as an excuse to pass on general disaster preparedness information — but the zombie apocalypse story differs greatly from what happens in actual disasters by discouraging people from banding together. While the zombie apocalypse tie-in may have increased the audience for this information, the popularity of the zombie apocalypse narrative, (and before it, the nuclear holocaust narrative), seriously damages our intuition about what real disasters are like. It’s too easy, likewise, to generalize the lessons of a zombie apocalypse story to any kind of civilizational collapse — it’s too easy to take the lesson that the strange behaviors of zombie apocalypse survivors are part of ‘human nature’ rather than the result of very specific, nearly impossible pressures.
Fiction is a very powerful tool. By manipulating the expectations of its audience it changes their behaviors. So, we must be careful of the kinds of stories we tell. Luckily, even in the genre of zombie films, we have some mutations that break the set of assumptions discussed above.
Night of the Comet is a zombie film with some characters who turn slowly but without infection. Instead, all the zombies are created by a single event (dust from the tail of Halley’s Comet causing desiccation), and those characters who turn slowly had less exposure to the dust. In Night of the Comet, characters who are turning can be identified; the threat is quickly over, and banding together with other survivors is not only necessary but actually desirable — in a rare properly-happy ending in zombie apocalypse films, our protagonists begin to rebuild society by getting hitched and adopting foster children.
Dead Snow is a zombie film without zombies. Instead, it features traditional revenants — dead Nazis are given a magical pseudo-life, along with extreme strength, by the power of their greed. The revenant is an interesting candidate for the role of anti-zombie, because a revenant is necessarily rich (there’s an untapped mine of stories involving the Walt Disney Company as a revenant — Walt Disney’s corpse protecting his stash of IP, growing bigger and stronger over time). Dead Snow doesn’t do much ideologically with this premise, but again in the absence of infection, banding together is encouraged.
We don’t live in a world where the zombie apocalypse is possible or where its assumptions are valid. We live in a world where some people are trustworthy and others aren’t, and where working together is often but not always worth the risks. Ours is a messy universe and we deserve stories that prepare us for that complexity. Be careful of stories that tell you to discard your most important tools.
Tempting Fate: an asymmetric card game for psychics
Tempting Fate: an asymmetric card game for psychics
Tempting Fate is a game for two players.
Game layout
[1]
Terms
The querent is the player who takes an offensive position.
Fate is the defensive player.
A path is a line of cards set in front of Fate. The goal of the game is for the querent to defeat Fate by attacking and extinguishing paths.
A token is an item, such as a bead or coin, used to indicate that a card has been attack. One token per attack point is used. The bank is a pile of unused tokens.
A zener card is a card containing one of five symbols: a circle, a cross, three wavy lines, a square, and a five-pointed star. Decks of zener cards were historically used to test for ESP.
A tarot deck is similar to a typical playing card deck, but with an extra suit (the trumps or major arcana). Tarot decks are well-known divination tools. The four non-trump suits in a tarot deck are called the minor arcana, and they are coins (or pantacles), cups, swords, and wands.
Rules
One player, the “querent”, takes an offensive position and plays with a standard 78-card tarot deck. The other player, “fate”, takes a defensive position and plays with 100 zener cards (20 of each type, or five sets).
Normally, one is expected after a game to switch roles and play again.
Before each game, both players shuffle their respective decks. Each player has a five card hand.
Players alternate turns. Fate moves first.
Each of Fate’s turns consists of three actions. Acceptable actions are:
1. Add a card from the hand to one of the five possible ‘paths’. A card cannot be added to a path with tokens on it. The cards are placed face down. 2. Add a card from the top of the deck to one of the paths, without looking at it. (The same rules apply as in action #1.) 3. Remove a token from a card.
Each of the querent’s turns consist of one action: ‘attack’ the furthest-out card in any path, using a card from his hand or the first card from the top of the deck (without looking at it).
At the end of each turn, the players bring their hands up to five cards by taking cards from the top of their decks.
Points
The points for zener cards are as follows:
Circle — 1 Cross — 2 Wavy lines-3 Square-4 Star-5
The points for tarot cards are:
Cards one/ace to five correspond to their face value Cards six to ten correspond to their face value minus five Page — 1 point Knight-2 points Queen-3 points King-4 points Fool-0 points Magician-1 point Papess/High Priestess-2 points Empress-3 points Emperor-4 points Pope/Heirophant-5 points Lovers-1 point Chariot-2 points Strength-3 points Hermit-4 points Wheel-5 points Justice-1 point Hanged man-2 points Death-3 points Temperance-4 points Devil-5 points Tower-1 point Star-2 points Moon-3 points Sun-4 points Judgement-5 points World-1 point
In other words: the minor arcana are all worth their rank modulo five, while the trumps are worth their rank modulo five, excluding the Fool, which is worth zero.
Attack rules
The only cards that can be attacked are the ones in their paths furthest from Fate (i.e., closest to the querent).
When the querent attacks a fate card, he chooses a tarot card from his hand or from the top of his deck and lays it in front of the card he is attacking, before flipping the fate card face-up. If the tarot card is worth a greater or equal number of points than Fate’s zener card and the zener card is not immune, Fate’s card is defeated and both cards are discarded. Otherwise, tokens equivalent to the value of the querent’s card are placed on Fate’s card and only the querent’s card is discarded.
If a card is attacked that already has tokens on it, the point value of the querent’s card is added to the number of tokens; if this is greater than or equal to the point value of Fate’s card, then both cards are discarded and the tokens returned to the bank.
Immunity
Each zener card has immunity to a particular suit:
The circle is immune to the suit of coins/pantacles The cross is immune to the suit of swords The wavy lines is immune to the suit of cups The square is immune to wands The star is immune to trumps
Furthermore, each zener card is immune to any trump with exactly its point value.
The Fool
The Fool can attack and defeat a Fate card without turning it over. Such a card will be returned to the bottom of Fate’s deck. The Fool is worth zero points against any Fate card that is face-up.
Ending conditions
The game ends when either the querent removes the last card in any path or any path becomes more than five cards long. If the querent removes the last card in a path, the querent wins. If a path becomes more than five cards long, Fate wins.
The game also ends when all cards are extinguished, in which case Fate wins, or when one player extinguishes all of his or her cards, in which case the other player wins.
If a player refuses to take all of the actions associated with his or her turn, that player throws the game.
For shorter games, players can agree to end the game when the number of discarded cards exceeds some number. In this case, the winner is the one whose discarded cards have the greatest combined point value.
Post-game divination
You can treat a game of Tempting Fate as a way to jointly cut the deck, and then use your favorite tarot layout.
Alternately, you can perform a reading with only the discarded cards.
You can also shuffle your hand and produce a 3-card spread.
Lol Koibito Cafe.
Luckily, businesses (and business concerns) are basically completely irrelevant.
If your startup is decentralized then your startup is making a poor business decision, but calling it a startup is a good indicator of that too: “startup” is a term for “small business” used exclusively by people who think VC funding is a good idea.
If your open source project uses decentralization, on the other hand, then it’s the product of sensible design. After all, an open source project cannot necessarily rely upon the persistence of any particular hardware, nor can it rely upon having a team actively update it to adapt to changes in infrastructure: a flexible peer to peer system is the only way to make sure such a project (if it depends on networking with other nodes) survives the inevitable expiration of whatever companies provide it with early support.
Now, yes, blockchain tech is overused. It’s overused because capitalism rewards con artists who jump on misunderstood buzzwords. (When money isn’t involved, there’s no net benefit to maximizing the number of users running your code, so marketing becomes a non-issue and these incentives toward trendy but ill-fitting tools go away.)
IPFS is a good example of a useful technology that has incentives at odds with capitalism. IPFS provides content addressing, which is great: it means permanent static addresses (something the web fails to provide, leading to the necessity of hacks like the wayback machine, and leading to anti-patterns like domain sitting & malicious redirects of formerly-popular sites). IPFS also does some caching and routing, which is also great: it means that even if your original host goes away, popular content remains accessible from the same address indefinitely. But IPFS runs counter to capitalism, since the bait-and-switch and the ability to centrally control distribution are necessary parts of extracting money from information flows. IPFS presents something similar to ‘free energy’: a system for hard-to-meter and hard-to-interrupt data sharing that’s about as straightforward as the web; if I worked for Facebook I’d probably be opposed to it, and if I worked for the MPAA it would terrify me. As someone who uses computers, though, it presents a set of very useful tools for solving everyday problems.
The community around the newsletter is still pretty active on slack, right?
The community around the newsletter is still pretty active on slack, right? Why not queue up a bunch of guest posts from that group, and accept submissions for guest posts from subscribers?
(This might mean keeping the patreon paused — after all, it wouldn’t be you writing each email.)
I would certainly be willing to write a few if you did this. And, you could probably do it that way indefinitely. If you got the impulse to write your own, you could just inject it into the queue like anything else.
The work would come down to making sure submissions are on topic, which is easier than actually writing them.
Contributor represents the easiest way out of our adtech dystopia, and it’s a shame that it was discontinued
I paid for Contributor when it existed, because it represented a small-scale change in incentives. (A big flaw is that Contributor would never, even at the highest rates, replace all ads. If it replaced all ads, it would mean that most of the reasons for running ad blockers would go away.) Contributor doesn’t fix the incentive to split long articles into many small pages, but it fixes the incentive toward disruptive and annoying ads, while lowering the barrier for transitioning from ad-driven to micropayment-driven monetization. After all, the mechanisms and rates don’t change (unlike something like Youtube Red, where youtubers get paid based on a totally different algorithm for Red subscribers than for ad impressions).
Contributor was discontinued around the time Youtube Red had its first big marketing push — which is funny, since Contributor never affected youtube ads.
My ideal model for something like Contributor would be a system by which I pay (from some store of escrowed credit) exactly the fees that the system would normally charge advertisers for the ads that I would be seeing. I would be charged when my pool of credit went below some minimum balance. Revenue from my adsense account would optionally go into that same pool. Google seems to have the capacity to implement it this way, although maybe there’s some complexity in the ad-bidding system or in contracts that I’m missing.
The perils of identity
The perils of identity
I try to remind myself (despite using words like “I” and “myself”) that, in a meaningful sense, “I” don’t exist. There’s no essential eternal personality that drives my behavior; no soul sitting in a driver’s seat making executive decisions. Instead, my behaviors are just that: behaviors, resulting presumably from an interference pattern between many competing semi-autonomous processes in my nervous system. The sense I have of myself is a model I have created by observing my own behaviors and predicting my future behaviors (something we know from experiment is very unreliable), and I don’t have any special access to my real motivations that an outsider doesn’t — instead, I merely have a greater capacity for self-delusion. To myself and to others, I am a loose collection of habits and biases, and the mechanisms that produce those habits are of only academic interest.
I remind myself of this because I’ve seen the kinds of mistakes that tend to happen when I forget — and the kinds of mistakes that happen when others forget. It’s easy to apply selective memory based on failed predictions: “I would never have done that, so something else must have caused it; I’m a good person so there must have been extenuating circumstances.” People’s models of their own behavior are subject to cache poisoning, and comparing to vague normative models (“I am a good person”) produces perverse incentives toward systematic self-delusion. However, you are what you pretend to be — or, more accurately, your behavior determines which classification you and others would be best off applying in order to predict your future behavior — and so if you have a goal model (such as being a good person) you’re best off focusing on your failures rather than your successes. Successfully achieving such a goal is hard, miserable work — falsely believing yourself to be wonderful is easy and fun, since it’s all reward, while holding yourself to a high standard requires mostly punishment — but at the same time, the bar is often very low because your peers give into the lure of intellectual dishonesty.
When we get into groups the problem gets worse. Tribalism is processed on pre-linguistic parts of the brain, and tribal behaviors will happen automatically and without observation if you don’t make an effort to consciously observe your own behavior. We perform gatekeeping, signalling, and shunning the unbelievers automatically: what we emit seems like language, but it doesn’t contain meaningful statements, or the statements it contains are tangential to the intended signal and their truth values irrelevant.
I recently saw a bumper sticker that said “Frank Sinatra didn’t need a website”. It’s a trivially true statement: Sinatra died prior to the invention of the web, so he achieved his fame without websites. Genghis Khan didn’t need a Big Mac; Jesus didn’t need a hot rod; Joan of Arc didn’t need a ballpoint pen; Hitler didn’t need pokemon cards. Buying a bumper sticker and putting it on your car isn’t something that happens without quite a lot of activity, and manufacturing a bumper sticker requires even more, so putting an irrelevant and trivially true statement on your car is rare. Obviously someone saw that statement and decided it meant something about their personality — presumably, they identify with the first half of the twentieth century and see technologies from the second half as belonging to the outgroup. Yet, the actual content of the statement runs counter to the intended meaning here: Sinatra didn’t have a website, sure, so he relied upon the Mafia, the print media, the TV, radio, and record industries (all of which were new at the time), and Las Vegas (a completely new city built from scratch for the purpose of skirting regulations, with new entertainment industry structures and new social technology like lounge clubs with performer exclusivity contracts); had Sinatra been born later, he would have taken advantage of the web the same way that he took advantage of radio and television. In other words, this bumper sticker says nothing meaningful to anybody aside from indicating a vague and incoherent bias on the part of the owner of the car. Such a bias can be transmitted more effectively through symbols: rather than talking about Frank Sinatra, why not get a decal of Humphrey Bogart? By separating our sense of identity from our sense of group identity, we can reason about our own biases, and not only can we hedge against them but we can seek to communicate them in a more effective way.
Political memes are a major form of group signalling on Facebook. We all have a friend who uses them too often. Signalling your politics isn’t without value: political positions are, theoretically, proxies for your stance on meta-ethical issues. Does life have inherent positive value or does it gain value from its potential? Is it better to be forced to live well or to have the opportunity to live poorly? Does intent matter or do only consequences matter? Is the purpose of justice revenge or rehabilitation? Is it the responsibility of the community to support itself, or is the community simply a temporary structure for holding individuals without mutual responsibility? These questions are theoretically answered by somebody’s political position, and they impact many different kinds of interactions which wouldn’t seem to have a political dimension at first glance. However, without the ability to step back from identification with one’s political position, one is not able to reason about intent: Am I trying to convince other people to adopt my position? Am I trying to change or reinforce group norms? Am I trying to indicate my position to peers in order to allow them to predict my future behavior? Am I trying to indicate my position to strangers so that people who agree with me will talk to me? Without the ability to step back from identification, one cannot even reason about whether or not one’s stated political position is even an accurate reflection of one’s beliefs: I don’t believe that the purpose of justice is revenge & I think communities have a responsibility toward their members, so maybe I’m no longer a republican but actually a communist. I think people should have the ability to choose how to live their lives even if they make poor decisions, so maybe I’m an anarchist. I think a really crappy life is worse than no life at all so maybe I lean toward anti-natalism. This kind of introspection is part of what makes labels like these remain useful.
Imagining yourself to be a consistent and stable personality can be comforting, and it can save effort in the same way that stereotypes save effort: as long as deviations from the stereotype are unimportant & you don’t enforce the stereotype by punishing people who don’t fit it, you can quickly predict behaviors of large groups based on heuristics. Likewise, feeling like you belong in a group and that members of that group are on the same wavelength as you are is comforting. But you aren’t a stable personality, and groups change; if you aren’t sufficiently intellectually honest you will lack the flexibility to change the way you think of yourself in response to changes in your behaviors, and you will be unable to leave a group that gives less and less of value back to you.
I don’t think this is an internet thing per-se.
I don’t think this is an internet thing per-se. What you describe seems to happen with subcultures in general, but the internet increases speed and scale so that these tendencies are amplified (and does a lot of the time-binding necessary for creating institutional memories useful for inter-group rivalries automatically).
4chan & other imageboards are an interesting case regarding time binding because they don’t typically have a good archive facility the way their most direct competitors (reddit-alikes and discussion forums) do & they lack reasonable logging defaults the way that irc does (while irc servers don’t keep logs, every irc client has a logging facility and most enable it by default, so any conversation on irc is probably being logged). This means that the institutional memory is being cultivated by insiders — even the parts of it that are used by outsiders — which is a little like a secret society publishing heavily-redacted copies of its meeting minutes. But the upper limit on speed and scale on imageboards is so much higher (and effortposting / reading the whole thread is considered a newbie mistake, further accelerating this) that both the mutation rate of cultural signals and the degree to which those signals get overemphasized is much larger.
We’ve had heavily intellectually-incestuous groups in the past, often driving big major cultural forces. We call them art movements or skunkworks. Now nearly every affinity group is like that because communication is fast and cheap. The material impact of this change in scale will probably have even stranger side effects than the ones you describe here.
Labor in Japan
Labor in Japan
(This is a response to a reddit thread, which was locked while I was composing it. Since misinformation about work conditions in Japan is widespread, I’m posting it here. I don’t have any particular special insight here, other than that I pay attention to news stories about labor problems in various japanese industries & to how those issues are covered in japanese media.)
Japan isn’t doing capitalism “correctly”; if anything, ideas about expected behavior have caused it to be more warped. Normal white collar office work in Japan looks a lot like the warped work culture of SV and of the AAA game industry in the US, with unpaid overtime being extremely common. (This is a big driver of Karoshi.)
The Japanese media industry relies pretty heavily on exploitative labor practices, because it’s considered low-status even though it accounts for a lot of exports.
For instance, comics & fiction typically get serialized in cheap magazines in a kind of pulp model, where people are signed and paid per chapter but will lose the ability to continue if they submit chapters late, and the pay rate isn’t really sustainable for what amounts to 80 hour weeks (which is normal for weekly manga written & drawn by a single artist).
Live action film and television is also low-status, although I don’t know about pay or working conditions offhand.
Animators are independent contractors paid by the cut (and required to do corrections for free, often on a very short deadline — in other words, if they receive inadequate initial instruction & draw something that doesn’t match the previous or following scene, they will need to completely redo the animation for free, and this sometimes happens several times).
The pop music industry in japan is extra exploitative because it has a heavy focus on girl groups / pop idol acts consisting of minors, with anybody over the age of 14 or 15 typically getting expelled from a group and replaced; the big players in this industry take copyright litigation a lot more seriously than is done in the west, and merchandising and tie-in stuff is done at a much larger scale, while the artists are considered disposable figureheads.
Sure, Japan has collectivist tendencies which sometimes manifest as thinking of the greater good. However, more often, those tendencies are warped by capitalist incentives into tolerating exploitative practices out of a feeling that this toleration helps one’s immediate peers or family (ex., not doing free overtime would be screwing over your coworkers, even though these death marches only exist because everybody does free overtime).
Criticism that’s primarily transmitted by teachers is limited in scope; after all, most people who have read this series have not done it because of its place on some curriculum (and, at least in the US, where various politically powerful groups see it as problematic for very different reasons, children often read it in spite of educational institutions).
The most sensible mechanism for transmitting criticism is probably another work of fiction — one that makes the implicit problems in the Harry Potter universe more explicit. Such fiction is relatively rare (the closest I’m aware of is Lev Grossman’s The Magicians and Elizer Yudkowsky’s Harry Potter and the Methods of Rationality, and of the two, The Magicians comes closer to a real criticism of the pedagogy despite making the focus a significantly more progressive institution and barely touching upon the arbitrary segmentation).
There are lots of low-hanging fruit here. Consider a Harry Potter like story where the Dumbledore character is the antagonist — an initially charismatic and likeable character who is later revealed to be using his institutional power to extend a vendetta against the Voldemort stand-in who was a scapegoat for Dumbledore’s actions taken as part of his rise to power. Or, consider the possibilities of expanding the focus on the Hagrid character — someone who becomes a tool of an institution because the difference between his gentle nature and his imposing physical presence makes him shunned by most of society and therefore easily manipulated by anybody who shows him kindness. Where The Magicians focuses on breaking down the idea of the institution as necessarily mostly-good (by amplifying the kind of dangers already present in Harry Potter’s institution) and breaking down the idea of the chosen one (by making the protagonist an outsider whose drive causes him to keep showing up even though he almost inevitably fails), and HPMOR mostly focuses on breaking down traditionalism, serious criticism of class and institutional power in Harry Potter is rare.
If we had a proliferation of stories that deconstruct the assumptions of Harry Potter, people would naturally come to occasionally read them in conjunction with each other; furthermore, teachers could assign them together (in the same way that teachers often assign students to read both 1984 and Brave New World).
The Harry Potter universe is a master-class in dysfunctional power structures that almost every reader managed to sleep through. Exposing it through narrative means has enormous value.
This comment wasn’t intended as snark.
Please, somebody who subscribed, tell me: is it actually worth $5 a month?
Is popularity even desirable?
Looking at my Medium front page, I see endless articles promising me tips (as a writer of Medium posts) for how to gain more views. Nevermind the content of these articles, or whether or not the tips are effective: why do these exist? Obviously, for some people, the easiest way to gain more views is to promise to tell people how to gain more views — and as far as I can tell, this seems to be working.
Because I’m not terribly concerned with maximizing views at the cost of writing content that I personally find interesting, I’m not going to tell you how to maximize views. Instead, I’m going to step back and try to figure out under what circumstances (and to what degree) popularity is even valuable.
Most authors on Medium are not writing primarily for paid publications or for subscription users. Most authors are, in fact, readers whose posts are mostly comments. Medium made an excellent design decision when it eliminated the distinction between a top-level post and a comment: the result is that comments look more like top-level posts (i.e., thoughtful, carefully structured, and often longer than a few sentences), and therefore, comment threads aren’t the kind of low-effort purgatory they are on Youtube (or even Reddit). Authors on Medium therefore inhabit this middle realm, writing something that hovers between article, essay, and comment, and doing so with a competence level in that no-man’s-land between talented amateur and budding professional.
Even people who write for money often write for free on Medium. (I have written paid Medium pieces but nearly all of my pieces are not commissioned; professional journalists like Sonya Mann write stuff like the excellent Exolymph here — and while that has a Patreon, I don’t think the primary goal is profit — and professional authors like Nassim Nicholas Taleb and Mitch Horowitz write new stuff for Medium, often not behind a paywall. Hell, Doc Searls writes new stuff and reposts older articles here all the time.) It’s my suspicion (and if any of those tagged deign to respond, you can correct me on this) that any potential profit made directly from Medium posts is incidental, even to many professional authors and journalists, and that even when one of the purposes of one’s online presence is marketing-related, this kind of writing is substantially different in intent from book tours, resume-padding freelance articles for well-known print-publications, and other kinds of more widely visible or explicitly self-promotional work. As an example of this kind of thing outside of Medium, Charlie Stross and Peter Watts both keep personal blogs full of interesting tangents, and while these blogs occasionally make announcements, they seem to primarily serve as a way to record passing interesting thoughts by the author and engage feedback from fans. This serves a certain kind of marketing purpose: reminding people who are already likely to buy these guys’ books that the authors are thoughtful people with interesting ideas even when new publications are nearly a year away, and maintaining engagement, while also giving fans a sense of personal connection and skin in the game, even if that sense is going to be mostly illusory except for extremely dedicated commenters. But, it also provides other functions: even professional authors benefit from daily practice, and the work done for seemingly off-topic posts (such as Peter Watts covering the occasional neuroscience paper, or Charlie Stross making near- or far-future predictions based on current political and economic trends) adds material to the mental compost heap that may fertilize later novels.
To what degree does wide popularity of their individual posts benefit these people? Stross and Watts are mid-list SF authors: while they’d benefit materially from suddenly graduating to the big leagues and selling as many copies as Neal Stephenson and William Gibson, their blogs really aren’t structured around cultivating that kind of growth (and probably couldn’t be); instead, they cater specifically to the kinds of fans who are interested in these authors’ niche side interests. Were their blogs’ popularity to suddenly skyrocket permanently, rather than being a positive, it would actually probably be a net negative for the authors in several ways: where the comment sections had previously been mostly full of dedicated core fans, many of whom recognize each other and have years-long relationships with each other and with the authors, after the explosion of new users the culture would change because the vast majority of users would be unfamiliar with most of the author’s work and have no prior connection to the community; furthermore, server costs and the time and difficulty related to (currently volunteer-based) moderation would skyrocket, and any corresponding book sales that might occur are unlikely to translate into paychecks for months. In other words, there’s a pretty narrow target range for the popularity of these blogs: median traffic for a 3-month period probably shouldn’t grow by more than a third between two consecutive three-month windows, or else the culture of the comment threads is liable to be destroyed.
Likewise, while I am familiar with Sonya from Exolymph, most people are probably only familiar with her work for Inc — and don’t even know about her newsletter or her blog posts. In other words, she’s attached to an infrastructure that, by itself, provides far more promotion for her professional work than the stuff she does here. The same is true of Klint Finley— I’m familiar with him from Technoccult and Infictive (and through mutual friends in the PDX occult community) but most people only know him from his work for Wired; his posts here are mostly about tabletop RPG design, and so almost definitely don’t cater to the Wired audience. This goes double for Doc Searles (since Dr. Dobbs’ Journal is a major institution) and for popular published non-fiction authors like Horowitz and Taleb: none of these people need to self-promote, and the likelihood that their work on Medium is producing a significant direct increase in sales is extremely low. (In the case of those writing books, maintaining an audience that will buy these books consistently is something that blogging can do, but the majority of even consistent buyers probably are unaware of the author’s blog or uninterested in it, particularly when the subject matter differs substantially from the author’s books, as it does with Taleb’s blog.)
Presumably, the non-fiction authors on here are getting something out of Medium much like the SF authors mentioned above get out of their personal blogs: a limited-audience space in which to experiment with new ideas, where the audience is primarily composed of the most dedicated subset of repeat readers. For journalists & authors who work in several genres, this audience is even more rarefied: Sonya & Klint’s audience of tech-industry-gossip readers are unlikely to want to follow their blogs here, whereas people who are interested in several of the subjects they write about will follow them to see that intersection. While Medium has no author-level server costs associated with it, the other negative effects of a massive audience increase are still going to be present.
If people who get paid to write don’t benefit from maximizing their audience on Medium, why should someone who doesn’t get paid to write benefit?
There are a couple possible reasons, but I think all of them are somewhat limited in scope.
One reason is that authors who are looking to become professionals can build up a portfolio of high-quality work, and having some of that work become very popular means that the likelihood that an editor will stumble upon some work and invite the author to submit pitches is somewhat increased. However, an author’s most popular work is often not their best: what an editor would like to pay for is not the same as what a casual reader is willing to click a little heart button on. (For instance, with the exception of a paid piece written for a marketing agency associated with a popular book and TV show, my most successful post on Medium in terms of both read count and like count is three lines of snark about Steve Jobs and Steve Ballmer.) Furthermore, editors already publicly post calls for pitches, and responding to one of these calls for pitches with a good pitch and a link to previous work is far more likely to result in a commission than blind luck and an audience in the thousands.
Another reason is that writing for a large audience means needing to write a little outside your comfort zone, adding information that your core audience would normally know but that a general audience would need explanations for (while carefully avoiding insulting the intelligence of either) and avoiding constructions that are too heavily tied to particular subcultures. However, while this can be a useful skill, successful authors often write for a specific niche. Furthermore, the kinds of tips presented in these tricks-for-maximizing-audience articles often depend upon the assumption that you don’t already have a niche core audience to expand from, or that you are willing to cater to a wider audience at the cost of your niche. In other words, if the goal of getting a larger audience is to get experience in appealing to two groups at once, then these tips articles are completely useless.
The third reason — and the one I find most convincing — is that having a large audience is a personal ego boost. When people click that little heart, the author feels like their value has been confirmed. That said, if your popularity comes entirely from a bullet point list of SEO tricks, isn’t that popularity completely disconnected from your actual value as a writer? If you have sacrificed your ability to write something meaningful in exchange for clicks, haven’t those clicks entirely ceased to indicate the importance of your ideas?
This kind of ‘marketing of marketing’ mindset isn’t limited to writing. Most forms of creative work are subject to it — particularly musical acts and open source projects. Much like novelists, most people who start a band or write an open source project will never get paid for it. This is fine: these are things that don’t come with the expectation of profit — our culture has made it clear that when you do these things, you are sacrificing your time and energy for passion. The ‘marketing of marketing’ mindset, however, suggests that thinking in terms of cost and benefit is sufficient (if supplemented by SEO tips) for slowly transforming a hobby into a business. Some people — if they are both skilled and lucky, or if they are unskilled but very lucky — manage to do this, and working very hard slightly increases the chances of success. However, no matter how hard you work or what your innate skill level is, the odds of success are so low that you should not make decisions with the expectation of profit in these fields unless you have already been consistently profiting for an extended period of time. Furthermore, nothing is more effective at killing your love for a hobby than the stress created by a persistent false expectation of profit. Expecting to turn your hobby into your career is a dangerous path, and should be tread very carefully.
When music is in that intermediate zone of semi-professionalism, strange exploitative business structures take the reigns. (This is not unique to music: ask any waiter in Hollywood what their real job is; take a look at what vanity publishing houses sometimes promise to aspiring novelists. However, the music industry’s particular genre of exploitation is both widespread and wide-reaching, and it tends to directly affect even people who go on to become successful, while both the music industry and the publishing industry have more direct routes for people who have a leg up.) If you are making music because you love to make music*, why spend a bunch of money touring and playing at dive bars (which won’t get you exposure and won’t pay for equipment costs) when you can just record your material and release it free online (which will get you only slightly more exposure and won’t pay for equipment costs but at least doesn’t waste gas money that could instead be used on rent)? If you’re making music for the love of it, why try to get signed to a record company (who will give you an advance that they don’t expect you to recoup and then keep you on the hook for the difference) when you can stick your material on bandcamp or soundcloud (and avoid going into debt with an organization whose practices border on qualifying as a crime syndicate)? If you’re making music for the love of it, isn’t having twenty people who buy every album you put out enough to make you feel justified in trying to balance a day job and a recording schedule?
Open source software has a similar, yet even stranger, situation going on. Most pieces of open source software are small and have extremely few users. They are open source because turning them into products would be pointless and a waste of effort. They are the best kind of software: small, clean, suited to a very specific task, easily understood, and easily maintained. When a talented software engineer writes a very small shell script to solve a niggling problem that seems to bother only him, this is the end result. There are so many of these because this kind of thing happens several times a week for many software engineers, and because a simple and carefully written script can avoid becoming obsolete or requiring maintenance for years or even decades by sticking to standard features. However, if you weigh open source software projects by number of users, the distribution changes substantially: projects with large numbers of users tend to be large, complex, difficult to maintain, and full of technical and social challenges.
When a post on Medium promises tips to get your open source project a larger number of users, it mystifies me. Increasing the number of users for your open source project is an even worse idea than increasing the number of commenters on your blog. A small project with a handful of highly-technically-skilled users and a single maintainer requires approximately zero marginal effort: it can be written, dumped into the public domain, and forgotten; anyone who uses it can also fork it and make any modifications they like; it is easy enough to maintain a fork that there’s no point in migrating changes upstream. But, add a single non-technical user to the user base for such a project and suddenly you get bug reports (for non-bugs or low-priority bugs or things that the other users performed a one-line change to fix) and feature requests (for features that are completely out of scope, only marginally related, mathematically impossible, or would by themselves require several times more code to implement than the entire existing project). Grow to fifty or one hundred users and some of them will inevitably be non-technical — and just to separate the wheat from the chaff you will probably need to bring on a couple trusted people to be maintainers or administrators. Grow to one thousand users and your project now has a “community” that is more complex and drama-filled than many small towns. Yet, one thousand users isn’t remotely enough for somebody to start paying you to work on the project. By the time an open source project has the capacity to start making the maintainers money, it has already been a nightmare timesink for years.
Having something you created become popular can be a great ego boost: it cements a feeling of connection between you and the rest of the world that can often otherwise be quite tenuous. But, it’s not always — or even typically — logistically desirable. Before giving into the instinct to increase the popularity of something you are working on, step back and ask yourself whether or not the benefits are worth what you give up.
*Note: I’m not criticizing side hustles here. A side hustle, however, needs to have a high profit-to-effort ratio to be worthwhile (even if that profit is inconsistent, and even if that effort it tempered by joy). Instead, I’m criticizing the idea that a creative side hustle should aspire to become a consistent source of income and a full time career. Side hustles, particularly creative ones done partly or mostly out of passion, benefit greatly from the kind of scheduling flexibility that can only come from not being the primary source of income upon which one’s life and livelihood depends: if you’re tired you can avoid taking another commission rather than tainting your hobby by hate-working through a burnout period. Your creative side hustle probably benefits a little bit from exposure, but not only is exposure not necessarily worth doing for free what you would normally charge for, but it also is not necessarily worth doing at all what you would normally avoid.
Some chess variants
Some chess variants
Twisted chess
3d chess (with 8 boards) but each board is rotated 90 degrees vs the one above it (so traversing z twists you into a different context). Possible moves have forward replaced with down. All boards are fully loaded and turns rotate round robin between boards (each makes a move on first board, then second, etc). Pawns that make it to the far side of the lowest board become queens. Winner is the first to take (not check) 5 kings.
8–1 chess
You have 8 boards. (Each board is adjacent along one of 8 dimensions.) Each move involves a move on each of the 8 parallel games, but a valid move is to swap a piece with another (adjacent) board. Swapping occurs by switching with a piece on the same spot on another board (or, if there is no piece there, swapping with the notional ‘empty space’ piece).
Winner is first to take (not check) 5 kings.
(The kings are taken because checkmate is hard to calculate when any piece can spontaneously move into hyperspace.)
If you want to make it more interesting, you can have black move first on even-numbered boards.
(“8–2” chess would have a second order of 8 boards adjacent to each of the first 8, for 64 boards. Still playable but harder.)
Punk & cosplay
Punk & cosplay
This morning I wrote a toot about cosplay. I knew it would be unpopular, but it spawned a lot of discussion based on a number of unintended readings.
What I said was:
If you didn’t make your cosplay costume, you’re cheating.
What I meant was not that only expert tailors should cosplay. Instead, I was trying to criticize the encroachment of commerce into fandom spaces.
I don’t mean to say commerce in fandom spaces is new. It’s not. It presents a unique kind of toxicity to fandom spaces, however — one I’d like to unpack here.
Costly signalling
I consider the function of cosplay to be costly signalling. That is to say, the feeling of belonging and comunity that cosplay can create is caused by the shared perception that the cosplayer has sacrificed something for the group.
Costly signalling can take all kinds of forms. Sometimes, the cost is in money. Other times, the cost is in lost opportunities, social status, damage to the physical body, or the accumulation of blackmail material. The power of hazing comes from costly signalling, as does the power of fashion movements, internal gang solidarity, and music subcultures.
Cosplaying, in the sense that it would be considered embarassing by a normie, constitutes a form of costly signalling by itself: by dressing up in public, you have sacrificed social status with the outgroup in exchange for social status with the ingroup. There’s a second cost associated with cosplay: what is put into the costume itself.
Scaling and marginal cost
Most forms of value sacrificed in costly signalling are more or less stable. Embarassing yourself in public (as in hazing rituals like streaking) applies more or less equally to everybody — even to people who would be immune to first order embarassment (since social stigma is also involved). While access to medicine isn’t universal, costly signalling related to bodily mutilation tends to be relatively low-risk (amputation of the pinky is used as costly signalling in the Yakuza but trepanation is not; tattoos are commonly used but scarification is rare).
The big exception to the general rule that the cost of costly signalling scales is when the cost is in the form of money: the amount of dedication indicated by making a purchase is based on marginal cost, so its meaning is inversely proportional to the size of the bank account of the person producing the signal.
As a result, stable communities don’t tend to center around signals that can easily be gamed with money. Communities whose signals can be bought tend to become divided based on net worth: after all, the poor can’t afford signals that would be meaningful to the rich, and the rich cannot distinguish between signals within the budget of the poor. And, of course, those who benefit from these monetary signals (charities, clothing brands, country clubs, media conglomerates) will try to cater to the rich fans, since the rich fans have more money to blow on signalling.
The cosplay expectation treadmill
How does this relate to cosplay? Well, cosplay is a domain where the norm was originally the creation of costumes by those wearing them. When everybody has approximately the same budget, the quality of a costume is a good indication of dedication to the group — outside of a handful of outliers (people with unusually high or low skill), the association is linear.
The emergence of semi-professional costumers in the cosplay sphere has disturbed this mechanism. It is now possible to pay for a costume that is better-looking than what you could make with an amount of effort equivalent to the marginal value of the money you paid — in other words, by paying for the costume, you’ve gamed the quality/dedication axis. Because high quality costuming skill is rare, purchases of costumes center on a relatively small number of semi-professional costumers. The costumers with the highest quality are able to benefit from volume discounts, so that at the same cost they can use superior materials, making their quality to cost ratio even higher.
Over time, the association looks less and less linear — even assuming everybody starts off with the same budget, those who have bought costumes from the best costumers have much higher quality costumes than you would predict from their dedication level.
While the appropriate interpretation is a sub-linear association, the way people actually interpret this is a linear association shifted downward: if people with hollywood-quality costumes are only casual fans, then people whose costumes actually look home-made mustn’t be fans at all! This interpretation is stupid, but common.
The situation gets worse when you consider that real fandoms are economically diverse. There are Evangelion fans who can’t afford to subscribe to Crunchyroll and other Evangelion fans who can afford to buy the entire figurine line (or fly to Japan to visit the museum). Painting a leotard like a plugsuit could bankrupt the former, but the latter could have a walk-in closet full of professionally made plugsuits. Cosplay says a lot more about the dedication of the former than that of the latter.
The punk connection
If you want to see what happens when commercial interests infiltrate a DIY space, you need look no further than Hot Topic.
Punk fashion has a lot in common with cosplay. Both are forms of costly signalling; both developed in the 70s; both were associated with a subculture centered around media; both were primarily DIY; both were blown up in the media as indicators of a dangerous, embarassing group of undesirables.
The big difference is that punk became fully commercialized much earlier.
Commercialization has side effects other than substitution of effort for money. Once someone is making money off an ingroup signifier, it makes sense to maximize how many people purchase that signifier; the easiest way to do this is to lower the non-monetary cost of that signifier (i.e., to make the signifier socially acceptable to the outgroup). This kills the utility of the signifier, and turns it into a meaningless fashion statement.
Sometimes, rather than maximizing the number of people purchasing the signifier, the company producing it tries to maximize the return on each unit. In other words, the non-monetary cost stays the same and the monetary cost skyrockets.
Typically, both happen — leading to the subculture fracturing into a “casual” portion full of poseurs and a “hardcore” portion accessible only to the very wealthy. This is how we get hipster subcultures: the idolization of overpriced items like vinyl records, the normalization of ten dollar cupcakes.
Fixing the cosplay signalling axis
There’s an easy way to reduce the degree to which the abovementioned factors affect the cosplay subculture: aggressively devalue ready-made cosplay while aggressively affirming the value of low-effort cosplay. Treat a green t-shirt as more impressive than a store-bought Ninja Turtle costume. Affirm the importance of creativity and dedication while treating the exchange of money as taboo.
This is an up-hill battle. Much like the maker community, the cosplay community has already become heavily dependent upon semi-professionals, and has already expanded significantly through the normalization efforts of commercial concerns. We haven’t reached Hot Topic levels of selling-out, but we’re going in that direction, and the inertia is substantial.
If we want to put the breaks on the commercialization (and thus dissolution) of cosplay-as-community, we need to bring out the big guns. This means considering the exchange of money as shameful in cosplay communities, to a degree in excess of the actual target level of shame.
Hostility, inclusivity, and target mismatch in open source community management
The reputation that open source communities have for hostility is justified. Open source communities, particularly those surrounding older projects or those whose members are mostly part of the old guard, tend to be elitist (in the sense that they expect a high level of competence from even new contributors and react with hostility to contributions that don’t meet their standards). Whether or not this is a bad thing depends on which of several groups you belong to within the open source sphere.
There are a couple different ways to look at the purpose of an open source community. One is the FSF attitude: that licensing is a social and moral concern, and so opening up software is valuable in of itself. The second is the Cathedral & Bazaar attitude: development methods associated with open source communities are more efficient at creating reliable software than those associated with traditional proprietary shops. The third I will call the BSD attitude (not because I think BSD maintainers necessarily subscribe to it, but because people who take this attitude often use permissive licenses like the BSDL): open source development methods are sensible because, in the absence of a profit motive, there’s no point in making any particular effort to lock down the source code for your personal projects.
If you hang out with contributors or maintainers of open source projects, you will meet all three types of people. The thing is, these goals have very different ramifications.
The CatB attitude and the BSD attitude both favor hostility, in a sense. Within the CatB attitude, the primary goal is to produce good software — so, low-quality submissions aren’t worth considering, and nurturing beginners to the point where their contributions aren’t a net resource drain is a potentially risky business decision. Within the BSD attitude, the point is to minimize effort, so taking even high-quality submissions is often not worthwhile: forks are encouraged over pull requests, since looking at patches is too much work. In the FSF attitude, however, software freedom is a social justice issue to be weighed against other issues, and the codebase is just the foundation upon which the much more important community is built — in other words, the health of the community takes priority over the functionality of the code.
This is not a defense of elitism on grounds other than technical competence. In theory, discrimination along irrelevant lines damages all three of these goals [1]. Instead, this is to say that for two of the three types of open source contributors, being welcoming to contributors who aren’t already producing professional-level code literally runs counter to their goals. The kind of performative hostility toward low-quality submissions that Linus Torvalds (for instance) is known for is useful in this context: so long as people have a reasonably accurate sense of their own competence & the degree to which their code meets the goals of the project and adheres to the community’s best practices[2], these outbursts act to raise the average quality of contributions — and both good patches that are never submitted and the loss of potentially good future contributors is considered justifiable given the huge number of current contributions.
I don’t consider any of these attitudes necessarily wrong. I value high-quality code, inclusive communities, and my own free time. I think a lot of conflict comes from these concerns being juggled within the same project by people who are unaware that they’re dealing with a completely different set of goals. In particular, people who are primarily concerned with inclusiveness tend to assume that all open source projects are (or should be) primarily concerned with inclusiveness (and see people who are primarily concerned with submission quality as callous), and people who are primarily concerned with submission quality tend to assume that all open source projects are (or should be) primarily concerned with submission quality (and see people who are primarily concerned with inclusiveness as naive). Since many open source projects have adopted community guidelines as a way of codifying intended behavior, it makes sense to explicitly specify the priority of the project in those guidelines.
For my own case, I generally take the BSD attitude with regard to projects I administrate or maintain: releasing my code publicly is what I do because, in the absence of a good reason to keep the source code secret, I feel like only an asshole wouldn’t make a useful utility generally available. To that end, I am considering adding a blurb to the README for my projects indicating that, while I am happy for people to use my projects, I do not intend to provide support or take patches, and that those who want to distribute their own changes should create a fork. (This is something notably done by mewo2’s terrain generator project.) For larger or more long-running projects, I will probably simply do the pull-request hack.
For those of you who take the FSF or CatB attitude, I recommend inserting an indication of your intent into your READMEs as well. Something along the lines of “This project’s community is paramount, so contributors are expected to support each other, to the point of mentoring less experienced community members” or “The quality of this codebase takes precedence over its community, so please expect poor-quality submissions to be summarily rejected”, respectively.
[1] Obviously there are toxic and dysfunctional communities that act against their own goals, and when money is being funneled in or there’s corporate influence, these goals often get put on the back burner with profit motive — often in the form of mere continued existence — becomes the primary motivation between actions; in the case of Silicon Valley VC funding, continued existence may actually depend upon racial or sex discrimination, in the form of maximizing investment by making sure most contributors physically resemble Marc Zuckerberg.
There are also systemic biases that make it harder for certain groups to achieve competence, but of the three attitudes we discuss, only one considers trying to correct for these biases to be within the scope of their projects.
[2] The ability to evaluate one’s own competence is associated with competence — in other words, the Dunning-Kreuger effect applies. What people forget about the Dunning-Kreuger effect is that above a certain level of competence, perceived competence is systematically underestimated. So, if the base level of confidence is lower for some group (for instance, women in the open source community), the most competent members of this group may decide their abilities are below the accepted level — in other words, only a band in the middle of the competence spectrum will actually contribute.
This is a problem that huge projects concerned primarily with code quality won’t bother to address: the bottleneck is typically the time spent by maintainers evaluating patches on the border of acceptability, and there are enough excellent patches just by the law of large numbers that systematically missing high quality patches from a particular group isn’t a big deal.
The thing about pair programming is that it’s similar to other closely-collaborative creative activities. If you perform pair programming with someone you wouldn’t marry or write a novel with, you’re going to have a bad time, because it requires the same kind of skills and chemistry.
Pair programming involves having somebody point out your stupidest, most embarassing mistakes and then acknowledging and fixing them. And then you switch roles. Most of us only have the kind of relationship that allows us to be so critical without permanent damage with one or two people.
(Now, having someone of a greater skill level behind you and not switching roles is not pair programming: it’s one on one instruction. That’s fine, but it’s different because the power dynamic is different.)
Are legendary generals just lucky?
Are legendary generals just lucky?
There’s an infrastructure around legendary generals (and other figures charged with the strategies, tactics, logistics, and general management of war) — not just around teaching their techniques to new officers, but around applying cultural importance to them. We have statues of generals, universities aimed at producing effective military commanding officers, and until pretty recently history has largely been the story of the decisions of military officers rather than the story of bottom-up organization by civilians. But, it occurs to me that for all the effort we put into military colleges, the generals that get the most cultural reverence either didn’t attend them or did poorly. Are we focusing on underdog stories? Are we elevating particular generals in the popular consciousness for reasons unrelated to their effectiveness? Or, is performance in a school designed to produce effective military officers largely unrelated to the effectiveness of military officers?
There’s plenty of reason to suspect the last case, because there’s plenty of reason to suspect that military command falls into the category of something one can be bad at but not good at. Lots of things are like this, but I’d like to focus on one that I think has a lot of similairities: stock picking.
Like war, stock picking is an extremely complex adversarial system. Certainly, in both, one can pick strategies that are much worse than random: you can have all your soldiers shot for cowardice before their first sortie (losing by default) or spend all your money on the single highest-priced stock and sell as soon as its price drops precipitously. Yet, we’ve found that a completely random strategy in stock picking does better than professionals. Is the same true of war?
There are plenty of similarities. Both are a little like ‘quantum suicide’: a good recipe for creating the impression that some people are much better than others at something is to start with a large pool and drop everybody after their first big mistake; in a situation where outcomes are totally random, someone will rarely but reliably win everything or almost everything by chance, and that person will never be able to know whether or not skill played a part. We can’t estimate well whether or not somebody’s successes were fully attributed to chance in such a situation (although we would expect that some were, and we could guess by the distribution of successes). It’s difficult to determine, in that circumstance, the difference between a very difficult problem that only very few people can reliably solve and an impossible problem that nobody can reliably solve (though with a large enough pool we can find the difference between the actual success rate and the expected rate for a fully random one), and when a system is adversarial we have a good excuse for ‘skilled’ people ‘losing their edge’.
There’s a pretty big difference, however. We can test empirically how well a stock picker does: it just takes a big pool of money, and while money is rare, it’s not as rare as cannon fodder. Nobody’s willing to put a random number generator in charge of an army, even if we had pretty good evidence to suggest that a random number generator would reliably perform better than the best human general.
I’m neither a statistician nor a military historian, so I’m not really qualified to crunch the numbers on this. However, it’s a question that I’ve never seen asked, let alone answered. And there’s some import to it: if skill at war isn’t possible, our already-unreliable mechanisms for ranking military leaders are completely pointless; expensive systems for officer training would benefit from being overhauled for less of a focus on tactics and strategy and more of a focus on keeping up morale; war becomes even more of a numbers game with respect to resources; we have no more excuse to have statues of generals in our squares (and we should probably replace them with statues of common soldiers, who, in the absence of value in top-down strategy, at least contributed their lives).
Is this the first high profile example of a unicode homoglyph attack in the wild?
Is this the first high profile example of a unicode homoglyph attack in the wild? It’s the first I’m aware of, at least.
Fun fact: if we go by formal declarations, Japan is still at war with Russia.
Fun fact: if we go by formal declarations, Japan is still at war with Russia. They haven’t exchanged fire under the banner of the russo-japanese war for a hundred years.
The Lone Gunman
The Lone Gunman
There’s a lot of debate about gun control in the United States. However, both sides, by participating in the conversation at all, have a central confusion. The gun control debate isn’t (or at least shouldn’t be) about guns at all.
Gun control advocates and anti-gun-control advocates typically focus on the use of firearms in a very specific situation: when firearms are used in mass violence. The debate centers around mass shootings on one hand, and on the other hand, upon self defense against a large group of targets. Regulation debates focus on automatic and semi-automatic weapons and large clips. This is strangely at odds with reality. After all, even a machine gun is significantly less effective at mowing down large numbers of targets than a bomb — or a car. The firearm is a weapon oddly unsuited to mass murder: even for semi-automatic weapons, the ideal use case is against a single easily identified stationary target from relatively far away. As a weapon, a gun is a great deal like a bow and arrow, although a gun can shoot farther with more accuracy and with greater force, and it’s faster to reload its projectiles. This should be enough to immediately reject both sides’ arguments from the perspective of materialism: any constraints placed on guns should be placed doubly or triply on automobiles, pressure cookers, fertilizer, boats, and weak poisons. The argument isn’t about guns as physical objects.
If the best analogy to the gun as a physical object is the crossbow, then the best analogy to the gun as a symbol is the katana. Physically, the katana is a very limited weapon: it’s a sword, long and heavy enough to take a great deal of effort to wield yet with clearly shorter range than a projectile weapon or even a spear or lance; created via a laborious process made necessary by the poor quality of Japanese iron deposits and the relatively primitive state of Japanese metalworking techniques, even katanas legendary for their high quality steel would be laughed at by medieval european blacksmiths. Yet, because of the association between the katana and the samurai class (enforced by multi-century rules about who was allowed to own these weapons), the katana has incredible symbolic power. In an age where actual warfare in Japan was largely being performed by domestic copies of imported Portugese flintlocks, a sword ban was instated to keep the samurai down. Even today, Japanese cinema is full of sword users, and invents magical techniques by which the sword might act as a ranged weapon. Despite its impracticality as an actual weapon, the katana has an incredible symbolic power to the Japanese (and to some westerners) that keeps it from being ignored. The katana represents a romanticized view of the samurai, and especially the ronin — in other words, it represents the image of a lone warrior who maintains his pride despite disgrace and whose power comes from intense training and self-discipline.
It is another such image that keeps the idea of the firearm relevant in a world where most actual warfare is performed by bombs of varying degrees of autonomy: the image of the lone gunman.
Let us examine the action hero. He is a middle-aged white man — never young, never black, never blonde or a red-head. He is very much like the standard FPS protagonist. He is muscular, poorly shaven, and is usually either ex-police or ex-military (although occasionally he is still affiliated, but not considered a part of the in-group). He works alone. He fights a large and organized force of well-equipped enemies; he does not do so out of some traditional defense of “justice” or “the law” (because he is too cynical to believe in such things) but instead for some intensely personal reason (usually to protect or avenge a family member, who is most often female). Even as the enemy uses bombs, noxious gasses, poisonous injections, throwing knives, or other weapons, our action hero protagonist uses firearms; to the extent that he uses any other weapon, he does so out of necessity, improvising, after he loses his gun or runs out of ammunition, and the weapon he improvises is almost never more destructive than a gun. (This is mirrored in samurai flicks, particularly in parodies — in the first episode of Gintama, the title character destroys a highly advanced alien-made nuclear weapon by hitting it with a wooden sword, having refused to accept a laser gun previously.) The action hero doesn’t plant bombs, although he may allow the enemy to be blown up by their own bombs; when encountering a piece of destructive machinery, even after defeating its operator, the action hero will not choose to use it, except perhaps as a transportation device, and any destructive effects of such a device will be accidental — our action hero won’t steal a tank, and although he might steal an attack helicopter he won’t use the helicopter’s bombs or machine guns.
Our gun control advocates fear the action hero to some degree; after all, the action hero works toward the goal of a safe society only incidentally. Our gun control advocates also fear those actual human beings who have been possessed by the action hero / lone gunman archetype: school shooters, right-wing terrorists, and corrupt cops. To some degree this is justified: while the action hero himself does not and cannot exist, those who have sublimated themselves into this archetype can do quite a lot of damage before their luck runs out. However, in another sense, this is foolish: the terrorist who packs a machine gun instead of a bomb is a bit like the man who tries to take on the army with a sword; he has confused symbolic strength with literal strength, and the limitations of his weapon will prevent him from doing nearly as much damage as he expects. In a sense, those who fear these groups should feel lucky that they suffer under the delusion that their weapon of choice is ideal; were they to replace their media consumption with proper training and think clearly about weapons as tools, they would be far more dangerous.
On the other hand, those who fear gun control identify strongly with the action hero, or at least believe that they could become his manifestation under the right circumstances. People who hoard guns against what they see as an oppressive government are operating on action movie logic: a small group of people with automatic weapons cannot even defend themselves against a national army, although a single con artist could probably decimate a national army with some poison and a great deal of courage.
The lone gunman, though he is often associated with the religious right’s reformulation of Randian Objectivism, in a sense is a stranger bedfellow with Objectivism than the religious right itself is. No Randian hero, the lone gunman is a loser who does not win, but instead causes others to lose. He never profits from his actions, nor does he intend to; he comes into the story already damaged and rejected by a world that he doesn’t fit into, and his goal is to save someone (usually a family member) from a threat that appears after the beginning of the narrative, or to take revenge for that threat. He plays only negative sum games: his goal is to return to the same level of dysfunction he is used to, having caused harm to some third party (usually some variety of “foreign terrorist”). The family he rescues is one he is almost invariably estranged from, just as he invariably has a warped relationship with the career that gave him the training he uses: while usually a former soldier or police officer, if he happens to be a current officer he is a pariah.
I would place the beginning of the lone gunman figure in film with the release of Die Hard. The elements of Die Hard that were originally (in the style of the Last Action Hero) a satire or subversion of action movie tropes eventually became the defining traits that separate the lone gunman from older 80s-style action hero figures, and these traits are important to note: the lone gunman, though skilled, is not ‘fit’; rather than being a well-rounded person who happens to excel at violence, this figure is a loser and outsider who (in a strange warping of the hero’s journey) discovers that he has a talent for violence when he is thrust into a situation where he uses it. He may be an ex-police-officer, but he can fight off hundreds of current police officers who have better training. Much like how, out of context, the stories of popular detective characters appear to be about a person who supernaturally attracts criminal acts to happen around them, the lone gunman appears to attract swarms of unrelated attacks.
I would like to also distinguish the lone gunman figure from another star in our constellation of men of action, the hardboiled detective. While the hardboiled/noir protagonist appears to have much in common with the lone gunman — both are losers thrust into lives of violence to which they are unnaturally acclimated, within the matrix of a society they cannot integrate into — the hardboiled protagonist’s cynicism is always a put-on. A hardboiled protagonist, being a “shop-worn Galahad”, has more in common with the ronin figure or with the hero of westerns: he may pretend to have purely selfish and material reasons for his actions, but he acts according to a strict moral code he would rather not admit he adheres to. The cynicism and nihilism of the lone gunman figure is real, and in an inversion of the hardboiled protagonist, the lone gunman acts as if his behavior is justified by familial loyalty or revenge, when it is clear that revenge is just an excuse for immersing himself in a world of violence. Where all other action hero protagonists are acclimated to violence by necessity and are at least as estranged from violent exchange as they are from the rest of the social world, the lone gunman has a greater connection to violence than with the every-day. All other forms we have discussed are rejects who carry a set of moral guidelines from a world that no longer exists or is closed to them; the lone gunman has never had a home, but finds one in the process of taking revenge, and his moral sense is warped accordingly.
In other words, the lone gunman breaks from the tradition of justified violence, instead engaging in violence that justifies itself: loss for loss’s sake. Hardly sociopathic; this is instead the logic of a perpetually frustrated death wish. That this resonates with society is interesting but not impossible to predict: prescriptive codes of ethics, to the extent that they are narratively interesting, must be problematic (a hardboiled protagonist who will “never hit a woman” is foiled on several fronts, not least by wicked women who take advantage of him); furthermore, prescriptive codes of ethics also don’t age well, particularly now that widespread and fast communications across demographics have brought about a nearly scientific style of inspection of moral and ethical issues in the public sphere. An everyman whose abilities are unknown to him at the start, the lone gunman can become an aspirational figure for those who have no skills but suspect that they may discover that they too can mow down faceless waves of military police if given the opportunity. Finally, the lack of interiority in the lone gunman figure — the reliance on a supernatural luck, the lack of planning or aspirations, and the absence of intellectual rather than material challenges — is easily mistaken for unflappable cool: it is not that the lone gunman is unflappable out of some internal wellspring of strength, but instead because there is nothing inside him to flap.
Announcing the winners of the 2017 No Budget Film Contest
Announcing the winners of the 2017 No Budget Film Contest
First place: The Ledger of St. Dermain, by tENTATIVELY, a cONVENIENCE
Second place: A GOMBOSTŰRENGETEGBEN, by SYPORCA WHANDAL
Third place: TEXAS CHAINSAW MASSACRE SWED, by William Sellari
[1] Your trophy, a picture of a coin
tENTATIVELY, a cONVENIENCE, please contact me with your preferred email address for taking paypal transfers.
Announcing the winners of the 2017 No Budget Film Contest
Announcing the winners of the 2017 No Budget Film Contest
First place: The Ledger of St. Dermain, by tENTATIVELY, a cONVENIENCE
Second place: A GOMBOSTŰRENGETEGBEN, by SYPORCA WHANDAL
Third place: TEXAS CHAINSAW MASSACRE SWED, by William Sellari
[1] Your trophy, a picture of a coin
tENTATIVELY, a cONVENIENCE, please contact me with your preferred email address for taking paypal transfers.
A short glossary of tech industry terms
A short glossary of tech industry terms
Enterprise-grade: slow and broken
Enterprise ready: once reached the front page of HN
Script: a small segment of someone’s shell history
Software engineer: a programmer with a bachelor’s degree
Software engineering: writing code that doesn’t need to exist, but writing it in Java
Market Myths: Good, Bad, and Bazaar
Market Myths: Good, Bad, and Bazaar
The stories that hold up western* capitalism
A procedural note
The truth value of a myth doesn’t matter. However, some myths have become so strongly internalized that they become difficult to identify as myths: they are mistaken for “common sense”. For most of us, the ideas underlying western* capitalism are like this. It’s difficult to separate ourselves from these myths & gain the appropriate distance, so I’m going to engage in a little bit of ‘debunking’ — specifically, I’m going to take some time specifically pointing out parts of the capitalist model that don’t match with reality or history, during the course of analyzing its structure and function. This doesn’t take away from the immense power and importance of capitalist mythology, nor does it indicate that I consider all of the ideas associated with capitalism to be strictly false.
On tautology
Academics tend to treat tautologies as a lesser form. Tautologies are shallow, by their nature. It’s quite reasonable for a system optimizing for novel and interesting ideas to reject tautologies. Nevertheless, some really important ideas can be rephrased as tautologies — as Charles Fort points out, natural selection is better summarized as “survival of the survivors” than “survival of the fittest” — and one can make the argument that any really true argument is in some sense circular. There’s no shame in a circular argument that depends only on true premises: in fact, this is one way to look at all of mathematics — which is true because of its internal consistency, and only accidentally coincides with physical reality.
When someone dismisses a seemingly profound statement as “just a tautology” they omit important information. An obvious tautology contains no information; however, a non-obvious tautology is just about the most profound thing imaginable: it takes a complex, incomplete, vague collection of loosely related ideas and replaces it with a much smaller and simpler set of rules, which (if the tautology is reasonably close to correct) is both at least as accurate as the original set of ideas and easier to reason about. A non-obvious true tautology refactors huge sections of our mental models. Obviousness is a function of existing knowledge, so what is an obvious tautology to some people will be non-obvious to others. It should come as no surprise that people seek out ideas that present themselves as non-obvious tautologies.
The drive toward seeking non-obvious tautologies can lead to mistakes. Looking for simple and efficient models of the world is a mechanism for enabling lazy thinking; when lazy thinking is correct it’s strictly superior to difficult thinking, but lazy thinking often comes with lazy metacognition. If we jump on ideas that look like non-obvious tautologies too greedily, we fail to see hidden assumptions.
Market efficiency is a very attractive model. Under certain circumstances, we can expect things to actually work that way. If a large number of competing producers really do start off completely even in capability, we really can expect the best product to price ratio to win out. To accept it completely means ignoring hidden assumptions that serious thinkers should at least consider.
One hidden assumption in market efficiency is that competitors start off even in capability. This is almost never the case outside of a classroom demonstration. Companies enter established markets and compete with established competitors, and companies established in one market will enter another; both of these mechanisms make use of existing resource inequality in order to reduce precisely the kinds of risks that lead to efficient markets, and while perhaps in the long run poor products might lose out, with the extreme spread of resource availability the “long run” can easily last until long after we are all dead. Given no other information, if age is not normally or logarithmically distributed, we can reasonably expect something to last about twice as long as it already has; with corporations, the tails of this distribution are further apart: we can expect a startup to be on its last legs, and we can expect a 50 year old company to last 75 more years, because resource accumulation corrects for risks. A company that has a great deal of early success can coast on that success for a much longer period of poor customer satisfaction.
Another hidden assumption is that communication is free within the set of consumers and between consumers and producers but not within the set of producers.
Free communication within the set of producers is called collusion, and the SEC will hit you with an antitrust suit if you are found to engage in it. People do it all the time, and it is usually worth the risk, since it reduces market efficiency down to almost zero.
Free communication between producers and consumers is also pretty rare: even failing producers typically have too many consumers to manage individually and must work with lossy and biased aggregate information; successful producers have enough resources to be capable of ignoring consumer demand for quite a while, and often encourage ‘customer loyalty’ via branding (in other words, cultivating a live stock of people who will buy their products regardless of quality — ideally enough to provide sufficient resources that appealing to the rest of the customers is unnecessary). Customer loyalty can have its benefits compounded if wealthy customers are targeted: “luxury brands” are lucrative because something can be sold well above market price regardless of its actual quality or desirability, and sometimes the poor price/desirability ratio is actually the point (as a form of lekking / conspicuous consumption).
Free communication between consumers is becoming more and more rare, since flooding consumer information channels with fake reviews and native advertising is cheap and easy. There used to be stronger social and economic incentives to clearly differentiate advertising from word of mouth, but advertising’s effectiveness has dropped significantly as customers develop defenses against it and economic instability has encouraged lots of people to lower their standards. Eventually, consumer information channels will become just as untrusted as clearly paid advertising is now considered to be, and communication between consumers will be run along the same lines as cold war espionage.
Motivated reasoning
Considering that the hidden assumptions in market efficiency are dependent upon situations even uninformed consumers know from experience are very rare, why would people accept it so easily? The inefficiency of markets has no plausible deniability, but motivated reasoning lowers the bar for plausibility significantly.
During the bulk of the 20th century we could probably argue that anti-communist propaganda played a large role. I don’t think that’s true anymore. Nevertheless, in many circles faith in the invisible hand actually is increasing.
There’s another kind of circular reasoning — one that operates on the currency of guilt and hope. If one accepts market efficiency, it tells the poor that they can rise up through hard work, and it tells the rich that they earned their wealth. (This is remarkably similar to the prosperity gospel, which claims that god rewards the righteous with wealth and therefore the poor must have secret sins; it also resembles the mandate of heaven, which claims that all political situations are divinely ordained and therefore disagreeing with the current ruler is sinful.)
The similarity between the guilt/hope axis of the market efficiency myth and the prosperity gospel explains the strange marriage between Randian objectivists and Evangelical christians found in the religious right. We can reasonably expect many members of this group to be heavily motivated by the desire to believe that the world is fair. It’s not appropriate to characterize this movement as lacking in empathy: empathy is a necessary prerequisite for a guilt so extreme that it makes an elaborate and far-fetched framework for victim-blaming look desirable.
For the poor of this movement, at least on the prosperity gospel side, it might not be so terrible: motivating a group of people to do the right thing has a good chance of actually improving life generally, even if their promised reward never materialized; second order effects from accidental windfalls are more dangerous, though (if you disown your gay son and then win the lottery, you’re liable to get the wrong idea about what “doing the right thing” means).
That said, while the above factors encourage people to trust more strongly in an idea of market efficiency they already accept, bootstrapping the idea of market efficiency is much more difficult.
Natural law / myth vs legend
Market efficiency draws power from an older myth: the idea that money is a natural and universal means of exchange. This is historically and anthropologically dubious. David Graeber, in his book Debt: The First 5,000 Years, makes an argument for the idea that systematic accounting of debts predates the use of actual currency and furthermore only became necessary when cities became large enough to necessitate something resembling modern bureaucracy. Regardless of how accurate that timeline is, we know that gift economies, potlatch, and feasting are more common in tribal nomadic societies than any kind of currency exchange, and that feasting in particular remained extremely important in Europe through the Renaissance.
The legend that backs up the myth of money-as-natural-law takes place in a town. A shoemaker trades shoes for potatoes, but doesn’t want potatoes, so he organizes a neutral currency so that potatoes and apples can be traded for shoes. Graeber points out that this level of specialization couldn’t be ‘natural’ — the town is an appropriate place to set it, since specializing in a particular crop or craft would have been suicidal in the bands of 20–50 people that most humans lived in prior to around 2000 BC.
Our first examples of writing, of course, coincide with the first permanent settlements to have a large enough population to justify heavy specialization. Our first examples of writing are, in fact, spreadsheets recording debt and credit. This, along with the evidence that the unit of currency (the mina of silver) was too substantial for most people to afford even one of (and probably was mostly moved between rooms in the temple complex), is part of Graeber’s argument that independent individuals carrying money for the purpose of direct transactions (i.e., our conception of money) probably only became common later, when imperial armies were expected to feed themselves in foreign lands.
So, on the one hand, it seems to have taken a very long time for the ‘natural’ ‘common sense’ concept of money to take hold among humans. On the other hand, people exposed to the idea of money tend to adapt to it quickly and we have even been able to teach apes to exchange tokens between themselves in exchange for goods and services — in other words, it’s a simple and intuitive system that even animals we mostly don’t consider conscious can grasp.
If something is considered natural law, it’s very easy for people to believe that it is also providence. If something is straightforward and useful in every day life, it’s very easy for people to consider it natural law.
Moral economies
Thoughtful economists tend to recognize the caveats I present here. Some behavioral economists have done great work on illuminating what kinds of things aren’t — or shouldn’t be — subject to the market. This, in turn, illuminates the market myth itself.
It’s possible to think of social relations as economic in nature. Indeed, this is a pretty common model. Transactional psychology presents social interactions as the exchange of a currency of strokes, for instance. Nevertheless, Khaneman presents an experiment that shows social relations aren’t, and shouldn’t, be fungible.
The experiment went like this: a busy day care center has a problem with parents picking up their children late, and instates a fee; parents respond by picking up their kids late more often, and paying the fee; after the fee is eliminated, the percentage of on-time pickups does not return to the pre-fee state.
Khaneman interprets the results in this way: initially, parents thought of picking their kids up late as incurring a social debt (they were guilty about inconveniencing the day care); the fee reframed it as a service (they can pay some money in exchange for their kids being watched a little longer, guilt-free); when the fee was eliminated, they felt as though they were getting the service for free.
This result looks a whole lot like the way fines for immoral business practices end up working.
If we consider that, typically, we can make up to people we feel we have wronged in a variety of ways, we consider social currency to be somewhat fungible. Nevertheless, exchanging money for social currency is still mostly taboo: paying for sex is widely considered taboo, and even those of us who feel no taboo about sex work would find the idea of someone paying someone else to be their friend a little disturbing. If my best friend helps me move furniture and I give him a twenty dollar bill, he would be insulted; if I left money on the dresser after having sex with my girlfriend, she would be insulted.
We could consider the ease with which money is quantified to be the problem. We rarely can put a number on our guilt or joy. On the other hand, we can generally determine if we feel like we’ve “done enough” to make up for something — our measures of social currency have ordinality, if not cardinality.
Instead, I think the disconnect is that money is, by design, impersonal. I cannot pay back my guilt over Peter by giving him Paul’s gratitude toward me. This is where transactional psychology’s monetary metaphor for strokes falls apart: a relationship is built up via the exchange of strokes, and that relationship has value based on trust; meanwhile, any currency has, as a key feature, the ability to operate without trust or even with distrust. Money makes living under paranoia possible, and sometimes even pleasant. But exchange of strokes has its own inherent value, and the trust it builds likewise: it cannot be replaced with money because money’s value is based only on what it can buy.
Speculation
The belief in market efficiency, and the emotional and moral dimensions of that belief, have some unfortunate consequences in speculation. Paradoxically, these consequences are opposed by the myth of money as natural law.
With speculation, one can create money without substance: promises, bets, and hedges can be nested indefinitely to create value held in superposition. A stake in a speculative market is both credit and debt until it is sold. This is natural, since social constructs are eldrich, operating on fairy logic. This is both a pot of gold and a pile of leaves until I leave the land of the sidhe. Of course, there’s every incentive to oversell, so more often than not it’s a pile of leaves: when too many superpositions collapse, so does the market.
Naive materialism, when it intersects with the idea of money as natural law, finds the eldrich nature of money in speculation disturbing. Isn’t money gold? Or coins? How can something be in my hand and then disappear? So, we get arguments for the gold standard along moral lines: “it’s immoral for something that’s real to behave like fairy dust, so we should limit its growth to match mining efficiency”.
The eldrich behavior of money has some paradoxical results. Being aware that money is a social construct tends to decrease its value (clap your hands if you believe!). The question “if someone burns a million quid on TV, does the value of the pound go up or down” is very had to answer. (If you think you know the answer, substitute a million for a trillion, or for twenty.) On the other hand, being aware of its eldrich nature also tends to slightly decouple one from potentially-destructive drives.
Belief in market efficiency leads successful speculators to believe themselves skilled. While skill at speculation might be possible, statisticians who have studied the problem have generally come to the conclusion that the success distribution is adequately explained by market speculation being entirely random. Unwarranted confidence can lead to larger bets, which (if results are random) means half the time the money disappears into thin air. This does not require malfeasance, misrepresentation, or willful ignorance (as with the 2008 housing crisis): believing that speculation involves skill is sufficient to cause the market to have larger and larger bubbles and crashes.
The Lone Gunman
The Lone Gunman
There’s a lot of debate about gun control in the United States. However, both sides, by participating in the conversation at all, have a central confusion. The gun control debate isn’t (or at least shouldn’t be) about guns at all.
Gun control advocates and anti-gun-control advocates typically focus on the use of firearms in a very specific situation: when firearms are used in mass violence. The debate centers around mass shootings on one hand, and on the other hand, upon self defense against a large group of targets. Regulation debates focus on automatic and semi-automatic weapons and large clips. This is strangely at odds with reality. After all, even a machine gun is significantly less effective at mowing down large numbers of targets than a bomb — or a car. The firearm is a weapon oddly unsuited to mass murder: even for semi-automatic weapons, the ideal use case is against a single easily identified stationary target from relatively far away. As a weapon, a gun is a great deal like a bow and arrow, although a gun can shoot farther with more accuracy and with greater force, and it’s faster to reload its projectiles. This should be enough to immediately reject both sides’ arguments from the perspective of materialism: any constraints placed on guns should be placed doubly or triply on automobiles, pressure cookers, fertilizer, boats, and weak poisons. The argument isn’t about guns as physical objects.
If the best analogy to the gun as a physical object is the crossbow, then the best analogy to the gun as a symbol is the katana. Physically, the katana is a very limited weapon: it’s a sword, long and heavy enough to take a great deal of effort to wield yet with clearly shorter range than a projectile weapon or even a spear or lance; created via a laborious process made necessary by the poor quality of Japanese iron deposits and the relatively primitive state of Japanese metalworking techniques, even katanas legendary for their high quality steel would be laughed at by medieval european blacksmiths. Yet, because of the association between the katana and the samurai class (enforced by multi-century rules about who was allowed to own these weapons), the katana has incredible symbolic power. In an age where actual warfare in Japan was largely being performed by domestic copies of imported Portugese flintlocks, a sword ban was instated to keep the samurai down. Even today, Japanese cinema is full of sword users, and invents magical techniques by which the sword might act as a ranged weapon. Despite its impracticality as an actual weapon, the katana has an incredible symbolic power to the Japanese (and to some westerners) that keeps it from being ignored. The katana represents a romanticized view of the samurai, and especially the ronin — in other words, it represents the image of a lone warrior who maintains his pride despite disgrace and whose power comes from intense training and self-discipline.
It is another such image that keeps the idea of the firearm relevant in a world where most actual warfare is performed by bombs of varying degrees of autonomy: the image of the lone gunman.
Let us examine the action hero. He is a middle-aged white man — never young, never black, never blonde or a red-head. He is very much like the standard FPS protagonist. He is muscular, poorly shaven, and is usually either ex-police or ex-military (although occasionally he is still affiliated, but not considered a part of the in-group). He works alone. He fights a large and organized force of well-equipped enemies; he does not do so out of some traditional defense of “justice” or “the law” (because he is too cynical to believe in such things) but instead for some intensely personal reason (usually to protect or avenge a family member, who is most often female). Even as the enemy uses bombs, noxious gasses, poisonous injections, throwing knives, or other weapons, our action hero protagonist uses firearms; to the extent that he uses any other weapon, he does so out of necessity, improvising, after he loses his gun or runs out of ammunition, and the weapon he improvises is almost never more destructive than a gun. (This is mirrored in samurai flicks, particularly in parodies — in the first episode of Gintama, the title character destroys a highly advanced alien-made nuclear weapon by hitting it with a wooden sword, having refused to accept a laser gun previously.) The action hero doesn’t plant bombs, although he may allow the enemy to be blown up by their own bombs; when encountering a piece of destructive machinery, even after defeating its operator, the action hero will not choose to use it, except perhaps as a transportation device, and any destructive effects of such a device will be accidental — our action hero won’t steal a tank, and although he might steal an attack helicopter he won’t use the helicopter’s bombs or machine guns.
Our gun control advocates fear the action hero to some degree; after all, the action hero works toward the goal of a safe society only incidentally. Our gun control advocates also fear those actual human beings who have been possessed by the action hero / lone gunman archetype: school shooters, right-wing terrorists, and corrupt cops. To some degree this is justified: while the action hero himself does not and cannot exist, those who have sublimated themselves into this archetype can do quite a lot of damage before their luck runs out. However, in another sense, this is foolish: the terrorist who packs a machine gun instead of a bomb is a bit like the man who tries to take on the army with a sword; he has confused symbolic strength with literal strength, and the limitations of his weapon will prevent him from doing nearly as much damage as he expects. In a sense, those who fear these groups should feel lucky that they suffer under the delusion that their weapon of choice is ideal; were they to replace their media consumption with proper training and think clearly about weapons as tools, they would be far more dangerous.
On the other hand, those who fear gun control identify strongly with the action hero, or at least believe that they could become his manifestation under the right circumstances. People who hoard guns against what they see as an oppressive government are operating on action movie logic: a small group of people with automatic weapons cannot even defend themselves against a national army, although a single con artist could probably decimate a national army with some poison and a great deal of courage.
The lone gunman, though he is often associated with the religious right’s reformulation of Randian Objectivism, in a sense is a stranger bedfellow with Objectivism than the religious right itself is. No Randian hero, the lone gunman is a loser who does not win, but instead causes others to lose. He never profits from his actions, nor does he intend to; he comes into the story already damaged and rejected by a world that he doesn’t fit into, and his goal is to save someone (usually a family member) from a threat that appears after the beginning of the narrative, or to take revenge for that threat. He plays only negative sum games: his goal is to return to the same level of dysfunction he is used to, having caused harm to some third party (usually some variety of “foreign terrorist”). The family he rescues is one he is almost invariably estranged from, just as he invariably has a warped relationship with the career that gave him the training he uses: while usually a former soldier or police officer, if he happens to be a current officer he is a pariah.
I would place the beginning of the lone gunman figure in film with the release of Die Hard. The elements of Die Hard that were originally (in the style of the Last Action Hero) a satire or subversion of action movie tropes eventually became the defining traits that separate the lone gunman from older 80s-style action hero figures, and these traits are important to note: the lone gunman, though skilled, is not ‘fit’; rather than being a well-rounded person who happens to excel at violence, this figure is a loser and outsider who (in a strange warping of the hero’s journey) discovers that he has a talent for violence when he is thrust into a situation where he uses it. He may be an ex-police-officer, but he can fight off hundreds of current police officers who have better training. Much like how, out of context, the stories of popular detective characters appear to be about a person who supernaturally attracts criminal acts to happen around them, the lone gunman appears to attract swarms of unrelated attacks.
I would like to also distinguish the lone gunman figure from another star in our constellation of men of action, the hardboiled detective. While the hardboiled/noir protagonist appears to have much in common with the lone gunman — both are losers thrust into lives of violence to which they are unnaturally acclimated, within the matrix of a society they cannot integrate into — the hardboiled protagonist’s cynicism is always a put-on. A hardboiled protagonist, being a “shop-worn Galahad”, has more in common with the ronin figure or with the hero of westerns: he may pretend to have purely selfish and material reasons for his actions, but he acts according to a strict moral code he would rather not admit he adheres to. The cynicism and nihilism of the lone gunman figure is real, and in an inversion of the hardboiled protagonist, the lone gunman acts as if his behavior is justified by familial loyalty or revenge, when it is clear that revenge is just an excuse for immersing himself in a world of violence. Where all other action hero protagonists are acclimated to violence by necessity and are at least as estranged from violent exchange as they are from the rest of the social world, the lone gunman has a greater connection to violence than with the every-day. All other forms we have discussed are rejects who carry a set of moral guidelines from a world that no longer exists or is closed to them; the lone gunman has never had a home, but finds one in the process of taking revenge, and his moral sense is warped accordingly.
In other words, the lone gunman breaks from the tradition of justified violence, instead engaging in violence that justifies itself: loss for loss’s sake. Hardly sociopathic; this is instead the logic of a perpetually frustrated death wish. That this resonates with society is interesting but not impossible to predict: prescriptive codes of ethics, to the extent that they are narratively interesting, must be problematic (a hardboiled protagonist who will “never hit a woman” is foiled on several fronts, not least by wicked women who take advantage of him); furthermore, prescriptive codes of ethics also don’t age well, particularly now that widespread and fast communications across demographics have brought about a nearly scientific style of inspection of moral and ethical issues in the public sphere. An everyman whose abilities are unknown to him at the start, the lone gunman can become an aspirational figure for those who have no skills but suspect that they may discover that they too can mow down faceless waves of military police if given the opportunity. Finally, the lack of interiority in the lone gunman figure — the reliance on a supernatural luck, the lack of planning or aspirations, and the absence of intellectual rather than material challenges — is easily mistaken for unflappable cool: it is not that the lone gunman is unflappable out of some internal wellspring of strength, but instead because there is nothing inside him to flap.
