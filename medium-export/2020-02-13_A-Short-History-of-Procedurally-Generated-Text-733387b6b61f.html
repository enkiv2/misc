<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>A Short History of Procedurally Generated Text</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">A Short History of Procedurally Generated Text</h1>
</header>
<section data-field="subtitle" class="p-summary">
An edited version of this story has been published on tedium. This is an expanded version of the original draft.
</section>
<section data-field="body" class="e-content">
<section name="8ff1" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c73c" id="c73c" class="graf graf--h3 graf--leading graf--title">A Short History of Procedurally Generated Text</h3><p name="bb2e" id="bb2e" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">An edited version of this story has been published on </em><a href="https://tedium.co/2019/11/14/procedural-text-history/" data-href="https://tedium.co/2019/11/14/procedural-text-history/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">tedium</em></a><em class="markup--em markup--p-em">. This is an expanded version of the original draft.</em></p><p name="6ee5" id="6ee5" class="graf graf--p graf-after--p">AI is in the news a lot these days, and journalists, being writers, tend to be especially interested in computers that can write. Between <a href="https://www.google.com/url?q=https://www.vox.com/2019/5/15/18623134/openai-language-ai-gpt2-poetry-try-it&amp;sa=D&amp;ust=1573133052056000" data-href="https://www.google.com/url?q=https://www.vox.com/2019/5/15/18623134/openai-language-ai-gpt2-poetry-try-it&amp;sa=D&amp;ust=1573133052056000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI’s GPT-2</a> (the text-generating ‘transformer’ whose creators are releasing it a chunk at a time out of fear that it could be used for evil), <a href="https://www.google.com/url?q=https://www.wired.com/story/botnik-ai-comedy-app/&amp;sa=D&amp;ust=1573133052056000" data-href="https://www.google.com/url?q=https://www.wired.com/story/botnik-ai-comedy-app/&amp;sa=D&amp;ust=1573133052056000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Botnik Studios</a> (the comedy collective that inspired the “we forced a bot to watch 100 hours of seinfeld” meme), and <a href="https://www.google.com/url?q=https://www.gizmodo.com.au/2018/11/forget-nanowrimo-read-these-mad-nanogenmo-novels-generated-by-computer-programs/&amp;sa=D&amp;ust=1573133052056000" data-href="https://www.google.com/url?q=https://www.gizmodo.com.au/2018/11/forget-nanowrimo-read-these-mad-nanogenmo-novels-generated-by-computer-programs/&amp;sa=D&amp;ust=1573133052056000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">National Novel Generation Month</a> (henceforth NaNoGenMo — a yearly challenge to write a program that writes a novel during the month of November), when it comes to writing machines, there’s a lot to write about. But if you only read about writing machines in the news, you might not realize that the current batch is at the tail end of a tradition that is very old.</p><blockquote name="e538" id="e538" class="graf graf--pullquote graf-after--p"><a href="https://www.youtube.com/watch?v=6nlW4EbxTD8" data-href="https://www.youtube.com/watch?v=6nlW4EbxTD8" class="markup--anchor markup--pullquote-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--pullquote-strong">A TIMELINE OF PROCEDURAL TEXT GENERATION</strong></a></blockquote><blockquote name="cb30" id="cb30" class="graf graf--pullquote graf-after--pullquote">1230 BC: The creation of the earliest known oracle bones.</blockquote><blockquote name="7775" id="7775" class="graf graf--pullquote graf-after--pullquote">9th century BC: The creation of the I Ching, an extremely influential text for bibliomancy.</blockquote><blockquote name="5576" id="5576" class="graf graf--pullquote graf-after--pullquote">1305: The publication of the first edition of Ramon Llull’s Ars Magna, whose later editions introduced combinatorics.</blockquote><blockquote name="373f" id="373f" class="graf graf--pullquote graf-after--pullquote">1781: Court de Geblin, in his book <em class="markup--em markup--pullquote-em">Le Monde Primitif</em>, claims ancient origins for tarot &amp; ties it, for the first time, deeply to cartomancy. Tarot is now one of the most popular systems for turning random arrangements of cards into narratives, and numerous guides exist for using tarot spreads to plot books</blockquote><blockquote name="7ad0" id="7ad0" class="graf graf--pullquote graf-after--pullquote">1906: Andrey Markov describes the markov chain.</blockquote><blockquote name="c93c" id="c93c" class="graf graf--pullquote graf-after--pullquote">1921: Tristan Tzara publishes ‘How To Write a Dadaist Poem’, describing the cut-up technique.</blockquote><blockquote name="3009" id="3009" class="graf graf--pullquote graf-after--pullquote">1945: Claude Shannon drafts “Communication Theory of Secrecy Systems”.</blockquote><blockquote name="80ec" id="80ec" class="graf graf--pullquote graf-after--pullquote">1948: Claude Shannon publishes “A Mathematical Theory of Communication”.</blockquote><blockquote name="b77d" id="b77d" class="graf graf--pullquote graf-after--pullquote">1952: The Manchester Mark I writes love letters.</blockquote><blockquote name="69ce" id="69ce" class="graf graf--pullquote graf-after--pullquote">1959: William S. Burroughs starts experimenting with the cut-up technique while drafting <em class="markup--em markup--pullquote-em">The Naked Lunch</em>.</blockquote><blockquote name="8e9e" id="8e9e" class="graf graf--pullquote graf-after--pullquote">1960: Oulipo founded.</blockquote><blockquote name="f6e4" id="f6e4" class="graf graf--pullquote graf-after--pullquote">1961: SAGA II’s western screenplay is broadcast on television.</blockquote><blockquote name="3e6d" id="3e6d" class="graf graf--pullquote graf-after--pullquote">1963: Marc Saporta publishes <em class="markup--em markup--pullquote-em">Composition No. I</em>, an early work of ‘shuffle literature’, wherein pages are read in an arbitrary order.</blockquote><blockquote name="ca7c" id="ca7c" class="graf graf--pullquote graf-after--pullquote">1966: Joseph Wizenbaum writes ELIZA.</blockquote><blockquote name="35d6" id="35d6" class="graf graf--pullquote graf-after--pullquote">1969: Georges Perec’s lipogrammic novel <em class="markup--em markup--pullquote-em">La disparition</em> is published.</blockquote><blockquote name="15c1" id="15c1" class="graf graf--pullquote graf-after--pullquote">1970: Richard &amp; Martin Moskof publish <em class="markup--em markup--pullquote-em">A Shufflebook</em>, an early work of shuffle literature.</blockquote><blockquote name="e79f" id="e79f" class="graf graf--pullquote graf-after--pullquote">1972: The Dissociated Press ‘travesty generator’ is described in <em class="markup--em markup--pullquote-em">HAKMEM</em>.</blockquote><blockquote name="4e95" id="4e95" class="graf graf--pullquote graf-after--pullquote">1976: James Meehan creates TaleSpin.</blockquote><blockquote name="e7b9" id="e7b9" class="graf graf--pullquote graf-after--pullquote">1977: William S. Burroughs and Brion Gysin publish <em class="markup--em markup--pullquote-em">The Third Mind</em>, which popularized cut-ups in the literary and occult sets.</blockquote><blockquote name="fc80" id="fc80" class="graf graf--pullquote graf-after--pullquote">1978: Georges Perec, a member of Oulipo, publishes <em class="markup--em markup--pullquote-em">Life: A User’s Manual</em>, which is subject to several oulipolian constraints. Robert Grenier publishes <em class="markup--em markup--pullquote-em">Sentences</em>, another early work of shuffle literature.</blockquote><blockquote name="9f5c" id="9f5c" class="graf graf--pullquote graf-after--pullquote">1983: The ‘travesty generator’ is described in <em class="markup--em markup--pullquote-em">Scientific American</em>. <em class="markup--em markup--pullquote-em">The Policeman’s Beard is Half Constructed</em>, a book written by ‘artificial insanity’ program Racter, is published.</blockquote><blockquote name="ab25" id="ab25" class="graf graf--pullquote graf-after--pullquote">1984: Source code for various travesty generators are published in BYTE. Racter is released commercially.</blockquote><blockquote name="92cd" id="92cd" class="graf graf--pullquote graf-after--pullquote">1997: Espen Aarseth coins the term ‘Ergodic Literature’, makes a distinction between hypertext and cybertext in his book <em class="markup--em markup--pullquote-em">Cybertext — Perspectives on Ergodic Literature.</em></blockquote><blockquote name="0c33" id="0c33" class="graf graf--pullquote graf-after--pullquote">2005: A paper written by SCIgen is accepted into the WMSCI conference.</blockquote><blockquote name="d72a" id="d72a" class="graf graf--pullquote graf-after--pullquote">2013: Darius Kazemi starts the first annual National Novel Generation Month.</blockquote><blockquote name="4049" id="4049" class="graf graf--pullquote graf-after--pullquote">2014: Eugene Goostman passes Royal Society Turing Test.</blockquote><blockquote name="531a" id="531a" class="graf graf--pullquote graf-after--pullquote">2017: <em class="markup--em markup--pullquote-em">Inside The Castle</em>’s first <em class="markup--em markup--pullquote-em">Castle Freak</em> remote residency produces <em class="markup--em markup--pullquote-em">Lonely Men Club</em>.</blockquote><blockquote name="b9ea" id="b9ea" class="graf graf--pullquote graf-after--pullquote">2019: OpenAI releases the complete GPT-2 model.</blockquote><p name="85c7" id="85c7" class="graf graf--p graf-after--pullquote">Depending on how loosely you want to define ‘machine’ and ‘writing’, you can plausibly claim that writing machines are almost as old as writing. The earliest examples of Chinese writing we have were part of <a href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Oracle_bone%23Cracking_and_interpretation&amp;sa=D&amp;ust=1573133052060000" data-href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Oracle_bone%23Cracking_and_interpretation&amp;sa=D&amp;ust=1573133052060000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">a divination method</a> where random cracks on bone were treated as choosing from or eliminating part of a selection of pre-written text (a technique still used in composing computer-generated stories). Descriptions of forms of divination whereby random arrangements of shapes are treated as written text go back as far as we have records — today we’d call this kind of thing ‘asemic writing’ (asemic being a fancy term for ‘meaningless’), and yup, computers do that too.</p><p name="9a0e" id="9a0e" class="graf graf--p graf-after--p">But, the first recognizably systematic procedure for creating text is probably that of the medieval mystic Ramon Llull. For the second edition of his book Ars Magna (first published in 1305), he introduced the use of diagrams and spinning concentric papercraft wheels as a means of combining letters — something he claimed could show all possible truths about a subject. Much as a tarot spread produces a framework for slotting randomly chosen symbols (cards) such that any possible combination produces a statement that, given sufficient effort on the part of an interpreter, is both meaningful and true, Llull’s circles or rota do the same for domains of knowledge: one is meant to meditate on the connections revealed by these wheels.</p><p name="8127" id="8127" class="graf graf--p graf-after--p">While computer-based writing systems today tend to have more complicated rules about how often to combine letters, the basic concept of defining all possible combinations of some set of elements (a branch of mathematics now called combinatorics) looms large over AI and procedural art in general. Llull is credited with inspiring this field, through his influence upon philosopher and mathematician Gottfried von Leibniz, who also was inspired by the hexagrams of the I Ching to systematize binary arithmetic and introduce it to the west. The I Ching combines bibliomancy (divination by choosing random passages from a book) with cleromancy (divination through random numbers).</p><figure name="4a81" id="4a81" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*hjlzzrZ0b1GfNxla_hl3rw.jpeg" data-width="1318" data-height="1800" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*hjlzzrZ0b1GfNxla_hl3rw.jpeg"><figcaption class="imageCaption">One of Llull’s rota</figcaption></figure><p name="9610" id="9610" class="graf graf--p graf-after--figure">The twentieth century, though, is really when procedural literature comes into its own — and even in the modern era, the association between randomness, procedural generation, and divination persists.</p><p name="5f16" id="5f16" class="graf graf--p graf-after--p">Llull’s combinatorics, which had echoed through mathematics for six hundred years, got combined with statistics and in 1906 Andrey Markov published his first paper on what would later become known as the ‘Markov Chain’ — a still-popular method of making a whole sequence of events (such as an entire novel) out of observations about how often one kind of event follows another (such as how often the word ‘cows’ comes after the word ‘two’). Markov chains would become useful in other domains (for instance, they became an important part of the ‘monte carlo method’ used in the first computer simulations of hydrogen bombs), but they are most visible in the form of text generators: they are the source of the email ‘spam poetry’ you probably receive daily (an attempt to weaken automatic spam-recognition software, which looks at the same statistics about text that markov chains duplicate) and they are the basis of the nonsense-spewing ‘ebooks’ bots on twitter.</p><p name="2105" id="2105" class="graf graf--p graf-after--p">Fifteen years after Markov’s paper, the Dadaist art movement popularized another influential technique with the essay “HOW TO MAKE A DADAIST POEM”. This is known as the ‘cut up’ technique, because it involves cutting up text and rearranging it at random. It’s the basis for refrigerator poetry kits, but literary luminaries like T. S. Elliot, William S. Burroughs, and David Bowie used the method (on source text edgier than the magnetic poetry people dare use, such as negative reviews) to create some of their most groundbreaking, famous, and enduring work.</p><p name="15dd" id="15dd" class="graf graf--p graf-after--p">Burroughs, in his book about cutups, explains that “when you cut up the present, the future leaks out” — but it seems like when we interpret the meaningless, we see our own unknown-knowns. Divination gives us access to the parts of our intuition we might otherwise distrust, by externalizing it and attributing it to another source — and what better and more reliable source of knowledge is there than a computer, with its mythology of perfect information and perfect logic?</p><p name="a2fb" id="a2fb" class="graf graf--p graf-after--p">When computerized text generators use markov chains, a lot of the appeal comes from the information lost by the model — the discontinuities and juxtapositions created by the fact that there’s more that matters in an essay than how often two words appear next to each other — so most use of markov chains in computerized text generation are also, functionally, using the logic of the cut-up technique. That said, there are computerized cut-up generators of various varieties, some mimicking particular patterns of paper cut-ups.</p><figure name="8ae0" id="8ae0" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/6nlW4EbxTD8?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><blockquote name="28c0" id="28c0" class="graf graf--blockquote graf-after--figure">David Bowie used a computer program called the verbasizer to automate a variation of the cut-up technique to produce the lyrics on his 1995 album Outside. Based on his description, <a href="https://www.google.com/url?q=https://github.com/enkiv2/verbasizer-verbalizer&amp;sa=D&amp;ust=1573133052061000" data-href="https://www.google.com/url?q=https://github.com/enkiv2/verbasizer-verbalizer&amp;sa=D&amp;ust=1573133052061000" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">I wrote a program to simulate it</a>.</blockquote><blockquote name="30c1" id="30c1" class="graf graf--blockquote graf-after--blockquote">He’s no stranger to the technique — he’s been using it since the 70s.</blockquote><blockquote name="5ecd" id="5ecd" class="graf graf--blockquote graf-after--blockquote">Cut-ups can be performed by cutting out individual words or phrases, or by cutting into columns that are then rearranged. While the clip below shows Bowie cutting out individual phrases from diaries, the verbasizer simulates columnar cuts, which are then shifted up or down.</blockquote><blockquote name="6226" id="6226" class="graf graf--blockquote graf-after--blockquote">Word-based cutups are familiar to most people from their use in magnetic poetry kits. These kits are part of a long tradition of using elements of procedural art to make avant-garde work more accessible to a general audience — a tradition that is now quite lucrative.</blockquote><figure name="3437" id="3437" class="graf graf--figure graf--iframe graf-after--blockquote"><iframe src="https://www.youtube.com/embed/m1InCrzGIPU?feature=oembed" width="640" height="480" frameborder="0" scrolling="no"></iframe></figure><p name="2ab0" id="2ab0" class="graf graf--p graf-after--figure">Claude Shannon (scientific pioneer, juggling unicyclist, and inventor of the machine that shuts itself off) was thinking about markov chains in relation to literature when he came up with his concept of ‘information entropy’. When his paper on this subject was published in 1948, it launched the field of information theory, which now forms the basis of much of computing and telecommunications. The idea of information entropy is that rare combinations of things are more useful in predicting future events — the word ‘the’ is not very useful for predicting what comes after it, because nouns come after it at roughly the same rate as nouns come after all sorts of other words, whereas in english ‘et’ almost always comes before ‘cetera’. In his model, this means ‘the’ is a very low-information word, while ‘et’ is a very high-information word. In computer text generation, information theory gets used to reason about what might be interesting to a reader: a predictable text is boring, but one that is too strange can be hard to read.</p><p name="8f70" id="8f70" class="graf graf--p graf-after--p">Making art stranger by increasing the information in it is the goal of some of the major 20th century avant-garde art movements. Building upon Dada, Oulipo (a french ‘workshop of potential literature’ formed in 1960) took procedural generation of text by humans to the next level, inventing a catalogue of ‘constraints’ — games to play with text, either limiting what can be written in awkward ways (such as never using the letter ‘e’) or changing an existing work (such as replacing every noun with the seventh noun listed after it in the dictionary). Oulipo has been very influential on computer text generation, in part because they became active shortly after computerized generation of literature began, and in part because constraints both justify the stylistic strangeness of computer-generated text and form an interesting basis for easily-programmed filters.</p><p name="7cac" id="7cac" class="graf graf--p graf-after--p">In 1952,<a href="https://www.google.com/url?q=https://www.newyorker.com/tech/elements/christopher-stracheys-nineteen-fifties-love-machine&amp;sa=D&amp;ust=1573133052062000" data-href="https://www.google.com/url?q=https://www.newyorker.com/tech/elements/christopher-stracheys-nineteen-fifties-love-machine&amp;sa=D&amp;ust=1573133052062000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> the Manchester Mark I was programmed to write love letters</a>, but the first computerized writing machine to get a TV spot was <a href="https://www.google.com/url?q=https://lizadaly.com/pages/saga-iii/&amp;sa=D&amp;ust=1573133052063000" data-href="https://www.google.com/url?q=https://lizadaly.com/pages/saga-iii/&amp;sa=D&amp;ust=1573133052063000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">1961’s SAGA II, which wrote screenplays for TV westerns</a>. SAGA II has the same philosophy of design as <a href="https://www.google.com/url?q=https://grandtextauto.soe.ucsc.edu/2006/09/13/the-story-of-meehans-tale-spin/&amp;sa=D&amp;ust=1573133052063000" data-href="https://www.google.com/url?q=https://grandtextauto.soe.ucsc.edu/2006/09/13/the-story-of-meehans-tale-spin/&amp;sa=D&amp;ust=1573133052063000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">1976’s TaleSpin</a>: what Judith van Stegeren and Marlet Theune (in <a href="https://www.google.com/url?q=https://pdfs.semanticscholar.org/d744/edc7e5b2e01220b34ba0bd8240abdb3c36f6.pdf&amp;sa=D&amp;ust=1573133052063000" data-href="https://www.google.com/url?q=https://pdfs.semanticscholar.org/d744/edc7e5b2e01220b34ba0bd8240abdb3c36f6.pdf&amp;sa=D&amp;ust=1573133052063000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">their paper on techniques used in NaNoGenMo</a>) term ‘simulation’. Simulationist text generation involves creating a set of rules for a virtual world, simulating how those rules play out, and then describing the state of the world: sort of like narrating as a robot plays a video game. Simulation has high ‘narrative coherence’ — everything that happens makes sense — but tends to be quite dull, &amp; to the extent that systems like SAGA II and TaleSpin are remembered today, it’s because bugs occasionally caused them to produce amusingly broken or nonsensical stories (what the authors of TaleSpin called ‘<a href="https://www.google.com/url?q=https://grandtextauto.soe.ucsc.edu/2008/02/25/ep-56-re-reading-tale-spin/&amp;sa=D&amp;ust=1573133052063000" data-href="https://www.google.com/url?q=https://grandtextauto.soe.ucsc.edu/2008/02/25/ep-56-re-reading-tale-spin/&amp;sa=D&amp;ust=1573133052063000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">mis-spun tales</a>’).</p><figure name="082e" id="082e" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/5YBIrc-6G-0?start=1908&amp;feature=oembed&amp;start=1908" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><blockquote name="8d93" id="8d93" class="graf graf--blockquote graf-after--figure">The screenplay by SAGA II (above) has quite a different feel from Sunspring, another screenplay also staged by actors (below).</blockquote><figure name="9d88" id="9d88" class="graf graf--figure graf--iframe graf-after--blockquote"><iframe src="https://www.youtube.com/embed/LY7x2Ihqjmc?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><blockquote name="a9e8" id="a9e8" class="graf graf--blockquote graf-after--figure">This is because, while SAGA II explicitly models characters and their environment, the neural net that wrote Sunspring does not. Neural nets, like markov chains, really only pay attention to frequency of co-occurrence, although modern neural nets can do this in a very nuanced way, able to weigh not just how the immediate next word is affected but how that affects words half a sentence away, and at the same time able to invent new words by making predictions at the level of individual letters. But, because they are statistical, everything a neural net knows about the world comes down to associations in its training data — in the case of a neural net trained on text, how often certain words appear near each other.</blockquote><blockquote name="4190" id="4190" class="graf graf--blockquote graf-after--blockquote">This accounts for how dreamlike Sunspring feels. SAGA II is about people with clear goals, pursuing those goals and succeeding or failing. Characters in Sunspring come off as more complicated because they are not consistent: the neural net wasn’t able to model them well enough to give them motivations. They speak in a funny way because the neural net didn’t have enough data to know how people talk, and they make sudden shifts in subject matter because the neural net has a short attention span.</blockquote><blockquote name="b26f" id="b26f" class="graf graf--blockquote graf-after--blockquote">These sound like drawbacks, but Sunspring is (at least to my eyes) the more entertaining of the two films. The actors were able to turn the inconsistencies into subtext, and in turn, they allowed the audience to believe that there was a meaning behind the film — one that is necessarily more complex &amp; interesting than the rudimentary contest SAGA produced.</blockquote><p name="b29d" id="b29d" class="graf graf--p graf-after--blockquote">In 1966, Joseph Wizenbaum wrote a simple chatbot called Eliza, and was surprised that human beings (even those who knew better) were willing to suspend disbelief &amp; engage intellectually with this machine. This phenomenon became known as <a href="https://www.google.com/url?q=https://books.google.com/books?id%3DsomvbmHCaOEC%26pg%3DPA157&amp;sa=D&amp;ust=1573133052065000" data-href="https://www.google.com/url?q=https://books.google.com/books?id%3DsomvbmHCaOEC%26pg%3DPA157&amp;sa=D&amp;ust=1573133052065000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the eliza effect</a>, &amp; it’s here that the origins of text generation in divination once again join up with computing.</p><p name="8c5c" id="8c5c" class="graf graf--p graf-after--p">Just as a monkey typing on a typewriter will eventually produce Hamlet (assuming a monkey actually was random — it turns out they tend to sit on the keyboard, which prevents certain patterns from coming up), there is no reason why a computer-generated text cannot be something profound. But, the biggest factor in whether or not we experience that profoundness is whether or not we are open to it — whether or not we are willing to believe that words have meaning even when written by a machine, and go for whatever ride the machine is taking us on — the eliza effect is akin to suspension of disbelief in cinema or the kind of altered state of mind that goes along with religious and magical ritual, and it should not be surprising that the techniques of rogerian psychotherapy (coming as they do from the humanistic movement in psychology, with its interest in encounter therapy and other kinds of shocking reframings) would produce it in an effective way: eliza, like a tarot deck, is a mirror of the querent’s soul.</p><p name="c125" id="c125" class="graf graf--p graf-after--p">All the refinement that goes into these techniques is really aimed at decreasing the likelihood that a reader will reject the text as meaningless out of hand — and meanwhile, text generated by substantially less nuanced systems are declared as genius when attributed to William S. Burroughs.</p><p name="409e" id="409e" class="graf graf--p graf-after--p">Some text-generation techniques, therefore, attempt to evoke the eliza effect (or, as Stegren &amp; Theune call it, ‘charity of interpretation’) directly. After all, some kinds of art simply require more work from an audience before they make any sense; if you make sure the generated art gets classified into that category, people will be more likely to attribute meaning to it. It is in this way that<a href="https://www.google.com/url?q=https://www.theguardian.com/technology/2014/jun/08/super-computer-simulates-13-year-old-boy-passes-turing-test&amp;sa=D&amp;ust=1573133052065000" data-href="https://www.google.com/url?q=https://www.theguardian.com/technology/2014/jun/08/super-computer-simulates-13-year-old-boy-passes-turing-test&amp;sa=D&amp;ust=1573133052065000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> ‘Eugene Goostman’, a fairly rudimentary bot, ‘passed’ the Royal Society’s turing test in 2014</a>: Eugene pretended to be a pre-teen who didn’t speak English very well, and judges attributed his inability to answer questions coherently to that rather than to his machine nature. Inattention can also be a boon to believability: as Gwern Branwen (<a href="https://www.google.com/url?q=https://www.gwern.net/TWDNE&amp;sa=D&amp;ust=1573133052066000" data-href="https://www.google.com/url?q=https://www.gwern.net/TWDNE&amp;sa=D&amp;ust=1573133052066000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">of neural-net-anime-face-generator thisWaifuDoesNotExist.net fame</a>) notes, <a href="https://www.google.com/url?q=https://www.gwern.net/GPT-2&amp;sa=D&amp;ust=1573133052066000" data-href="https://www.google.com/url?q=https://www.gwern.net/GPT-2&amp;sa=D&amp;ust=1573133052066000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI’s GPT-2 produces output that only really seems human when you’re skimming</a>, and if you’re reading carefully, <a href="https://www.google.com/url?q=https://slatestarcodex.com/2019/02/18/do-neural-nets-dream-of-electric-hobbits/&amp;sa=D&amp;ust=1573133052066000" data-href="https://www.google.com/url?q=https://slatestarcodex.com/2019/02/18/do-neural-nets-dream-of-electric-hobbits/&amp;sa=D&amp;ust=1573133052066000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the problems can be substantial</a>.</p><p name="4930" id="4930" class="graf graf--p graf-after--p">On the other hand, some folks are interested in computer-generated text precisely because computers cannot pass for human.</p><p name="67bc" id="67bc" class="graf graf--p graf-after--p">Botnik Studios uses markov chains and a predictive keyboard to bias writers in favor of particular styles, speeding up the rate at which comedians can create convincing pastiches. Since you can create a markov model of two unrelated corpora (for instance, cookbooks and the works of Nietzsche), they can create mash-ups too — so they recently released an album called Songularity featuring tracks like “Bored with this Desire to get Ripped” (written with the aid of a computer trained on Morrissey lyrics and posts from bodybuilding forums).</p><p name="9a76" id="9a76" class="graf graf--p graf-after--p">Award-winning author Robin Sloan <a href="https://www.google.com/url?q=https://www.robinsloan.com/notes/writing-with-the-machine/&amp;sa=D&amp;ust=1573133052067000" data-href="https://www.google.com/url?q=https://www.robinsloan.com/notes/writing-with-the-machine/&amp;sa=D&amp;ust=1573133052067000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">attached a similar predictive text system to neural nets trained on old science fiction</a>, while Janelle Shane, in her <a href="https://www.google.com/url?q=https://aiweirdness.com/&amp;sa=D&amp;ust=1573133052067000" data-href="https://www.google.com/url?q=https://aiweirdness.com/&amp;sa=D&amp;ust=1573133052067000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AI Weirdness</a> blog, selects the most amusing material after the neural net is finished.</p><p name="9c76" id="9c76" class="graf graf--p graf-after--p">The publisher Inside The Castle holds <a href="https://www.google.com/url?q=http://www.insidethecastle.org/castle-freak/&amp;sa=D&amp;ust=1573133052067000" data-href="https://www.google.com/url?q=http://www.insidethecastle.org/castle-freak/&amp;sa=D&amp;ust=1573133052067000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">an annual ‘remote residency’ for authors who are willing to use procedural generation methods to make difficult, alienating literature</a> — notably spawning the novel Lonely Men Club, a surreal book about a time-traveling Zodiac Killer. Lonely Men Club is an interesting stylistic experiment — deliberately repetitive to the point of difficulty — and actually reading it could produce an altered state of mind; it is theory-fiction embodied in a way that an unaided human probably could not produce, and it was itself produced in an altered state (days of little sleep).</p><p name="df41" id="df41" class="graf graf--p graf-after--p">The use of automation and its function in the advancement of potential literature is <a href="https://medium.com/@enkiv2/the-hidden-benefits-of-nanogenmo-dd91193bda6?source=friends_link&amp;sk=81c5a68c2a1e03f546082ffc8132b268" data-href="https://medium.com/@enkiv2/the-hidden-benefits-of-nanogenmo-dd91193bda6?source=friends_link&amp;sk=81c5a68c2a1e03f546082ffc8132b268" class="markup--anchor markup--p-anchor" target="_blank">important and under-studied</a>. Machine-generated text has the capacity to be <a href="https://medium.com/@enkiv2/novelty-perversity-and-randomness-307158924298?source=friends_link&amp;sk=6674322b291afabafad94e16e2988eb8" data-href="https://medium.com/@enkiv2/novelty-perversity-and-randomness-307158924298?source=friends_link&amp;sk=6674322b291afabafad94e16e2988eb8" class="markup--anchor markup--p-anchor" target="_blank">perverse</a> in excess of <a href="https://medium.com/@enkiv2/the-manufacture-of-steam-engine-time-9f83cd24a9b0?source=friends_link&amp;sk=b681feb80672ae0553423b2e9ac4e286" data-href="https://medium.com/@enkiv2/the-manufacture-of-steam-engine-time-9f83cd24a9b0?source=friends_link&amp;sk=b681feb80672ae0553423b2e9ac4e286" class="markup--anchor markup--p-anchor" target="_blank">human maxima</a>, and <a href="https://mystudentvoices.com/spanning-problem-space-74e81c367bb7?source=friends_link&amp;sk=0aed75c4e982d3c1659e1f5bc0e0c50f" data-href="https://mystudentvoices.com/spanning-problem-space-74e81c367bb7?source=friends_link&amp;sk=0aed75c4e982d3c1659e1f5bc0e0c50f" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">to produce inhuman novelty</a>, <a href="https://medium.com/@enkiv2/art-imitation-and-the-power-of-flawed-media-7c1edb4e90a4?source=friends_link&amp;sk=d655a9cd69b45c9faa95b0462fb0072f" data-href="https://medium.com/@enkiv2/art-imitation-and-the-power-of-flawed-media-7c1edb4e90a4?source=friends_link&amp;sk=d655a9cd69b45c9faa95b0462fb0072f" class="markup--anchor markup--p-anchor" target="_blank">advancing the state of art</a>. Meanwhile, <a href="https://medium.com/@enkiv2/bot-capitalism-will-fail-6e7dede405fb?source=friends_link&amp;sk=dd76adeec7117aee74af9097da721775" data-href="https://medium.com/@enkiv2/bot-capitalism-will-fail-6e7dede405fb?source=friends_link&amp;sk=dd76adeec7117aee74af9097da721775" class="markup--anchor markup--p-anchor" target="_blank">commercial applications are overstated</a>, <a href="https://medium.com/@enkiv2/how-bots-were-born-from-spam-62f6c621351f?source=friends_link&amp;sk=3b8bf3e7ea365d1725c7cdf4c3af85c5" data-href="https://medium.com/@enkiv2/how-bots-were-born-from-spam-62f6c621351f?source=friends_link&amp;sk=3b8bf3e7ea365d1725c7cdf4c3af85c5" class="markup--anchor markup--p-anchor" target="_blank">with the exception of spam</a>.</p><blockquote name="f57d" id="f57d" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong">NaNoGenMo Entries Worth Reading Some Of</strong></blockquote><blockquote name="c3c5" id="c3c5" class="graf graf--blockquote graf-after--blockquote">Writing 50,000 words in such a way that somebody wants to read all of them is hard, even for humans. Machines do considerably worse — a deep or close reading of a novel-length machine-authored work may constitute risk of infohazard (as MARYSUE’s output warns). That said, some entries are worth looking at (either because they’re beautiful, perverse, strange, or very well crafted). Here are some of my favorites.</blockquote><blockquote name="22ba" id="22ba" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Most Likely to be Mistaken for the Work of a Human Author</strong>: <a href="https://www.google.com/url?q=https://catseye.tc/modules/MARYSUE/generated/A_Time_for_Destiny.html&amp;sa=D&amp;ust=1573133052068000" data-href="https://www.google.com/url?q=https://catseye.tc/modules/MARYSUE/generated/A_Time_for_Destiny.html&amp;sa=D&amp;ust=1573133052068000" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">MARYSUE</a>, an extremely accurate simulation of bad Star Trek fanfiction by Chris Pressey.</blockquote><blockquote name="7aea" id="7aea" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Text Most Likely to be Read to Completion</strong>: <a href="https://www.google.com/url?q=https://drive.google.com/file/d/0B97d5C256qbrOHFwSUhsZE4tU0k/view?usp%3Dsharing&amp;sa=D&amp;ust=1573133052068000" data-href="https://www.google.com/url?q=https://drive.google.com/file/d/0B97d5C256qbrOHFwSUhsZE4tU0k/view?usp%3Dsharing&amp;sa=D&amp;ust=1573133052068000" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Annals of the Perrigues</a>, a moody and surreal travel guide to imaginary towns by Emily Short. Annals uses a tarot-like ‘suit’ structure to produce towns with a difficult-to-pinpoint thematic consistency — suits like ‘salt’ and ‘egg’, each with its own group of attributes, objects, and modifiers.</blockquote><blockquote name="8ef3" id="8ef3" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Most Likely to be Mistaken for Moody Avant-Garde Art</strong>: <a href="https://www.google.com/url?q=https://gregborenstein.com/comics/generated_detective/&amp;sa=D&amp;ust=1573133052069000" data-href="https://www.google.com/url?q=https://gregborenstein.com/comics/generated_detective/&amp;sa=D&amp;ust=1573133052069000" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Generated Detective</a>, a comic book generated from hardboiled detective novels and flickr images, by Greg Borenstein.</blockquote><blockquote name="2dfe" id="2dfe" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Lowest Effort Entry</strong>: <a href="https://www.google.com/url?q=https://github.com/dariusk/NaNoGenMo-2014/issues/50&amp;sa=D&amp;ust=1573133052069000" data-href="https://www.google.com/url?q=https://github.com/dariusk/NaNoGenMo-2014/issues/50&amp;sa=D&amp;ust=1573133052069000" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">50,000 Meows</a>, a lot of meows, some longer than others, by Hugo van Kemenade.</blockquote><blockquote name="0f12" id="0f12" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Most Unexpectedly Compelling</strong>: <a href="https://www.google.com/url?q=https://github.com/NaNoGenMo/2017/issues/127&amp;sa=D&amp;ust=1573133052069000" data-href="https://www.google.com/url?q=https://github.com/NaNoGenMo/2017/issues/127&amp;sa=D&amp;ust=1573133052069000" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Infinite Fight Scene</a>, exactly what it says on the tin, by Filip Hracek.</blockquote><blockquote name="a9ed" id="a9ed" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">The Most Beautiful Entry That Is Completely Unreadable</strong>: <a href="https://www.google.com/url?q=https://github.com/lizadaly/nanogenmo2014&amp;sa=D&amp;ust=1573133052070000" data-href="https://www.google.com/url?q=https://github.com/lizadaly/nanogenmo2014&amp;sa=D&amp;ust=1573133052070000" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Seraphs</a>, a codex of asemic writing and illustrations by Liza Daly.</blockquote><figure name="6262" id="6262" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="1*LEBDokFyLfLxXHT1s_e6lw.png" data-width="856" data-height="482" src="https://cdn-images-1.medium.com/max/800/1*LEBDokFyLfLxXHT1s_e6lw.png"></figure><blockquote name="1e43" id="1e43" class="graf graf--blockquote graf-after--figure"><em class="markup--em markup--blockquote-em">Image from Seraphs by Liza Daly, by permission</em></blockquote><p name="37d2" id="37d2" class="graf graf--p graf-after--blockquote">Machine-generated text isn’t limited to the world of art and entertainment.</p><p name="dd85" id="dd85" class="graf graf--p graf-after--p">Spammers use markov chain based ‘spam poetry’ to perform<a href="https://www.google.com/url?q=https://blog.malwarebytes.com/security-world/2017/02/explained-bayesian-spam-filtering/&amp;sa=D&amp;ust=1573133052070000" data-href="https://www.google.com/url?q=https://blog.malwarebytes.com/security-world/2017/02/explained-bayesian-spam-filtering/&amp;sa=D&amp;ust=1573133052070000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> bayesian poisoning</a> (a way of making spam filters, which rely on the same kinds of statistics that markov chains use, less effective) and template-based ‘<a href="https://www.google.com/url?q=http://alexking.org/blog/2013/12/22/spam-comment-generator-script&amp;sa=D&amp;ust=1573133052070000" data-href="https://www.google.com/url?q=http://alexking.org/blog/2013/12/22/spam-comment-generator-script&amp;sa=D&amp;ust=1573133052070000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">spinners</a>’ to send out massive numbers of slightly-reworded messages with identical meanings. The sheer scale of semantic mangling at work in spam is itself surreal.</p><p name="6519" id="6519" class="graf graf--p graf-after--p"><a href="https://www.google.com/url?q=http://pdos.csail.mit.edu/scigen/&amp;sa=D&amp;ust=1573133052071000" data-href="https://www.google.com/url?q=http://pdos.csail.mit.edu/scigen/&amp;sa=D&amp;ust=1573133052071000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Code to generate nonsense academic</a> articles was written in order to identify fraudulent academic conferences and journals, which then became used by fraudulent academics as a means of padding their resume, leading to several major academic publishers using <a href="https://www.google.com/url?q=http://scigendetection.imag.fr/&amp;sa=D&amp;ust=1573133052071000" data-href="https://www.google.com/url?q=http://scigendetection.imag.fr/&amp;sa=D&amp;ust=1573133052071000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">statistical methods</a> to identify and filter out these machine-generated nonsense papers after <a href="https://www.google.com/url?q=http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763&amp;sa=D&amp;ust=1573133052071000" data-href="https://www.google.com/url?q=http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763&amp;sa=D&amp;ust=1573133052071000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">more than a hundred of them successfully passed peer review</a>. Somehow, actual scientists read these papers and decided they were acceptable.</p><p name="c6e5" id="c6e5" class="graf graf--p graf-after--p graf--trailing"><a href="https://www.google.com/url?q=http://www.usatoday.com/story/money/business/2014/06/30/ap-automated-stories/11799077/&amp;sa=D&amp;ust=1573133052071000" data-href="https://www.google.com/url?q=http://www.usatoday.com/story/money/business/2014/06/30/ap-automated-stories/11799077/&amp;sa=D&amp;ust=1573133052071000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The associated press is generating some of its coverage of business deals and local sports automatically</a>, and <a href="https://www.google.com/url?q=http://www.newstatesman.com/future-proof/2014/10/ultimate-weapon-against-gamergate-time-wasters-1960s-chat-bot-wastes-their-time&amp;sa=D&amp;ust=1573133052071000" data-href="https://www.google.com/url?q=http://www.newstatesman.com/future-proof/2014/10/ultimate-weapon-against-gamergate-time-wasters-1960s-chat-bot-wastes-their-time&amp;sa=D&amp;ust=1573133052071000" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">some activists are using simple chatbots to distract twitter trolls from human targets</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@enkiv2" class="p-author h-card">John Ohno</a> on <a href="https://medium.com/p/733387b6b61f"><time class="dt-published" datetime="2020-02-13T14:27:09.116Z">February 13, 2020</time></a>.</p><p><a href="https://medium.com/@enkiv2/a-short-history-of-procedurally-generated-text-733387b6b61f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 21, 2025.</p></footer></article></body></html>